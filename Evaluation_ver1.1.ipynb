{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical_data(df, target_column='Y'):\n",
    "    df_encoded = df.copy()\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_cols = [col for col in categorical_cols if col != target_column]\n",
    "    \n",
    "    for column in categorical_cols:\n",
    "        if df_encoded[column].dtype == 'object' or df_encoded[column].dtype.name == 'category':\n",
    "            df_encoded[column] = le.fit_transform(df_encoded[column].astype(str))\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def robust_transform(X):\n",
    "    scaler = RobustScaler()\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    else:\n",
    "        X = np.asarray(X)\n",
    "        return scaler.fit_transform(X)\n",
    "\n",
    "def apply_robust_transform(X, y):\n",
    "    X_transformed = robust_transform(X)\n",
    "    return X_transformed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def apply_synthetic_data_to_training(X, y, test_size=0.2, random_state=42):\n",
    "    # Ensure X is a DataFrame and reset its index\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    X = X.reset_index(drop=True)\n",
    "    \n",
    "    # Ensure y is a Series and reset its index\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Combine X_train and y_train into a single DataFrame\n",
    "    train_data = pd.concat([X_train, y_train.rename('Y')], axis=1)\n",
    "    \n",
    "    # Generate synthetic data using Decision Tree\n",
    "    synthetic_data = generate_data_decision_tree(train_data)\n",
    "    \n",
    "    # Combine the synthetic data with the original training data\n",
    "    augmented_train_data = pd.concat([train_data, synthetic_data], ignore_index=True)\n",
    "    \n",
    "    # Separate features and target for the augmented training data\n",
    "    X_train_augmented = augmented_train_data.drop('Y', axis=1)\n",
    "    y_train_augmented = augmented_train_data['Y']\n",
    "    \n",
    "    # Combine the augmented training data with the original test data\n",
    "    X_combined = pd.concat([X_train_augmented, X_test], ignore_index=True)\n",
    "    y_combined = pd.concat([y_train_augmented, y_test], ignore_index=True)\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "def generate_data_decision_tree(train_data):\n",
    "    \"\"\"Generate synthetic data using Decision Tree.\"\"\"\n",
    "    X = train_data.drop('Y', axis=1)\n",
    "    y = train_data['Y']\n",
    "    \n",
    "    dt = DecisionTreeRegressor(random_state=42)\n",
    "    dt.fit(X, y)\n",
    "    \n",
    "    synthetic_X = pd.DataFrame(columns=X.columns)\n",
    "    synthetic_y = []\n",
    "    \n",
    "    for _ in range(len(train_data)):\n",
    "        sample = X.sample(n=1, replace=True)\n",
    "        synthetic_sample = sample.copy()\n",
    "        \n",
    "        for feature in X.columns:\n",
    "            if np.random.rand() < 0.5:  # 50% chance to modify each feature\n",
    "                feature_min = X[feature].min()\n",
    "                feature_max = X[feature].max()\n",
    "                synthetic_sample[feature] = np.random.uniform(feature_min, feature_max)\n",
    "        \n",
    "        synthetic_X = pd.concat([synthetic_X, synthetic_sample], ignore_index=True)\n",
    "        synthetic_y.append(dt.predict(synthetic_sample)[0])\n",
    "    \n",
    "    synthetic_y = pd.Series(synthetic_y, name='Y')\n",
    "    \n",
    "    synthetic_data = pd.concat([synthetic_X, synthetic_y], axis=1)\n",
    "    return synthetic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def model_comparison(df, target_column, test_size=0.2, random_state=42, cv=5):\n",
    "    # Prepare the data\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Dictionary of models and their reduced hyperparameter grids\n",
    "    models = {\n",
    "        'Linear Regression': (LinearRegression(), {}),\n",
    "        'Ridge': (Ridge(), {\n",
    "            'alpha': [0.1, 1.0, 10.0],\n",
    "            'solver': ['auto', 'svd', 'cholesky']\n",
    "        }),\n",
    "        'Lasso': (Lasso(), {\n",
    "            'alpha': [0.1, 1.0, 10.0],\n",
    "            'selection': ['cyclic', 'random']\n",
    "        }),\n",
    "        'KNN': (KNeighborsRegressor(), {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]\n",
    "        }),\n",
    "        'Decision Tree': (DecisionTreeRegressor(), {\n",
    "            'max_depth': [None, 10, 20, 40],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "        }),\n",
    "        'Random Forest': (RandomForestRegressor(), {\n",
    "            'n_estimators': [100, 200, 400],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [2, 4]\n",
    "        }),\n",
    "        'Gradient Boosting': (GradientBoostingRegressor(), {\n",
    "            'n_estimators': [100, 200, 400],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4]\n",
    "        }),\n",
    "        'XGBoost': (XGBRegressor(), {\n",
    "            'n_estimators': [100, 200, 400],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4],\n",
    "            'gamma': [0, 0.1]\n",
    "        }),\n",
    "        'LightGBM': (LGBMRegressor(), {\n",
    "            'n_estimators': [100, 200, 400],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 50, 70]\n",
    "        }),\n",
    "        'CatBoost': (CatBoostRegressor(verbose=0), {\n",
    "            'iterations': [100, 200, 400],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'depth': [4, 6, 8]\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    for name, (model, param_grid) in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform GridSearchCV\n",
    "        grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Measure training time for best parameters\n",
    "        best_param_train_start = time.time()\n",
    "        best_model.fit(X_train_scaled, y_train)\n",
    "        best_param_train_time = time.time() - best_param_train_start\n",
    "\n",
    "        # Measure inference time for best parameters\n",
    "        inference_start_time = time.time()\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "        \n",
    "        # Calculate total computation time\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "        cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "        results[name] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2 Score': r2,\n",
    "            'CV Mean RMSE': np.mean(cv_rmse),\n",
    "            'CV Std RMSE': np.std(cv_rmse),\n",
    "            'Training Time (Best Params)': best_param_train_time,\n",
    "            'Inference Time (Best Params)': inference_time,\n",
    "            'Computation Time (Total)': computation_time,\n",
    "            'Best Parameters': grid_search.best_params_\n",
    "        }\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    result_df = pd.DataFrame(results).T\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def mlp_comparison(X, y, result_df, test_size=0.2, random_state=42, cv=3):\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define the MLP model and its hyperparameter grid\n",
    "    mlp = MLPRegressor(max_iter=1000, random_state=random_state)\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV with KFold\n",
    "    start_time = time.time()\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    grid_search = GridSearchCV(mlp, param_grid=param_grid, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Measure training time for best parameters\n",
    "    best_param_train_start = time.time()\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - best_param_train_start\n",
    "\n",
    "    # Measure inference time for best parameters\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "    # Store results in the existing result DataFrame\n",
    "    result_df.loc['MLP'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "    return result_df, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def dnn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1)).to(device)\n",
    "\n",
    "    # Define the DNN model\n",
    "    class DNN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dims):\n",
    "            super(DNN, self).__init__()\n",
    "            layers = []\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                input_dim = hidden_dim\n",
    "            layers.append(nn.Linear(input_dim, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        hidden_dims = [trial.suggest_int(f'hidden_dim_{i}', 32, 256) for i in range(3)]\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = DNN(input_dim, hidden_dims).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = DNN(input_dim, [best_params[f'hidden_dim_{i}'] for i in range(3)]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.values[train_index], y_train.values[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = DNN(input_dim, [best_params[f'hidden_dim_{i}'] for i in range(3)]).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['DNN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def dcn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class CrossLayer(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(CrossLayer, self).__init__()\n",
    "            self.weight = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "            self.bias = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "            nn.init.xavier_uniform_(self.weight)\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "        def forward(self, x0, x):\n",
    "            x = x.unsqueeze(2)\n",
    "            x0 = x0.unsqueeze(2)\n",
    "            interaction = torch.matmul(x0, torch.matmul(x.transpose(1, 2), self.weight))\n",
    "            return x0.squeeze(2) + interaction.squeeze(2) + self.bias.T\n",
    "\n",
    "    class DCN(nn.Module):\n",
    "        def __init__(self, input_dim, cross_layers, hidden_layers):\n",
    "            super(DCN, self).__init__()\n",
    "            self.cross_layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(cross_layers)])\n",
    "            \n",
    "            deep_layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                deep_layers.append(nn.ReLU())\n",
    "            self.deep_net = nn.Sequential(*deep_layers)\n",
    "            \n",
    "            self.final_layer = nn.Linear(input_dim + hidden_layers[-1], 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            cross_out = x\n",
    "            for layer in self.cross_layers:\n",
    "                cross_out = layer(x, cross_out)\n",
    "            deep_out = self.deep_net(x)\n",
    "            concat_out = torch.cat([cross_out, deep_out], dim=1)\n",
    "            return self.final_layer(concat_out).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        cross_layers = trial.suggest_int('cross_layers', 1, 5)\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = DCN(input_dim, cross_layers, hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = DCN(input_dim, best_params['cross_layers'], \n",
    "                     [best_params[f'hidden_layer_{i}'] for i in range(3)]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.values[train_index], y_train.values[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = DCN(input_dim, best_params['cross_layers'], \n",
    "                         [best_params[f'hidden_layer_{i}'] for i in range(3)]).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['DCN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def wide_and_deep_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class WideAndDeepNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(WideAndDeepNetwork, self).__init__()\n",
    "            \n",
    "            # Wide part\n",
    "            self.wide = nn.Linear(input_dim, 1)\n",
    "            \n",
    "            # Deep part\n",
    "            deep_layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                deep_layers.append(nn.ReLU())\n",
    "            deep_layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "            self.deep = nn.Sequential(*deep_layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            wide_out = self.wide(x)\n",
    "            deep_out = self.deep(x)\n",
    "            return wide_out + deep_out\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = WideAndDeepNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = WideAndDeepNetwork(input_dim, \n",
    "                                    [best_params[f'hidden_layer_{i}'] for i in range(3)]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy().squeeze()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.values[train_index], y_train.values[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = WideAndDeepNetwork(input_dim, \n",
    "                                        [best_params[f'hidden_layer_{i}'] for i in range(3)]).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['Wide_and_Deep'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "def xgb_nn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            layers = []\n",
    "            prev_dim = input_dim\n",
    "            for hidden_dim in hidden_layers:\n",
    "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                prev_dim = hidden_dim\n",
    "            layers.append(nn.Linear(prev_dim, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for XGBoost\n",
    "        xgb_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "        }\n",
    "\n",
    "        # Train XGBoost model\n",
    "        xgb_model = XGBRegressor(**xgb_params)\n",
    "        xgb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Extract features using XGBoost\n",
    "        X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "        X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "        X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "        y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "        \n",
    "        # Define hyperparameters to tune for Neural Network\n",
    "        hidden_layers = []\n",
    "        for i in range(3):  # Allow up to 3 hidden layers\n",
    "            if trial.suggest_categorical(f'use_hidden_layer_{i}', [True, False]):\n",
    "                hidden_layers.append(trial.suggest_int(f'hidden_layer_{i}', 32, 256))\n",
    "        \n",
    "        nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the Neural Network model\n",
    "        input_dim = X_train_transformed.shape[1]\n",
    "        model = NeuralNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_transformed_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final XGBoost model with the best hyperparameters\n",
    "    xgb_best_params = {\n",
    "        'n_estimators': best_params['n_estimators'],\n",
    "        'max_depth': best_params['max_depth'],\n",
    "        'learning_rate': best_params['xgb_learning_rate'],\n",
    "        'subsample': best_params['subsample'],\n",
    "        'colsample_bytree': best_params['colsample_bytree']\n",
    "    }\n",
    "    xgb_model = XGBRegressor(**xgb_best_params)\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Extract features using XGBoost\n",
    "    X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "    X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    # Train the final Neural Network model with the best hyperparameters\n",
    "    nn_best_params = {\n",
    "        'hidden_layers': [],\n",
    "        'learning_rate': best_params['nn_learning_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'num_epochs': best_params['num_epochs']\n",
    "    }\n",
    "\n",
    "    for i in range(3):  # Assuming max 3 hidden layers\n",
    "        if f'use_hidden_layer_{i}' in best_params and best_params[f'use_hidden_layer_{i}']:\n",
    "            nn_best_params['hidden_layers'].append(best_params[f'hidden_layer_{i}'])\n",
    "\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(nn_best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_transformed_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        xgb_model.fit(X_fold_train, y_fold_train)\n",
    "        X_fold_train_transformed = xgb_model.apply(X_fold_train)\n",
    "        X_fold_val_transformed = xgb_model.apply(X_fold_val)\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train_transformed).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val_transformed).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(nn_best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['XGBoost + NN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "\n",
    "def lgbm_nn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for LightGBM\n",
    "        lgb_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('lgb_learning_rate', 1e-4, 1e-1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "        }\n",
    "\n",
    "        # Train LightGBM model\n",
    "        lgb_model = LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Extract features using LightGBM\n",
    "        X_train_transformed = lgb_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "        X_test_transformed = lgb_model.predict(X_test_scaled).reshape(-1, 1)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "        X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "        y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "        \n",
    "        # Define hyperparameters to tune for Neural Network\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "        nn_learning_rate = trial.suggest_float('nn_learning_rate', 1e-4, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the Neural Network model\n",
    "        input_dim = X_train_transformed.shape[1]\n",
    "        model = NeuralNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_transformed_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final LightGBM model with the best hyperparameters\n",
    "    lgb_best_params = {\n",
    "        'n_estimators': best_params['n_estimators'],\n",
    "        'max_depth': best_params['max_depth'],\n",
    "        'learning_rate': best_params['lgb_learning_rate'],\n",
    "        'num_leaves': best_params['num_leaves'],\n",
    "        'subsample': best_params['subsample'],\n",
    "        'colsample_bytree': best_params['colsample_bytree']\n",
    "    }\n",
    "    lgb_model = LGBMRegressor(**lgb_best_params)\n",
    "    lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Extract features using LightGBM\n",
    "    X_train_transformed = lgb_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "    X_test_transformed = lgb_model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    # Train the final Neural Network model with the best hyperparameters\n",
    "    nn_best_params = {\n",
    "        'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "        'learning_rate': best_params['nn_learning_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'num_epochs': best_params['num_epochs']\n",
    "    }\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(nn_best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_transformed_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        lgb_model.fit(X_fold_train, y_fold_train)\n",
    "        X_fold_train_transformed = lgb_model.predict(X_fold_train).reshape(-1, 1)\n",
    "        X_fold_val_transformed = lgb_model.predict(X_fold_val).reshape(-1, 1)\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train_transformed).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val_transformed).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(nn_best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    # Store results in the existing result DataFrame\n",
    "    result_df.loc['LightGBM + NN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def autoint_nn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class AutoInt(nn.Module):\n",
    "        def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "            super(AutoInt, self).__init__()\n",
    "            self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "            self.attention_layers = nn.ModuleList([\n",
    "                nn.MultiheadAttention(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "            ])\n",
    "            self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x).unsqueeze(1)\n",
    "            for attn_layer in self.attention_layers:\n",
    "                x, _ = attn_layer(x, x, x)\n",
    "            x = x.squeeze(1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for AutoInt\n",
    "        num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "        embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        \n",
    "        # Train AutoInt model\n",
    "        autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "        optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "            autoint_model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = autoint_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Extract features using AutoInt\n",
    "        autoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "            X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "        X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "        \n",
    "        # Define hyperparameters to tune for Neural Network\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "        nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the Neural Network model\n",
    "        input_dim = X_train_transformed.shape[1]\n",
    "        model = NeuralNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_transformed_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final AutoInt model with the best hyperparameters\n",
    "    embedding_dim = best_params['embedding_dim']\n",
    "    num_heads = best_params['num_heads']\n",
    "    num_layers = best_params['num_layers']\n",
    "    autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "        autoint_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = autoint_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Extract features using AutoInt\n",
    "    autoint_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "    # Train the final Neural Network model with the best hyperparameters\n",
    "    nn_best_params = {\n",
    "        'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "        'learning_rate': best_params['nn_learning_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'num_epochs': best_params['num_epochs']\n",
    "    }\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(nn_best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_transformed_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_autoint_model = AutoInt(X_fold_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_autoint_model.parameters(), lr=0.001)\n",
    "        fold_criterion = nn.MSELoss()\n",
    "        fold_train_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_train_loader = DataLoader(fold_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "            fold_autoint_model.train()\n",
    "            for batch_X, batch_y in fold_train_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_autoint_model(batch_X)\n",
    "                loss = fold_criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_autoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_fold_train_transformed = fold_autoint_model.embedding(X_fold_train_tensor).cpu().numpy()\n",
    "            X_fold_val_transformed = fold_autoint_model.embedding(X_fold_val_tensor).cpu().numpy()\n",
    "\n",
    "        X_fold_train_transformed_tensor = torch.FloatTensor(X_fold_train_transformed).to(device)\n",
    "        X_fold_val_transformed_tensor = torch.FloatTensor(X_fold_val_transformed).to(device)\n",
    "\n",
    "        fold_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "        fold_train_dataset = TensorDataset(X_fold_train_transformed_tensor, y_fold_train_tensor)\n",
    "        fold_train_loader = DataLoader(fold_train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(nn_best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_train_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_transformed_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "       # Store results in the existing result DataFrame\n",
    "    result_df.loc['AutoInt + NN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def ft_transformer_nn_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class FTTransformer(nn.Module):\n",
    "        def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "            super(FTTransformer, self).__init__()\n",
    "            self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "            self.transformer_layers = nn.ModuleList([\n",
    "                nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n",
    "            ])\n",
    "            self.fc = nn.Linear(embedding_dim, input_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x).unsqueeze(1)\n",
    "            for transformer_layer in self.transformer_layers:\n",
    "                x = transformer_layer(x)\n",
    "            x = x.squeeze(1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for FT-Transformer\n",
    "        num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "        embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        \n",
    "        # Train FT-Transformer model\n",
    "        ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "        optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "            ft_transformer_model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = ft_transformer_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Extract features using FT-Transformer\n",
    "        ft_transformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "            X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "        X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "        \n",
    "        # Define hyperparameters to tune for Neural Network\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "        nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the Neural Network model\n",
    "        input_dim = X_train_transformed.shape[1]\n",
    "        model = NeuralNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_transformed_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final FT-Transformer model with the best hyperparameters\n",
    "    embedding_dim = best_params['embedding_dim']\n",
    "    num_heads = best_params['num_heads']\n",
    "    num_layers = best_params['num_layers']\n",
    "    ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "        ft_transformer_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ft_transformer_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Extract features using FT-Transformer\n",
    "    ft_transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "    # Train the final Neural Network model with the best hyperparameters\n",
    "    nn_best_params = {\n",
    "        'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "        'learning_rate': best_params['nn_learning_rate'],\n",
    "        'batch_size': best_params['batch_size'],\n",
    "        'num_epochs': best_params['num_epochs']\n",
    "    }\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(nn_best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_transformed_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_ft_transformer_model = FTTransformer(X_fold_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_ft_transformer_model.parameters(), lr=0.001)\n",
    "        fold_criterion = nn.MSELoss()\n",
    "\n",
    "        fold_train_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_train_loader = DataLoader(fold_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            fold_ft_transformer_model.train()\n",
    "            for batch_X, batch_y in fold_train_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_ft_transformer_model(batch_X)\n",
    "                loss = fold_criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_ft_transformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_fold_train_transformed = fold_ft_transformer_model.embedding(X_fold_train_tensor).cpu().numpy()\n",
    "            X_fold_val_transformed = fold_ft_transformer_model.embedding(X_fold_val_tensor).cpu().numpy()\n",
    "\n",
    "        X_fold_train_transformed_tensor = torch.FloatTensor(X_fold_train_transformed).to(device)\n",
    "        X_fold_val_transformed_tensor = torch.FloatTensor(X_fold_val_transformed).to(device)\n",
    "\n",
    "        fold_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "        fold_train_dataset = TensorDataset(X_fold_train_transformed_tensor, y_fold_train_tensor)\n",
    "        fold_train_loader = DataLoader(fold_train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(nn_best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_train_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_transformed_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    cv_mean_rmse = np.mean(cv_rmse)\n",
    "    cv_std_rmse = np.std(cv_rmse)\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    # Store results in the existing result DataFrame\n",
    "    result_df.loc['FT-Transformer'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': cv_mean_rmse,\n",
    "        'CV Std RMSE': cv_std_rmse,\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def neural_architecture_search(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            layers = []\n",
    "            for i in range(len(hidden_layers)):\n",
    "                if i == 0:\n",
    "                    layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for Neural Network\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "        hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(num_layers)]\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the Neural Network model\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = NeuralNetwork(input_dim, hidden_layers).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final Neural Network model with the best hyperparameters\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = NeuralNetwork(input_dim, \n",
    "                               [best_params[f'hidden_layer_{i}'] for i in range(best_params['num_layers'])]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = NeuralNetwork(input_dim, \n",
    "                                   [best_params[f'hidden_layer_{i}'] for i in range(best_params['num_layers'])]).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    cv_mean_rmse = np.mean(cv_rmse)\n",
    "    cv_std_rmse = np.std(cv_rmse)\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    # Store results in the existing result DataFrame\n",
    "    result_df.loc['Neural Architecture Search'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': cv_mean_rmse,\n",
    "        'CV Std RMSE': cv_std_rmse,\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def kan_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class KAN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim):\n",
    "            super(KAN, self).__init__()\n",
    "            self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "            self.activation = nn.ReLU()\n",
    "            self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h = self.activation(self.hidden_layer(x))\n",
    "            out = self.output_layer(h)\n",
    "            return out.squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 32, 256)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = KAN(input_dim, hidden_dim).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = KAN(input_dim, best_params['hidden_dim']).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = KAN(input_dim, best_params['hidden_dim']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['KAN'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def node_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=10, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class NODE(nn.Module):\n",
    "        def __init__(self, input_dim, num_layers, num_trees, tree_dim):\n",
    "            super(NODE, self).__init__()\n",
    "            self.layers = nn.ModuleList()\n",
    "            for _ in range(num_layers):\n",
    "                layer = nn.ModuleList()\n",
    "                for _ in range(num_trees):\n",
    "                    tree = nn.Sequential(\n",
    "                        nn.Linear(input_dim, tree_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(tree_dim, 1)\n",
    "                    )\n",
    "                    layer.append(tree)\n",
    "                self.layers.append(layer)\n",
    "            self.output = nn.Linear(num_layers * num_trees, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            tree_outputs = []\n",
    "            for layer in self.layers:\n",
    "                layer_outputs = []\n",
    "                for tree in layer:\n",
    "                    layer_outputs.append(tree(x))\n",
    "                layer_output = torch.cat(layer_outputs, dim=1)\n",
    "                tree_outputs.append(layer_output)\n",
    "            x = torch.cat(tree_outputs, dim=1)\n",
    "            return self.output(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for NODE\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "        num_trees = trial.suggest_int('num_trees', 1, 10)\n",
    "        tree_dim = trial.suggest_int('tree_dim', 8, 64)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "        # Create the NODE model\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = NODE(input_dim, num_layers, num_trees, tree_dim).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final NODE model with the best hyperparameters\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = NODE(input_dim, \n",
    "                      best_params['num_layers'], \n",
    "                      best_params['num_trees'], \n",
    "                      best_params['tree_dim']).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        \n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = NODE(input_dim, \n",
    "                          best_params['num_layers'], \n",
    "                          best_params['num_trees'], \n",
    "                          best_params['tree_dim']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['NODE'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': np.mean(cv_rmse),\n",
    "        'CV Std RMSE': np.std(cv_rmse),\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "\n",
    "def tabnet_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=5, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Reshape y to be 2D\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune for TabNet\n",
    "        n_d = trial.suggest_int('n_d', 8, 64)\n",
    "        n_a = trial.suggest_int('n_a', 8, 64)\n",
    "        n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "        gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "        lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128,256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 40)\n",
    "\n",
    "        # Create the TabNet model\n",
    "        model = TabNetRegressor(\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=learning_rate),\n",
    "            device_name=device\n",
    "        )\n",
    "\n",
    "        # Training\n",
    "        model.fit(\n",
    "            X_train=X_train_scaled, y_train=y_train,\n",
    "            eval_set=[(X_test_scaled, y_test)],\n",
    "            eval_name=['val'],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=num_epochs,\n",
    "            patience=10,\n",
    "            batch_size=batch_size,\n",
    "            virtual_batch_size=batch_size // 2,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Train the final TabNet model with the best hyperparameters\n",
    "    best_model = TabNetRegressor(\n",
    "        n_d=best_params['n_d'],\n",
    "        n_a=best_params['n_a'],\n",
    "        n_steps=best_params['n_steps'],\n",
    "        gamma=best_params['gamma'],\n",
    "        lambda_sparse=best_params['lambda_sparse'],\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "        device_name=device\n",
    "    )\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    best_model.fit(\n",
    "        X_train=X_train_scaled, y_train=y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=['rmse'],\n",
    "        max_epochs=best_params['num_epochs'],\n",
    "        patience=10,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        virtual_batch_size=best_params['batch_size'] // 2,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    # Evaluation\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        fold_model = TabNetRegressor(\n",
    "            n_d=best_params['n_d'],\n",
    "            n_a=best_params['n_a'],\n",
    "            n_steps=best_params['n_steps'],\n",
    "            gamma=best_params['gamma'],\n",
    "            lambda_sparse=best_params['lambda_sparse'],\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "            device_name=device\n",
    "        )\n",
    "\n",
    "        fold_model.fit(\n",
    "            X_train=X_fold_train, y_train=y_fold_train,\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\n",
    "            eval_name=['val'],\n",
    "            eval_metric=['rmse'],\n",
    "            max_epochs=best_params['num_epochs'],\n",
    "            patience=10,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            virtual_batch_size=best_params['batch_size'] // 2,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        y_fold_pred = fold_model.predict(X_fold_val)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_fold_val, y_fold_pred))\n",
    "        cv_rmse.append(fold_rmse)\n",
    "\n",
    "    cv_mean_rmse = np.mean(cv_rmse)\n",
    "    cv_std_rmse = np.std(cv_rmse)\n",
    "\n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    # Store results in the existing result DataFrame\n",
    "    result_df.loc['TabNet'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': cv_mean_rmse,\n",
    "        'CV Std RMSE': cv_std_rmse,\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "\n",
    "def saint_comparison(X, y, result_df, test_size=0.2, random_state=42, n_trials=5, n_folds=5):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Ensure X and y are numpy arrays\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)\n",
    "\n",
    "    class SAINT(nn.Module):\n",
    "        def __init__(self, input_dim, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "            super(SAINT, self).__init__()\n",
    "            self.embeds = nn.Linear(input_dim, dim)\n",
    "            self.transformer = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "                num_layers=depth\n",
    "            )\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embeds(x)\n",
    "            x = x.unsqueeze(1)  # Add sequence dimension\n",
    "            x = self.transformer(x)\n",
    "            x = x.squeeze(1)  # Remove sequence dimension\n",
    "            return self.mlp_head(x).squeeze()\n",
    "\n",
    "    def objective(trial):\n",
    "        heads = trial.suggest_int('heads', 1, 8)\n",
    "        dim = trial.suggest_int('dim', heads, 256, step=heads)\n",
    "        depth = trial.suggest_int('depth', 1, 6)\n",
    "        mlp_dim = trial.suggest_int('mlp_dim', 32, 256)\n",
    "        dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128,256])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 10, 40)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = SAINT(input_dim, dim, depth, heads, mlp_dim, dropout).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)\n",
    "            mse = mean_squared_error(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "\n",
    "        return mse\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    best_model = SAINT(input_dim, best_params['dim'], best_params['depth'], \n",
    "                       best_params['heads'], best_params['mlp_dim'], \n",
    "                       best_params['dropout']).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        best_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = best_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    training_time = time.time() - training_start_time\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        inference_start_time = time.time()\n",
    "        predictions = best_model(X_test_tensor)\n",
    "        inference_time = time.time() - inference_start_time\n",
    "\n",
    "        y_true = y_test_tensor.cpu().numpy().squeeze()\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    cv_rmse = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        X_fold_train, X_fold_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        X_fold_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "        y_fold_train_tensor = torch.FloatTensor(y_fold_train.reshape(-1, 1)).to(device)\n",
    "        X_fold_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "        y_fold_val_tensor = torch.FloatTensor(y_fold_val.reshape(-1, 1)).to(device)\n",
    "\n",
    "        fold_model = SAINT(input_dim, best_params['dim'], best_params['depth'], \n",
    "                           best_params['heads'], best_params['mlp_dim'], \n",
    "                           best_params['dropout']).to(device)\n",
    "        fold_optimizer = optim.Adam(fold_model.parameters(), lr=best_params['learning_rate'])\n",
    "        fold_dataset = TensorDataset(X_fold_train_tensor, y_fold_train_tensor)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "        for epoch in range(best_params['num_epochs']):\n",
    "            fold_model.train()\n",
    "            for batch_X, batch_y in fold_loader:\n",
    "                fold_optimizer.zero_grad()\n",
    "                outputs = fold_model(batch_X)\n",
    "                loss = criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                fold_optimizer.step()\n",
    "\n",
    "        fold_model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_predictions = fold_model(X_fold_val_tensor)\n",
    "            fold_mse = mean_squared_error(y_fold_val_tensor.cpu().numpy(), fold_predictions.cpu().numpy())\n",
    "            cv_rmse.append(np.sqrt(fold_mse))\n",
    "\n",
    "    cv_mean_rmse = np.mean(cv_rmse)\n",
    "    cv_std_rmse = np.std(cv_rmse)\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    result_df.loc['SAINT'] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2 Score': r2,\n",
    "        'CV Mean RMSE': cv_mean_rmse,\n",
    "        'CV Std RMSE': cv_std_rmse,\n",
    "        'Training Time (Best Params)': training_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': best_params\n",
    "    }\n",
    "\n",
    "    return result_df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:41:11,568] A new study created in memory with name: no-name-3a6281e6-829b-47bc-8554-cd5dcd1da1fa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 10:41:26,496] Trial 0 finished with value: 11205650.0 and parameters: {'heads': 5, 'dim': 65, 'depth': 5, 'mlp_dim': 160, 'dropout': 0.11698618521862919, 'learning_rate': 0.001180545242414642, 'batch_size': 64, 'num_epochs': 17}. Best is trial 0 with value: 11205650.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 10:41:32,179] Trial 1 finished with value: 5076915.0 and parameters: {'heads': 2, 'dim': 70, 'depth': 2, 'mlp_dim': 233, 'dropout': 0.03905578596842341, 'learning_rate': 0.0261447175860303, 'batch_size': 64, 'num_epochs': 15}. Best is trial 1 with value: 5076915.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 10:41:49,673] Trial 2 finished with value: 10311736.0 and parameters: {'heads': 2, 'dim': 44, 'depth': 4, 'mlp_dim': 179, 'dropout': 0.43043403669016667, 'learning_rate': 0.0014926498458922672, 'batch_size': 64, 'num_epochs': 37}. Best is trial 1 with value: 5076915.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 10:42:03,597] Trial 3 finished with value: 11449521.0 and parameters: {'heads': 2, 'dim': 34, 'depth': 2, 'mlp_dim': 247, 'dropout': 0.21919296499647567, 'learning_rate': 0.00019203543040973586, 'batch_size': 32, 'num_epochs': 32}. Best is trial 1 with value: 5076915.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 10:42:33,221] Trial 4 finished with value: 11406723.0 and parameters: {'heads': 7, 'dim': 245, 'depth': 6, 'mlp_dim': 205, 'dropout': 0.22116484025828392, 'learning_rate': 0.00020304746251324158, 'batch_size': 128, 'num_epochs': 30}. Best is trial 1 with value: 5076915.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       MSE         RMSE          MAE  \\\n",
      "Linear Regression           4889229.831828  2211.160291  1691.734253   \n",
      "Ridge                        4883088.90566  2209.771234  1690.705258   \n",
      "Lasso                       4865416.912044  2205.769007  1688.270692   \n",
      "KNN                          3273818.03677  1809.369514  1027.592047   \n",
      "Decision Tree               8273080.036686  2876.296236  1650.821209   \n",
      "Random Forest               3824053.360395  1955.518693  1397.043044   \n",
      "Gradient Boosting           4477412.204499  2115.989651   1623.35943   \n",
      "XGBoost                     4467102.980339  2113.552219  1630.558773   \n",
      "LightGBM                     3659382.94254   1912.95137   1404.21618   \n",
      "CatBoost                    4676739.350386  2162.577016  1655.313719   \n",
      "MLP                         4847092.507387  2201.611343  1687.336878   \n",
      "DNN                              5001051.5  2236.302979  1701.878906   \n",
      "DCN                              4962697.5  2227.711182  1694.133423   \n",
      "Wide_and_Deep                    5024893.5  2241.627441  1726.643921   \n",
      "XGBoost + NN                     5003418.5  2236.832275   1699.18103   \n",
      "LightGBM + NN                    4943231.5  2223.337891  1831.526611   \n",
      "AutoInt + NN                     4914299.5  2216.822021  1664.519165   \n",
      "FT-Transformer                   5081804.0  2254.285645  1757.715576   \n",
      "Neural Architecture Search       5049299.5  2247.064697  1732.337769   \n",
      "KAN                              4979502.5   2231.47998  1686.365723   \n",
      "NODE                             4894695.5  2212.395752  1684.882568   \n",
      "TabNet                      4542283.464429  2131.263349  1581.528515   \n",
      "SAINT                            5054068.0  2248.125488  1728.179077   \n",
      "\n",
      "                            R2 Score CV Mean RMSE  CV Std RMSE  \\\n",
      "Linear Regression           0.028526  3706.382967  2214.571881   \n",
      "Ridge                       0.029746   3704.68848  2215.887314   \n",
      "Lasso                       0.033257  3702.626245   2217.52555   \n",
      "KNN                         0.349503  3671.766602  2149.877918   \n",
      "Decision Tree              -0.643835  4119.649355  1919.444916   \n",
      "Random Forest               0.240173  3705.281842   2161.06866   \n",
      "Gradient Boosting           0.110353  4404.560551   2091.07182   \n",
      "XGBoost                     0.112401  3771.938466  2152.957365   \n",
      "LightGBM                    0.272892  3607.297061   2213.85462   \n",
      "CatBoost                    0.070747  3678.684169  2231.898633   \n",
      "MLP                         0.036898  3846.915694  1933.739801   \n",
      "DNN                         0.006307  3757.086426  2157.006104   \n",
      "DCN                         0.013928  3755.920654  2120.147705   \n",
      "Wide_and_Deep                0.00157  3790.101074  2103.186279   \n",
      "XGBoost + NN                0.005837   3757.13623  2155.943359   \n",
      "LightGBM + NN               0.017796  3733.903076  2159.670898   \n",
      "AutoInt + NN                0.023544  3766.617188   2100.22168   \n",
      "FT-Transformer             -0.009738  3785.286377   2177.21875   \n",
      "Neural Architecture Search  -0.00328  3757.803223  2154.411621   \n",
      "KAN                         0.010589  3746.231201  2134.232178   \n",
      "NODE                         0.02744  3745.731689  2127.584229   \n",
      "TabNet                      0.097463  3679.181224  2146.387427   \n",
      "SAINT                      -0.004227  3762.380371  2162.302002   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.000998   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000998   \n",
      "KNN                                            0.00399   \n",
      "Decision Tree                                 0.007979   \n",
      "Random Forest                                 0.669213   \n",
      "Gradient Boosting                                0.364   \n",
      "XGBoost                                       0.050864   \n",
      "LightGBM                                      0.372005   \n",
      "CatBoost                                      0.609065   \n",
      "MLP                                           5.618087   \n",
      "DNN                                          16.229602   \n",
      "DCN                                           9.435766   \n",
      "Wide_and_Deep                                 3.437807   \n",
      "XGBoost + NN                                   1.82612   \n",
      "LightGBM + NN                                17.156126   \n",
      "AutoInt + NN                                  0.627318   \n",
      "FT-Transformer                               15.947355   \n",
      "Neural Architecture Search                   39.009685   \n",
      "KAN                                           2.229039   \n",
      "NODE                                         44.683588   \n",
      "TabNet                                       37.125721   \n",
      "SAINT                                         4.621641   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                          0.000996   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002992   \n",
      "Decision Tree                                  0.000999   \n",
      "Random Forest                                   0.01198   \n",
      "Gradient Boosting                              0.000993   \n",
      "XGBoost                                           0.001   \n",
      "LightGBM                                       0.009972   \n",
      "CatBoost                                       0.001995   \n",
      "MLP                                            0.000996   \n",
      "DNN                                            0.000998   \n",
      "DCN                                            0.001995   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                        0.0   \n",
      "LightGBM + NN                                  0.001993   \n",
      "AutoInt + NN                                   0.001996   \n",
      "FT-Transformer                                 0.001995   \n",
      "Neural Architecture Search                     0.001994   \n",
      "KAN                                            0.000996   \n",
      "NODE                                           0.001962   \n",
      "TabNet                                         0.195477   \n",
      "SAINT                                          0.011967   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                          3.994287   \n",
      "Ridge                                      0.087765   \n",
      "Lasso                                      0.065825   \n",
      "KNN                                        0.210437   \n",
      "Decision Tree                              0.749995   \n",
      "Random Forest                            137.508334   \n",
      "Gradient Boosting                         30.281713   \n",
      "XGBoost                                   10.962873   \n",
      "LightGBM                                  40.894349   \n",
      "CatBoost                                  91.662034   \n",
      "MLP                                      335.849844   \n",
      "DNN                                       128.96015   \n",
      "DCN                                      103.942046   \n",
      "Wide_and_Deep                             93.690464   \n",
      "XGBoost + NN                              42.136851   \n",
      "LightGBM + NN                            156.275112   \n",
      "AutoInt + NN                             128.207163   \n",
      "FT-Transformer                           289.818001   \n",
      "Neural Architecture Search               282.551433   \n",
      "KAN                                       29.836217   \n",
      "NODE                                     323.902909   \n",
      "TabNet                                   368.841685   \n",
      "SAINT                                    104.555547   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                       {'alpha': 10.0, 'solver': 'auto'}  \n",
      "Lasso                                  {'alpha': 10.0, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 10, 'min_samples_leaf': 4, 'min_...  \n",
      "Random Forest               {'max_depth': None, 'min_samples_leaf': 4, 'mi...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.01, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 400, '...  \n",
      "CatBoost                    {'depth': 8, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 83, 'hidden_dim_1': 92, 'hidd...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 160, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 224, 'hidden_layer_1': 44, ...  \n",
      "XGBoost + NN                {'n_estimators': 260, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 231, 'max_depth': 4, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 5, 'embedding_dim': 25, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 246, 'hidd...  \n",
      "KAN                         {'hidden_dim': 222, 'learning_rate': 0.0058305...  \n",
      "NODE                        {'num_layers': 3, 'num_trees': 8, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 53, 'n_a': 18, 'n_steps': 5, 'gamma': ...  \n",
      "SAINT                       {'heads': 2, 'dim': 70, 'depth': 2, 'mlp_dim':...  \n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"crop\"  # Change this to any word you like\n",
    "df =  pd.read_csv(f'Dataset/{file_prefix}.csv')\n",
    "df = encode_categorical_data(df)\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X, y = apply_robust_transform(X, y)\n",
    "#X, y = apply_synthetic_data_to_training(X, y)\n",
    "'''\n",
    "result = model_comparison(df, 'Y')\n",
    "print(result)\n",
    "result, best_params = mlp_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = dnn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = dcn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = wide_and_deep_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = xgb_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = lgbm_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = autoint_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = ft_transformer_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = neural_architecture_search(X, y, result)\n",
    "print(result)\n",
    "result, best_params = kan_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = node_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = tabnet_comparison(X, y, result)\n",
    "print(result)\n",
    "'''\n",
    "result, best_params = saint_comparison(X, y, result)\n",
    "print(result)\n",
    "\n",
    "result.to_csv(f'result/comparison/regression/{file_prefix}_result.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4558.826552\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1277\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4594.713062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4513.565310\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4441.927195\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 468, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4622.611111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                             MSE        RMSE         MAE  R2 Score  \\\n",
      "Linear Regression            0.0         0.0         0.0       1.0   \n",
      "Ridge                   0.242336    0.492277    0.398738       1.0   \n",
      "Lasso                   0.013676    0.116946    0.096502       1.0   \n",
      "KNN                161030.811799  401.286446  296.465121  0.952954   \n",
      "Decision Tree       30628.274734  175.009356  123.978311  0.991052   \n",
      "Random Forest       11178.686815  105.729309   65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997   92.081176     64.2453  0.997523   \n",
      "XGBoost              9031.190737   95.032577    66.63099  0.997361   \n",
      "LightGBM            12274.312832  110.789498   72.435986  0.996414   \n",
      "CatBoost             6256.066985   79.095303   61.673092  0.998172   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "60 fits failed out of a total of 288.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 751, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 495, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-14260898.7179361                 nan -14260898.7179361\n",
      "                nan  -9105406.86116601                nan\n",
      "  -9105406.86116601                nan   -247880.12928471\n",
      "                nan   -247880.12928471                nan\n",
      "   -167763.81117573                nan   -167763.81117573\n",
      "                nan -14260898.71953649                nan\n",
      " -14260898.71953649                nan  -9105452.6182227\n",
      "                nan  -9105452.6182227                 nan\n",
      "   -247915.15733612                nan   -247915.15733612\n",
      "                nan   -171668.15141333                nan\n",
      "   -171668.15141333                nan -14260898.58451387\n",
      "                nan -14260898.58451387                nan\n",
      "  -9105566.72050911                nan  -9105566.72050911\n",
      "                nan   -247692.07364877                nan\n",
      "   -247692.07364877                nan   -167576.1246796\n",
      "                nan   -167576.1246796                 nan\n",
      " -23353664.46108461   -233422.83949926 -23353664.46108461\n",
      "   -234951.34617794 -22299510.35528171   -168558.36677589\n",
      " -22299510.35528171   -166799.9792083  -23437507.31298353\n",
      "   -988267.22043895 -23437507.31298353   -914818.33142761\n",
      " -23453388.17454177   -913445.12603079 -23453388.17454177\n",
      "   -679894.30573094 -23353664.46160072   -225712.86241999\n",
      " -23353664.46160072   -217043.81466713 -22299510.35628885\n",
      "   -157716.4235915  -22299510.35628885   -154165.14521521\n",
      " -23437507.31342061  -1132854.41997735 -23437507.31342061\n",
      "   -866477.67029452 -23453388.1749566    -602491.59997202\n",
      " -23453388.1749566    -539379.52347267 -23353664.46676186\n",
      "   -215740.49157928 -23353664.46676186   -221620.64603752\n",
      " -22299510.36636023   -166216.08982889 -22299510.36636023\n",
      "   -169066.93690107 -23437507.31779141   -958564.31698986\n",
      " -23437507.31779141   -826960.54241805 -23453388.17910483\n",
      "   -579340.62112369 -23453388.17910483   -524104.62880496]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2024-08-06 10:48:31,157] A new study created in memory with name: no-name-e5777466-2679-4f93-8008-924a0862fb40\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE        RMSE         MAE  R2 Score  \\\n",
      "Linear Regression            0.0         0.0         0.0       1.0   \n",
      "Ridge                   0.242336    0.492277    0.398738       1.0   \n",
      "Lasso                   0.013676    0.116946    0.096502       1.0   \n",
      "KNN                161030.811799  401.286446  296.465121  0.952954   \n",
      "Decision Tree       30628.274734  175.009356  123.978311  0.991052   \n",
      "Random Forest       11178.686815  105.729309   65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997   92.081176     64.2453  0.997523   \n",
      "XGBoost              9031.190737   95.032577    66.63099  0.997361   \n",
      "LightGBM            12274.312832  110.789498   72.435986  0.996414   \n",
      "CatBoost             6256.066985   79.095303   61.673092  0.998172   \n",
      "MLP                112512.623433  335.429014  264.401499  0.967129   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:48:33,429] Trial 0 finished with value: 3675501.5 and parameters: {'hidden_dim_0': 252, 'hidden_dim_1': 241, 'hidden_dim_2': 33, 'learning_rate': 0.03535070946595991, 'batch_size': 256, 'num_epochs': 82}. Best is trial 0 with value: 3675501.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:34,297] Trial 1 finished with value: 22352352.0 and parameters: {'hidden_dim_0': 102, 'hidden_dim_1': 167, 'hidden_dim_2': 45, 'learning_rate': 0.00011294308168232602, 'batch_size': 256, 'num_epochs': 39}. Best is trial 0 with value: 3675501.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:39,506] Trial 2 finished with value: 3342298.25 and parameters: {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hidden_dim_2': 194, 'learning_rate': 0.06586621440068076, 'batch_size': 64, 'num_epochs': 96}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:39,780] Trial 3 finished with value: 22359632.0 and parameters: {'hidden_dim_0': 256, 'hidden_dim_1': 96, 'hidden_dim_2': 252, 'learning_rate': 0.0003537173062802281, 'batch_size': 256, 'num_epochs': 10}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:42,108] Trial 4 finished with value: 4425188.0 and parameters: {'hidden_dim_0': 56, 'hidden_dim_1': 228, 'hidden_dim_2': 87, 'learning_rate': 0.0002306562788276197, 'batch_size': 32, 'num_epochs': 31}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:42,789] Trial 5 finished with value: 21298552.0 and parameters: {'hidden_dim_0': 215, 'hidden_dim_1': 206, 'hidden_dim_2': 53, 'learning_rate': 0.0004939712355531866, 'batch_size': 256, 'num_epochs': 27}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:44,479] Trial 6 finished with value: 3591634.75 and parameters: {'hidden_dim_0': 52, 'hidden_dim_1': 189, 'hidden_dim_2': 79, 'learning_rate': 0.013935354219249765, 'batch_size': 256, 'num_epochs': 79}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:49,272] Trial 7 finished with value: 4789826.0 and parameters: {'hidden_dim_0': 77, 'hidden_dim_1': 105, 'hidden_dim_2': 226, 'learning_rate': 0.059321898592037806, 'batch_size': 64, 'num_epochs': 98}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:50,547] Trial 8 finished with value: 3929431.25 and parameters: {'hidden_dim_0': 144, 'hidden_dim_1': 85, 'hidden_dim_2': 63, 'learning_rate': 0.002130308275250484, 'batch_size': 256, 'num_epochs': 61}. Best is trial 2 with value: 3342298.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3425235463.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:48:51,903] Trial 9 finished with value: 3787443.25 and parameters: {'hidden_dim_0': 153, 'hidden_dim_1': 230, 'hidden_dim_2': 216, 'learning_rate': 0.03165897951078669, 'batch_size': 128, 'num_epochs': 34}. Best is trial 2 with value: 3342298.25.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:49:16,081] A new study created in memory with name: no-name-29053b42-14c4-425b-bbc5-fc657fbf8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:18,448] Trial 0 finished with value: 38926.41015625 and parameters: {'cross_layers': 2, 'hidden_layer_0': 149, 'hidden_layer_1': 226, 'hidden_layer_2': 249, 'learning_rate': 0.01224087062035887, 'batch_size': 64, 'num_epochs': 36}. Best is trial 0 with value: 38926.41015625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:28,280] Trial 1 finished with value: 18221.07421875 and parameters: {'cross_layers': 4, 'hidden_layer_0': 139, 'hidden_layer_1': 125, 'hidden_layer_2': 75, 'learning_rate': 0.004134234444134273, 'batch_size': 32, 'num_epochs': 81}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:31,585] Trial 2 finished with value: 163052.84375 and parameters: {'cross_layers': 3, 'hidden_layer_0': 219, 'hidden_layer_1': 160, 'hidden_layer_2': 71, 'learning_rate': 0.0011920539742262027, 'batch_size': 64, 'num_epochs': 51}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:36,307] Trial 3 finished with value: 1159445.5 and parameters: {'cross_layers': 4, 'hidden_layer_0': 232, 'hidden_layer_1': 161, 'hidden_layer_2': 139, 'learning_rate': 0.00014101871570013205, 'batch_size': 64, 'num_epochs': 63}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:40,837] Trial 4 finished with value: 34029.28515625 and parameters: {'cross_layers': 4, 'hidden_layer_0': 89, 'hidden_layer_1': 46, 'hidden_layer_2': 36, 'learning_rate': 0.011514907220876437, 'batch_size': 64, 'num_epochs': 76}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:42,166] Trial 5 finished with value: 72682.5859375 and parameters: {'cross_layers': 4, 'hidden_layer_0': 252, 'hidden_layer_1': 156, 'hidden_layer_2': 225, 'learning_rate': 0.06074967770488853, 'batch_size': 256, 'num_epochs': 39}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:42,627] Trial 6 finished with value: 22323544.0 and parameters: {'cross_layers': 5, 'hidden_layer_0': 47, 'hidden_layer_1': 239, 'hidden_layer_2': 63, 'learning_rate': 0.0006220270986632936, 'batch_size': 256, 'num_epochs': 14}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:45,647] Trial 7 finished with value: 110519.8359375 and parameters: {'cross_layers': 4, 'hidden_layer_0': 195, 'hidden_layer_1': 135, 'hidden_layer_2': 244, 'learning_rate': 0.0015437794207612263, 'batch_size': 32, 'num_epochs': 23}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:46,931] Trial 8 finished with value: 41269.71875 and parameters: {'cross_layers': 1, 'hidden_layer_0': 225, 'hidden_layer_1': 81, 'hidden_layer_2': 49, 'learning_rate': 0.021753921542761023, 'batch_size': 32, 'num_epochs': 15}. Best is trial 1 with value: 18221.07421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1138509370.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:49:52,191] Trial 9 finished with value: 8306.181640625 and parameters: {'cross_layers': 1, 'hidden_layer_0': 141, 'hidden_layer_1': 53, 'hidden_layer_2': 214, 'learning_rate': 0.009262384477344659, 'batch_size': 32, 'num_epochs': 61}. Best is trial 9 with value: 8306.181640625.\n",
      "[I 2024-08-06 10:50:17,276] A new study created in memory with name: no-name-425cf2df-38ef-487c-a974-1b08f702bb2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:19,062] Trial 0 finished with value: 312861.40625 and parameters: {'hidden_layer_0': 163, 'hidden_layer_1': 109, 'hidden_layer_2': 204, 'learning_rate': 0.0015575770976062023, 'batch_size': 256, 'num_epochs': 68}. Best is trial 0 with value: 312861.40625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:20,255] Trial 1 finished with value: 39647.0546875 and parameters: {'hidden_layer_0': 96, 'hidden_layer_1': 237, 'hidden_layer_2': 127, 'learning_rate': 0.04651181970325781, 'batch_size': 256, 'num_epochs': 44}. Best is trial 1 with value: 39647.0546875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:22,162] Trial 2 finished with value: 18973.21484375 and parameters: {'hidden_layer_0': 160, 'hidden_layer_1': 213, 'hidden_layer_2': 167, 'learning_rate': 0.019265541176803028, 'batch_size': 64, 'num_epochs': 34}. Best is trial 2 with value: 18973.21484375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:28,080] Trial 3 finished with value: 5720.16455078125 and parameters: {'hidden_layer_0': 176, 'hidden_layer_1': 219, 'hidden_layer_2': 143, 'learning_rate': 0.013637618458216445, 'batch_size': 64, 'num_epochs': 93}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:31,052] Trial 4 finished with value: 580612.8125 and parameters: {'hidden_layer_0': 111, 'hidden_layer_1': 155, 'hidden_layer_2': 194, 'learning_rate': 0.0002642913530093444, 'batch_size': 32, 'num_epochs': 34}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:35,098] Trial 5 finished with value: 90764.8359375 and parameters: {'hidden_layer_0': 58, 'hidden_layer_1': 172, 'hidden_layer_2': 170, 'learning_rate': 0.0017573017101917244, 'batch_size': 64, 'num_epochs': 78}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:37,043] Trial 6 finished with value: 21398792.0 and parameters: {'hidden_layer_0': 44, 'hidden_layer_1': 68, 'hidden_layer_2': 184, 'learning_rate': 0.00010117579117593487, 'batch_size': 64, 'num_epochs': 45}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:39,503] Trial 7 finished with value: 281731.40625 and parameters: {'hidden_layer_0': 170, 'hidden_layer_1': 65, 'hidden_layer_2': 98, 'learning_rate': 0.0010022576784002345, 'batch_size': 128, 'num_epochs': 82}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:40,133] Trial 8 finished with value: 126153.890625 and parameters: {'hidden_layer_0': 235, 'hidden_layer_1': 230, 'hidden_layer_2': 185, 'learning_rate': 0.043134528873649436, 'batch_size': 256, 'num_epochs': 21}. Best is trial 3 with value: 5720.16455078125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2047103876.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:50:42,484] Trial 9 finished with value: 29748.94921875 and parameters: {'hidden_layer_0': 217, 'hidden_layer_1': 169, 'hidden_layer_2': 104, 'learning_rate': 0.010252810177535302, 'batch_size': 64, 'num_epochs': 44}. Best is trial 3 with value: 5720.16455078125.\n",
      "[I 2024-08-06 10:51:09,372] A new study created in memory with name: no-name-c3f15e5a-d5bc-4916-a5f1-a8f187e2b3a1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep       11511.054688   107.289581     75.51059  0.996637   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "Wide_and_Deep       148.798538   41.887012                    5.563657   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "Wide_and_Deep                         0.000997                52.083259   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep      {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:10,619] Trial 0 finished with value: 9889692.0 and parameters: {'n_estimators': 127, 'max_depth': 8, 'xgb_learning_rate': 0.011959792063634055, 'subsample': 0.7244652389389996, 'colsample_bytree': 0.8689664654973686, 'use_hidden_layer_0': False, 'use_hidden_layer_1': True, 'hidden_layer_1': 147, 'use_hidden_layer_2': True, 'hidden_layer_2': 136, 'nn_learning_rate': 0.00011287203413641882, 'batch_size': 128, 'num_epochs': 32}. Best is trial 0 with value: 9889692.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:12,447] Trial 1 finished with value: 3119764.5 and parameters: {'n_estimators': 145, 'max_depth': 8, 'xgb_learning_rate': 0.0002554609990725687, 'subsample': 0.6709658127003636, 'colsample_bytree': 0.543379309685084, 'use_hidden_layer_0': False, 'use_hidden_layer_1': True, 'hidden_layer_1': 172, 'use_hidden_layer_2': True, 'hidden_layer_2': 234, 'nn_learning_rate': 0.0012180024711999772, 'batch_size': 256, 'num_epochs': 73}. Best is trial 1 with value: 3119764.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:15,691] Trial 2 finished with value: 3112031.25 and parameters: {'n_estimators': 184, 'max_depth': 6, 'xgb_learning_rate': 0.0002204254271994393, 'subsample': 0.8457931806424082, 'colsample_bytree': 0.6989469208873613, 'use_hidden_layer_0': False, 'use_hidden_layer_1': False, 'use_hidden_layer_2': True, 'hidden_layer_2': 79, 'nn_learning_rate': 0.006424307132399626, 'batch_size': 64, 'num_epochs': 98}. Best is trial 2 with value: 3112031.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:17,811] Trial 3 finished with value: 2369198.25 and parameters: {'n_estimators': 292, 'max_depth': 4, 'xgb_learning_rate': 0.0003307036467577316, 'subsample': 0.8006474317661161, 'colsample_bytree': 0.9794611321851184, 'use_hidden_layer_0': False, 'use_hidden_layer_1': True, 'hidden_layer_1': 37, 'use_hidden_layer_2': True, 'hidden_layer_2': 55, 'nn_learning_rate': 0.007291986134174138, 'batch_size': 128, 'num_epochs': 78}. Best is trial 3 with value: 2369198.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:20,256] Trial 4 finished with value: 3275539.0 and parameters: {'n_estimators': 262, 'max_depth': 9, 'xgb_learning_rate': 0.006783641499040651, 'subsample': 0.9158026783919512, 'colsample_bytree': 0.7441934419417698, 'use_hidden_layer_0': True, 'hidden_layer_0': 79, 'use_hidden_layer_1': False, 'use_hidden_layer_2': False, 'nn_learning_rate': 0.00017247758063390942, 'batch_size': 128, 'num_epochs': 84}. Best is trial 3 with value: 2369198.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:20,993] Trial 5 finished with value: 1686898.875 and parameters: {'n_estimators': 142, 'max_depth': 5, 'xgb_learning_rate': 0.0005747413822647764, 'subsample': 0.5733090587073465, 'colsample_bytree': 0.951414639255233, 'use_hidden_layer_0': False, 'use_hidden_layer_1': True, 'hidden_layer_1': 131, 'use_hidden_layer_2': True, 'hidden_layer_2': 115, 'nn_learning_rate': 0.0005908856048582727, 'batch_size': 64, 'num_epochs': 13}. Best is trial 5 with value: 1686898.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:21,872] Trial 6 finished with value: 2311601.5 and parameters: {'n_estimators': 268, 'max_depth': 10, 'xgb_learning_rate': 0.000687344549086651, 'subsample': 0.6204323202781399, 'colsample_bytree': 0.5270488905409627, 'use_hidden_layer_0': True, 'hidden_layer_0': 73, 'use_hidden_layer_1': True, 'hidden_layer_1': 51, 'use_hidden_layer_2': True, 'hidden_layer_2': 136, 'nn_learning_rate': 0.0034392238557027803, 'batch_size': 128, 'num_epochs': 13}. Best is trial 5 with value: 1686898.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:23,441] Trial 7 finished with value: 2268082.25 and parameters: {'n_estimators': 220, 'max_depth': 8, 'xgb_learning_rate': 0.05587321340568407, 'subsample': 0.8364114548147025, 'colsample_bytree': 0.542450270069385, 'use_hidden_layer_0': False, 'use_hidden_layer_1': True, 'hidden_layer_1': 197, 'use_hidden_layer_2': True, 'hidden_layer_2': 32, 'nn_learning_rate': 0.00031775893913693056, 'batch_size': 128, 'num_epochs': 42}. Best is trial 5 with value: 1686898.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:27,227] Trial 8 finished with value: 3102278.25 and parameters: {'n_estimators': 225, 'max_depth': 9, 'xgb_learning_rate': 0.004714370780825207, 'subsample': 0.897056152597955, 'colsample_bytree': 0.5540802110690384, 'use_hidden_layer_0': True, 'hidden_layer_0': 151, 'use_hidden_layer_1': True, 'hidden_layer_1': 119, 'use_hidden_layer_2': True, 'hidden_layer_2': 119, 'nn_learning_rate': 0.0006050221265468488, 'batch_size': 32, 'num_epochs': 40}. Best is trial 5 with value: 1686898.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1654751789.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:28,986] Trial 9 finished with value: 4585746.5 and parameters: {'n_estimators': 289, 'max_depth': 10, 'xgb_learning_rate': 0.00010860457801045743, 'subsample': 0.6114006132040972, 'colsample_bytree': 0.5391102529871902, 'use_hidden_layer_0': True, 'hidden_layer_0': 38, 'use_hidden_layer_1': False, 'use_hidden_layer_2': False, 'nn_learning_rate': 0.04074738827726201, 'batch_size': 32, 'num_epochs': 27}. Best is trial 5 with value: 1686898.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:51:32,668] A new study created in memory with name: no-name-b33437d5-a86a-4194-9775-b7dd168333af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep       11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN          1651738.75  1285.199829    876.83136  0.517436   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "Wide_and_Deep       148.798538   41.887012                    5.563657   \n",
      "XGBoost + NN       1495.378174  200.556961                    0.494677   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "Wide_and_Deep                         0.000997                52.083259   \n",
      "XGBoost + NN                          0.000998                23.276744   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep      {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN       {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "Using device: cpu\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:33,721] Trial 0 finished with value: 2868993.25 and parameters: {'n_estimators': 159, 'max_depth': 4, 'lgb_learning_rate': 0.000671034630683982, 'num_leaves': 24, 'subsample': 0.9183903949102641, 'colsample_bytree': 0.7890466128306823, 'hidden_layer_0': 188, 'hidden_layer_1': 52, 'hidden_layer_2': 217, 'nn_learning_rate': 0.0031833409938854295, 'batch_size': 128, 'num_epochs': 37}. Best is trial 0 with value: 2868993.25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:35,903] Trial 1 finished with value: 228554.84375 and parameters: {'n_estimators': 59, 'max_depth': 4, 'lgb_learning_rate': 0.029496532955236084, 'num_leaves': 50, 'subsample': 0.6889563629891278, 'colsample_bytree': 0.5425971897863822, 'hidden_layer_0': 186, 'hidden_layer_1': 96, 'hidden_layer_2': 87, 'nn_learning_rate': 0.024683404777309677, 'batch_size': 32, 'num_epochs': 31}. Best is trial 1 with value: 228554.84375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:36,752] Trial 2 finished with value: 3325210.75 and parameters: {'n_estimators': 130, 'max_depth': 10, 'lgb_learning_rate': 0.0001732649368776703, 'num_leaves': 30, 'subsample': 0.7674263780423947, 'colsample_bytree': 0.7293481512219095, 'hidden_layer_0': 209, 'hidden_layer_1': 148, 'hidden_layer_2': 40, 'nn_learning_rate': 0.0002530426938510906, 'batch_size': 128, 'num_epochs': 22}. Best is trial 1 with value: 228554.84375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:37,947] Trial 3 finished with value: 73897.109375 and parameters: {'n_estimators': 89, 'max_depth': 7, 'lgb_learning_rate': 0.02467774945633356, 'num_leaves': 32, 'subsample': 0.6727491590082778, 'colsample_bytree': 0.8822756169693712, 'hidden_layer_0': 33, 'hidden_layer_1': 203, 'hidden_layer_2': 42, 'nn_learning_rate': 0.03514830460902628, 'batch_size': 256, 'num_epochs': 59}. Best is trial 3 with value: 73897.109375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:41,704] Trial 4 finished with value: 19285.455078125 and parameters: {'n_estimators': 132, 'max_depth': 3, 'lgb_learning_rate': 0.09196238369415112, 'num_leaves': 61, 'subsample': 0.9848122774199848, 'colsample_bytree': 0.926831856626592, 'hidden_layer_0': 155, 'hidden_layer_1': 46, 'hidden_layer_2': 71, 'nn_learning_rate': 0.007218946885761279, 'batch_size': 64, 'num_epochs': 100}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:42,529] Trial 5 finished with value: 2379060.25 and parameters: {'n_estimators': 92, 'max_depth': 6, 'lgb_learning_rate': 0.002345131754625289, 'num_leaves': 49, 'subsample': 0.788051902656306, 'colsample_bytree': 0.5563092429642966, 'hidden_layer_0': 121, 'hidden_layer_1': 64, 'hidden_layer_2': 40, 'nn_learning_rate': 0.0002594834562767456, 'batch_size': 128, 'num_epochs': 33}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:46,160] Trial 6 finished with value: 2481433.75 and parameters: {'n_estimators': 72, 'max_depth': 8, 'lgb_learning_rate': 0.0024385561555397006, 'num_leaves': 40, 'subsample': 0.8937575421405362, 'colsample_bytree': 0.9529447808587821, 'hidden_layer_0': 182, 'hidden_layer_1': 156, 'hidden_layer_2': 71, 'nn_learning_rate': 0.051986027253995724, 'batch_size': 64, 'num_epochs': 74}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:46,634] Trial 7 finished with value: 1136585.625 and parameters: {'n_estimators': 146, 'max_depth': 3, 'lgb_learning_rate': 0.004790133570810796, 'num_leaves': 99, 'subsample': 0.8498604037395934, 'colsample_bytree': 0.6238814587503646, 'hidden_layer_0': 230, 'hidden_layer_1': 138, 'hidden_layer_2': 132, 'nn_learning_rate': 0.00039865899475304966, 'batch_size': 256, 'num_epochs': 18}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:49,235] Trial 8 finished with value: 175843.484375 and parameters: {'n_estimators': 130, 'max_depth': 8, 'lgb_learning_rate': 0.013134729535915362, 'num_leaves': 63, 'subsample': 0.8899849784419127, 'colsample_bytree': 0.6826857045878769, 'hidden_layer_0': 237, 'hidden_layer_1': 184, 'hidden_layer_2': 121, 'nn_learning_rate': 0.0004967123921114462, 'batch_size': 256, 'num_epochs': 98}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:51:50,359] Trial 9 finished with value: 3239205.75 and parameters: {'n_estimators': 290, 'max_depth': 4, 'lgb_learning_rate': 0.0001279387134732265, 'num_leaves': 70, 'subsample': 0.7805148632515138, 'colsample_bytree': 0.9067804893857552, 'hidden_layer_0': 174, 'hidden_layer_1': 64, 'hidden_layer_2': 202, 'nn_learning_rate': 0.00018389745449068516, 'batch_size': 256, 'num_epochs': 44}. Best is trial 4 with value: 19285.455078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 584, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4546.361301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1272\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4612.049251\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4507.051392\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1278\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4587.419700\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 467, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4467.601713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 468, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 4557.660256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:52:10,247] A new study created in memory with name: no-name-029eebe5-de14-468e-bb48-ca95b1fc9c0c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep       11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN          1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN       17473.076172    132.18576    92.866531  0.994895   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "Wide_and_Deep       148.798538   41.887012                    5.563657   \n",
      "XGBoost + NN       1495.378174  200.556961                    0.494677   \n",
      "LightGBM + NN       160.771698   26.745409                    3.776898   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "Wide_and_Deep                         0.000997                52.083259   \n",
      "XGBoost + NN                          0.000998                23.276744   \n",
      "LightGBM + NN                              0.0                37.564108   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep      {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN       {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN      {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:11,838] Trial 0 finished with value: 2652992.0 and parameters: {'num_heads': 5, 'embedding_dim': 5, 'num_layers': 2, 'hidden_layer_0': 207, 'hidden_layer_1': 155, 'hidden_layer_2': 191, 'nn_learning_rate': 0.005765542935304501, 'batch_size': 256, 'num_epochs': 25}. Best is trial 0 with value: 2652992.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:13,406] Trial 1 finished with value: 19184310.0 and parameters: {'num_heads': 1, 'embedding_dim': 12, 'num_layers': 2, 'hidden_layer_0': 104, 'hidden_layer_1': 185, 'hidden_layer_2': 82, 'nn_learning_rate': 0.0003972802075288308, 'batch_size': 128, 'num_epochs': 27}. Best is trial 0 with value: 2652992.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:17,639] Trial 2 finished with value: 573273.125 and parameters: {'num_heads': 2, 'embedding_dim': 36, 'num_layers': 1, 'hidden_layer_0': 208, 'hidden_layer_1': 100, 'hidden_layer_2': 42, 'nn_learning_rate': 0.0003866287383428963, 'batch_size': 32, 'num_epochs': 47}. Best is trial 2 with value: 573273.125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:19,605] Trial 3 finished with value: 84712.1640625 and parameters: {'num_heads': 8, 'embedding_dim': 16, 'num_layers': 1, 'hidden_layer_0': 80, 'hidden_layer_1': 212, 'hidden_layer_2': 140, 'nn_learning_rate': 0.043899521760698626, 'batch_size': 128, 'num_epochs': 41}. Best is trial 3 with value: 84712.1640625.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:21,730] Trial 4 finished with value: 1647771.125 and parameters: {'num_heads': 6, 'embedding_dim': 60, 'num_layers': 2, 'hidden_layer_0': 138, 'hidden_layer_1': 84, 'hidden_layer_2': 207, 'nn_learning_rate': 0.001706554196564131, 'batch_size': 256, 'num_epochs': 29}. Best is trial 3 with value: 84712.1640625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:23,884] Trial 5 finished with value: 28386.544921875 and parameters: {'num_heads': 1, 'embedding_dim': 24, 'num_layers': 3, 'hidden_layer_0': 36, 'hidden_layer_1': 136, 'hidden_layer_2': 199, 'nn_learning_rate': 0.016435643465991176, 'batch_size': 64, 'num_epochs': 27}. Best is trial 5 with value: 28386.544921875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:26,717] Trial 6 finished with value: 998.8632202148438 and parameters: {'num_heads': 3, 'embedding_dim': 54, 'num_layers': 1, 'hidden_layer_0': 134, 'hidden_layer_1': 52, 'hidden_layer_2': 160, 'nn_learning_rate': 0.0166033087667256, 'batch_size': 128, 'num_epochs': 79}. Best is trial 6 with value: 998.8632202148438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:30,972] Trial 7 finished with value: 1346004.0 and parameters: {'num_heads': 1, 'embedding_dim': 4, 'num_layers': 3, 'hidden_layer_0': 246, 'hidden_layer_1': 87, 'hidden_layer_2': 90, 'nn_learning_rate': 0.02344425719120848, 'batch_size': 64, 'num_epochs': 72}. Best is trial 6 with value: 998.8632202148438.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:33,333] Trial 8 finished with value: 370403.8125 and parameters: {'num_heads': 7, 'embedding_dim': 7, 'num_layers': 2, 'hidden_layer_0': 142, 'hidden_layer_1': 145, 'hidden_layer_2': 147, 'nn_learning_rate': 0.012474830990142348, 'batch_size': 64, 'num_epochs': 25}. Best is trial 6 with value: 998.8632202148438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2862604623.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:52:35,613] Trial 9 finished with value: 4404.8564453125 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 2, 'hidden_layer_0': 146, 'hidden_layer_1': 98, 'hidden_layer_2': 115, 'nn_learning_rate': 0.012313462912227222, 'batch_size': 128, 'num_epochs': 28}. Best is trial 6 with value: 998.8632202148438.\n",
      "[I 2024-08-06 10:52:50,231] A new study created in memory with name: no-name-a349e93a-fbc3-4234-ac47-552b63d00d09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep       11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN          1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN       17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN        10133.049805   100.663048    51.398952   0.99704   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "Wide_and_Deep       148.798538   41.887012                    5.563657   \n",
      "XGBoost + NN       1495.378174  200.556961                    0.494677   \n",
      "LightGBM + NN       160.771698   26.745409                    3.776898   \n",
      "AutoInt + NN         54.887157   24.947227                    2.099385   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "Wide_and_Deep                         0.000997                52.083259   \n",
      "XGBoost + NN                          0.000998                23.276744   \n",
      "LightGBM + NN                              0.0                37.564108   \n",
      "AutoInt + NN                               0.0                39.965675   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep      {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN       {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN      {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN       {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:01,368] Trial 0 finished with value: 3421469.0 and parameters: {'num_heads': 3, 'embedding_dim': 63, 'num_layers': 2, 'hidden_layer_0': 252, 'hidden_layer_1': 243, 'hidden_layer_2': 185, 'nn_learning_rate': 0.00010342671135657405, 'batch_size': 32, 'num_epochs': 73}. Best is trial 0 with value: 3421469.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:08,681] Trial 1 finished with value: 3108577.75 and parameters: {'num_heads': 8, 'embedding_dim': 24, 'num_layers': 2, 'hidden_layer_0': 244, 'hidden_layer_1': 108, 'hidden_layer_2': 85, 'nn_learning_rate': 0.0179485651204226, 'batch_size': 64, 'num_epochs': 90}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:14,184] Trial 2 finished with value: 3439108.5 and parameters: {'num_heads': 4, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 46, 'hidden_layer_1': 174, 'hidden_layer_2': 182, 'nn_learning_rate': 0.00586907579032138, 'batch_size': 256, 'num_epochs': 93}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:21,135] Trial 3 finished with value: 10513110.0 and parameters: {'num_heads': 1, 'embedding_dim': 32, 'num_layers': 2, 'hidden_layer_0': 149, 'hidden_layer_1': 217, 'hidden_layer_2': 35, 'nn_learning_rate': 0.00010200641840589118, 'batch_size': 64, 'num_epochs': 83}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:26,635] Trial 4 finished with value: 3525590.0 and parameters: {'num_heads': 8, 'embedding_dim': 56, 'num_layers': 2, 'hidden_layer_0': 92, 'hidden_layer_1': 63, 'hidden_layer_2': 170, 'nn_learning_rate': 0.0020194837547931432, 'batch_size': 128, 'num_epochs': 71}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:30,701] Trial 5 finished with value: 20692158.0 and parameters: {'num_heads': 1, 'embedding_dim': 9, 'num_layers': 1, 'hidden_layer_0': 197, 'hidden_layer_1': 60, 'hidden_layer_2': 101, 'nn_learning_rate': 0.00011014123631947157, 'batch_size': 128, 'num_epochs': 94}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:34,325] Trial 6 finished with value: 3975051.5 and parameters: {'num_heads': 5, 'embedding_dim': 15, 'num_layers': 2, 'hidden_layer_0': 252, 'hidden_layer_1': 41, 'hidden_layer_2': 153, 'nn_learning_rate': 0.0020872929772198347, 'batch_size': 256, 'num_epochs': 39}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:38,017] Trial 7 finished with value: 3545278.25 and parameters: {'num_heads': 4, 'embedding_dim': 4, 'num_layers': 2, 'hidden_layer_0': 36, 'hidden_layer_1': 38, 'hidden_layer_2': 223, 'nn_learning_rate': 0.057480400063058754, 'batch_size': 128, 'num_epochs': 47}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:41,758] Trial 8 finished with value: 3702587.5 and parameters: {'num_heads': 6, 'embedding_dim': 30, 'num_layers': 1, 'hidden_layer_0': 36, 'hidden_layer_1': 58, 'hidden_layer_2': 196, 'nn_learning_rate': 0.0005802497245062619, 'batch_size': 128, 'num_epochs': 75}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061024580.py:104: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:53:48,965] Trial 9 finished with value: 3451138.0 and parameters: {'num_heads': 1, 'embedding_dim': 55, 'num_layers': 3, 'hidden_layer_0': 229, 'hidden_layer_1': 156, 'hidden_layer_2': 166, 'nn_learning_rate': 0.06036719735671087, 'batch_size': 256, 'num_epochs': 93}. Best is trial 1 with value: 3108577.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:24,629] A new study created in memory with name: no-name-78085d92-948e-4022-ae0a-9f66abfac35e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression            0.0          0.0          0.0       1.0   \n",
      "Ridge                   0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                   0.013676     0.116946     0.096502       1.0   \n",
      "KNN                161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree       30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest       11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting    8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost              9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM            12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost             6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                    3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                 11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep       11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN          1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN       17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN        10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer        3346669.75  1829.390503   1536.50708  0.022254   \n",
      "\n",
      "                  CV Mean RMSE CV Std RMSE Training Time (Best Params)  \\\n",
      "Linear Regression          0.0         0.0                    0.001997   \n",
      "Ridge                 0.798848    0.144414                    0.000997   \n",
      "Lasso                 0.122118    0.006306                    0.000996   \n",
      "KNN                 522.242956   25.290932                    0.000997   \n",
      "Decision Tree        239.46383   32.541677                    0.004986   \n",
      "Random Forest       163.954712   27.180044                    0.827819   \n",
      "Gradient Boosting   125.340587   11.473233                    0.594411   \n",
      "XGBoost             123.654731   13.853207                    0.226393   \n",
      "LightGBM             161.25728    16.98225                    0.134639   \n",
      "CatBoost            109.085283   17.258991                    0.516617   \n",
      "MLP                 398.072005   21.709468                    3.516119   \n",
      "DNN                1941.074219   88.739159                    5.147236   \n",
      "DCN                 169.078384   42.784168                    5.152221   \n",
      "Wide_and_Deep       148.798538   41.887012                    5.563657   \n",
      "XGBoost + NN       1495.378174  200.556961                    0.494677   \n",
      "LightGBM + NN       160.771698   26.745409                    3.776898   \n",
      "AutoInt + NN         54.887157   24.947227                    2.099385   \n",
      "FT-Transformer     1986.821655  173.623123                    4.492984   \n",
      "\n",
      "                  Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Linear Regression                          0.0                  3.62482   \n",
      "Ridge                                      0.0                 0.070811   \n",
      "Lasso                                      0.0                 0.063828   \n",
      "KNN                                   0.002991                 0.192878   \n",
      "Decision Tree                              0.0                 0.444815   \n",
      "Random Forest                          0.02191                56.899384   \n",
      "Gradient Boosting                     0.000998                18.245007   \n",
      "XGBoost                               0.001995                10.466175   \n",
      "LightGBM                              0.000997                14.049421   \n",
      "CatBoost                              0.000998                80.273983   \n",
      "MLP                                   0.000997               134.738025   \n",
      "DNN                                        0.0                44.896956   \n",
      "DCN                                   0.000997                61.166004   \n",
      "Wide_and_Deep                         0.000997                52.083259   \n",
      "XGBoost + NN                          0.000998                23.276744   \n",
      "LightGBM + NN                              0.0                37.564108   \n",
      "AutoInt + NN                               0.0                39.965675   \n",
      "FT-Transformer                        0.000998                94.385147   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge                               {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                          {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest      {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost            {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM           {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost           {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep      {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN       {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN      {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN       {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer     {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:25,926] Trial 0 finished with value: 4488808.5 and parameters: {'num_layers': 4, 'hidden_layer_0': 36, 'hidden_layer_1': 152, 'hidden_layer_2': 194, 'hidden_layer_3': 205, 'learning_rate': 0.01350052291519599, 'batch_size': 64, 'num_epochs': 21}. Best is trial 0 with value: 4488808.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:28,908] Trial 1 finished with value: 3085277.25 and parameters: {'num_layers': 2, 'hidden_layer_0': 88, 'hidden_layer_1': 186, 'learning_rate': 0.047488785766772144, 'batch_size': 64, 'num_epochs': 81}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:31,011] Trial 2 finished with value: 5463435.5 and parameters: {'num_layers': 3, 'hidden_layer_0': 57, 'hidden_layer_1': 122, 'hidden_layer_2': 173, 'learning_rate': 0.0003755605260024644, 'batch_size': 128, 'num_epochs': 63}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:32,875] Trial 3 finished with value: 3634628.5 and parameters: {'num_layers': 2, 'hidden_layer_0': 48, 'hidden_layer_1': 177, 'learning_rate': 0.007405577446155828, 'batch_size': 256, 'num_epochs': 100}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:35,091] Trial 4 finished with value: 3529811.0 and parameters: {'num_layers': 3, 'hidden_layer_0': 250, 'hidden_layer_1': 58, 'hidden_layer_2': 256, 'learning_rate': 0.002485528244048486, 'batch_size': 128, 'num_epochs': 73}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:37,491] Trial 5 finished with value: 3640828.25 and parameters: {'num_layers': 2, 'hidden_layer_0': 244, 'hidden_layer_1': 213, 'learning_rate': 0.01218529361384021, 'batch_size': 64, 'num_epochs': 53}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:39,716] Trial 6 finished with value: 3549614.0 and parameters: {'num_layers': 3, 'hidden_layer_0': 55, 'hidden_layer_1': 240, 'hidden_layer_2': 138, 'learning_rate': 0.000930413552091333, 'batch_size': 32, 'num_epochs': 28}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:42,119] Trial 7 finished with value: 4627570.0 and parameters: {'num_layers': 4, 'hidden_layer_0': 82, 'hidden_layer_1': 128, 'hidden_layer_2': 250, 'hidden_layer_3': 171, 'learning_rate': 0.00022783897536052144, 'batch_size': 128, 'num_epochs': 59}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:47,806] Trial 8 finished with value: 3448216.0 and parameters: {'num_layers': 5, 'hidden_layer_0': 229, 'hidden_layer_1': 122, 'hidden_layer_2': 247, 'hidden_layer_3': 163, 'hidden_layer_4': 146, 'learning_rate': 0.0006651668289433902, 'batch_size': 32, 'num_epochs': 44}. Best is trial 1 with value: 3085277.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\3752916695.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([72, 1])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:54:49,168] Trial 9 finished with value: 3536854.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 191, 'hidden_layer_1': 76, 'hidden_layer_2': 210, 'learning_rate': 0.005566380335330156, 'batch_size': 128, 'num_epochs': 43}. Best is trial 1 with value: 3085277.25.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19, 1])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[I 2024-08-06 10:55:04,089] A new study created in memory with name: no-name-33c1902e-62d9-4ee8-8a27-f07fe0c22706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression                     0.0          0.0          0.0       1.0   \n",
      "Ridge                            0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                            0.013676     0.116946     0.096502       1.0   \n",
      "KNN                         161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree                30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest                11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting             8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost                       9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM                     12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost                      6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                         112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                             3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                          11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep                11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN                   1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN                17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN                 10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer                 3346669.75  1829.390503   1536.50708  0.022254   \n",
      "Neural Architecture Search     3838256.75  1959.146973  1625.576172 -0.121365   \n",
      "\n",
      "                           CV Mean RMSE CV Std RMSE  \\\n",
      "Linear Regression                   0.0         0.0   \n",
      "Ridge                          0.798848    0.144414   \n",
      "Lasso                          0.122118    0.006306   \n",
      "KNN                          522.242956   25.290932   \n",
      "Decision Tree                 239.46383   32.541677   \n",
      "Random Forest                163.954712   27.180044   \n",
      "Gradient Boosting            125.340587   11.473233   \n",
      "XGBoost                      123.654731   13.853207   \n",
      "LightGBM                      161.25728    16.98225   \n",
      "CatBoost                     109.085283   17.258991   \n",
      "MLP                          398.072005   21.709468   \n",
      "DNN                         1941.074219   88.739159   \n",
      "DCN                          169.078384   42.784168   \n",
      "Wide_and_Deep                148.798538   41.887012   \n",
      "XGBoost + NN                1495.378174  200.556961   \n",
      "LightGBM + NN                160.771698   26.745409   \n",
      "AutoInt + NN                  54.887157   24.947227   \n",
      "FT-Transformer              1986.821655  173.623123   \n",
      "Neural Architecture Search   1937.61853  116.816948   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.001997   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000996   \n",
      "KNN                                           0.000997   \n",
      "Decision Tree                                 0.004986   \n",
      "Random Forest                                 0.827819   \n",
      "Gradient Boosting                             0.594411   \n",
      "XGBoost                                       0.226393   \n",
      "LightGBM                                      0.134639   \n",
      "CatBoost                                      0.516617   \n",
      "MLP                                           3.516119   \n",
      "DNN                                           5.147236   \n",
      "DCN                                           5.152221   \n",
      "Wide_and_Deep                                 5.563657   \n",
      "XGBoost + NN                                  0.494677   \n",
      "LightGBM + NN                                 3.776898   \n",
      "AutoInt + NN                                  2.099385   \n",
      "FT-Transformer                                4.492984   \n",
      "Neural Architecture Search                    3.009951   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                               0.0   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002991   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                   0.02191   \n",
      "Gradient Boosting                              0.000998   \n",
      "XGBoost                                        0.001995   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.000998   \n",
      "MLP                                            0.000997   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                   0.000998   \n",
      "LightGBM + NN                                       0.0   \n",
      "AutoInt + NN                                        0.0   \n",
      "FT-Transformer                                 0.000998   \n",
      "Neural Architecture Search                     0.000997   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                           3.62482   \n",
      "Ridge                                      0.070811   \n",
      "Lasso                                      0.063828   \n",
      "KNN                                        0.192878   \n",
      "Decision Tree                              0.444815   \n",
      "Random Forest                             56.899384   \n",
      "Gradient Boosting                         18.245007   \n",
      "XGBoost                                   10.466175   \n",
      "LightGBM                                  14.049421   \n",
      "CatBoost                                  80.273983   \n",
      "MLP                                      134.738025   \n",
      "DNN                                       44.896956   \n",
      "DCN                                       61.166004   \n",
      "Wide_and_Deep                             52.083259   \n",
      "XGBoost + NN                              23.276744   \n",
      "LightGBM + NN                             37.564108   \n",
      "AutoInt + NN                              39.965675   \n",
      "FT-Transformer                            94.385147   \n",
      "Neural Architecture Search                39.444057   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                        {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                                   {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting           {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                         {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN                {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 88, 'hidde...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:05,090] Trial 0 finished with value: 9680211.0 and parameters: {'hidden_dim': 143, 'learning_rate': 0.007395867885574298, 'batch_size': 256, 'num_epochs': 66}. Best is trial 0 with value: 9680211.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:06,714] Trial 1 finished with value: 790641.6875 and parameters: {'hidden_dim': 220, 'learning_rate': 0.01001065513729506, 'batch_size': 128, 'num_epochs': 84}. Best is trial 1 with value: 790641.6875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:07,237] Trial 2 finished with value: 1021323.1875 and parameters: {'hidden_dim': 249, 'learning_rate': 0.027279409337392695, 'batch_size': 128, 'num_epochs': 26}. Best is trial 1 with value: 790641.6875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:09,841] Trial 3 finished with value: 4811961.0 and parameters: {'hidden_dim': 237, 'learning_rate': 0.002142148498731373, 'batch_size': 64, 'num_epochs': 93}. Best is trial 1 with value: 790641.6875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:10,486] Trial 4 finished with value: 22359030.0 and parameters: {'hidden_dim': 48, 'learning_rate': 0.0003851579780825633, 'batch_size': 128, 'num_epochs': 58}. Best is trial 1 with value: 790641.6875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:10,931] Trial 5 finished with value: 230622.828125 and parameters: {'hidden_dim': 244, 'learning_rate': 0.06881649264196976, 'batch_size': 128, 'num_epochs': 23}. Best is trial 5 with value: 230622.828125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:14,155] Trial 6 finished with value: 84731.9140625 and parameters: {'hidden_dim': 243, 'learning_rate': 0.009517744749412202, 'batch_size': 32, 'num_epochs': 77}. Best is trial 6 with value: 84731.9140625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:14,787] Trial 7 finished with value: 22371878.0 and parameters: {'hidden_dim': 220, 'learning_rate': 0.00016865268730945987, 'batch_size': 256, 'num_epochs': 41}. Best is trial 6 with value: 84731.9140625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:15,381] Trial 8 finished with value: 21429924.0 and parameters: {'hidden_dim': 42, 'learning_rate': 0.0033800578249520295, 'batch_size': 256, 'num_epochs': 64}. Best is trial 6 with value: 84731.9140625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\525385819.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:15,888] Trial 9 finished with value: 22304320.0 and parameters: {'hidden_dim': 234, 'learning_rate': 0.000610479522292971, 'batch_size': 128, 'num_epochs': 26}. Best is trial 6 with value: 84731.9140625.\n",
      "[I 2024-08-06 10:55:32,045] A new study created in memory with name: no-name-aca445c2-7c0c-4893-97d7-9b04caa3249b\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression                     0.0          0.0          0.0       1.0   \n",
      "Ridge                            0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                            0.013676     0.116946     0.096502       1.0   \n",
      "KNN                         161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree                30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest                11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting             8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost                       9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM                     12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost                      6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                         112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                             3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                          11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep                11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN                   1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN                17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN                 10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer                 3346669.75  1829.390503   1536.50708  0.022254   \n",
      "Neural Architecture Search     3838256.75  1959.146973  1625.576172 -0.121365   \n",
      "KAN                             91932.125   303.203094   220.203583  0.973142   \n",
      "\n",
      "                           CV Mean RMSE CV Std RMSE  \\\n",
      "Linear Regression                   0.0         0.0   \n",
      "Ridge                          0.798848    0.144414   \n",
      "Lasso                          0.122118    0.006306   \n",
      "KNN                          522.242956   25.290932   \n",
      "Decision Tree                 239.46383   32.541677   \n",
      "Random Forest                163.954712   27.180044   \n",
      "Gradient Boosting            125.340587   11.473233   \n",
      "XGBoost                      123.654731   13.853207   \n",
      "LightGBM                      161.25728    16.98225   \n",
      "CatBoost                     109.085283   17.258991   \n",
      "MLP                          398.072005   21.709468   \n",
      "DNN                         1941.074219   88.739159   \n",
      "DCN                          169.078384   42.784168   \n",
      "Wide_and_Deep                148.798538   41.887012   \n",
      "XGBoost + NN                1495.378174  200.556961   \n",
      "LightGBM + NN                160.771698   26.745409   \n",
      "AutoInt + NN                  54.887157   24.947227   \n",
      "FT-Transformer              1986.821655  173.623123   \n",
      "Neural Architecture Search   1937.61853  116.816948   \n",
      "KAN                          348.740906   42.645691   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.001997   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000996   \n",
      "KNN                                           0.000997   \n",
      "Decision Tree                                 0.004986   \n",
      "Random Forest                                 0.827819   \n",
      "Gradient Boosting                             0.594411   \n",
      "XGBoost                                       0.226393   \n",
      "LightGBM                                      0.134639   \n",
      "CatBoost                                      0.516617   \n",
      "MLP                                           3.516119   \n",
      "DNN                                           5.147236   \n",
      "DCN                                           5.152221   \n",
      "Wide_and_Deep                                 5.563657   \n",
      "XGBoost + NN                                  0.494677   \n",
      "LightGBM + NN                                 3.776898   \n",
      "AutoInt + NN                                  2.099385   \n",
      "FT-Transformer                                4.492984   \n",
      "Neural Architecture Search                    3.009951   \n",
      "KAN                                           3.274243   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                               0.0   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002991   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                   0.02191   \n",
      "Gradient Boosting                              0.000998   \n",
      "XGBoost                                        0.001995   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.000998   \n",
      "MLP                                            0.000997   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                   0.000998   \n",
      "LightGBM + NN                                       0.0   \n",
      "AutoInt + NN                                        0.0   \n",
      "FT-Transformer                                 0.000998   \n",
      "Neural Architecture Search                     0.000997   \n",
      "KAN                                                 0.0   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                           3.62482   \n",
      "Ridge                                      0.070811   \n",
      "Lasso                                      0.063828   \n",
      "KNN                                        0.192878   \n",
      "Decision Tree                              0.444815   \n",
      "Random Forest                             56.899384   \n",
      "Gradient Boosting                         18.245007   \n",
      "XGBoost                                   10.466175   \n",
      "LightGBM                                  14.049421   \n",
      "CatBoost                                  80.273983   \n",
      "MLP                                      134.738025   \n",
      "DNN                                       44.896956   \n",
      "DCN                                       61.166004   \n",
      "Wide_and_Deep                             52.083259   \n",
      "XGBoost + NN                              23.276744   \n",
      "LightGBM + NN                             37.564108   \n",
      "AutoInt + NN                              39.965675   \n",
      "FT-Transformer                            94.385147   \n",
      "Neural Architecture Search                39.444057   \n",
      "KAN                                       27.943278   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                        {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                                   {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting           {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                         {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN                {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 88, 'hidde...  \n",
      "KAN                         {'hidden_dim': 243, 'learning_rate': 0.0095177...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 10:55:35,739] Trial 0 finished with value: 22289934.0 and parameters: {'num_layers': 3, 'num_trees': 9, 'tree_dim': 22, 'learning_rate': 0.000488390552065308, 'batch_size': 256, 'num_epochs': 68}. Best is trial 0 with value: 22289934.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:40,243] Trial 1 finished with value: 2410389.25 and parameters: {'num_layers': 4, 'num_trees': 2, 'tree_dim': 45, 'learning_rate': 0.001593274747913012, 'batch_size': 64, 'num_epochs': 85}. Best is trial 1 with value: 2410389.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:46,636] Trial 2 finished with value: 1460056.25 and parameters: {'num_layers': 4, 'num_trees': 9, 'tree_dim': 60, 'learning_rate': 0.002842355564862037, 'batch_size': 128, 'num_epochs': 53}. Best is trial 2 with value: 1460056.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:47,744] Trial 3 finished with value: 22266872.0 and parameters: {'num_layers': 3, 'num_trees': 3, 'tree_dim': 56, 'learning_rate': 0.0007085231868398389, 'batch_size': 128, 'num_epochs': 29}. Best is trial 2 with value: 1460056.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:48,973] Trial 4 finished with value: 3546833.0 and parameters: {'num_layers': 1, 'num_trees': 1, 'tree_dim': 41, 'learning_rate': 0.005582995051092692, 'batch_size': 128, 'num_epochs': 73}. Best is trial 2 with value: 1460056.25.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:53,956] Trial 5 finished with value: 26881.154296875 and parameters: {'num_layers': 4, 'num_trees': 4, 'tree_dim': 33, 'learning_rate': 0.04536911546856252, 'batch_size': 128, 'num_epochs': 99}. Best is trial 5 with value: 26881.154296875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:57,679] Trial 6 finished with value: 113064.28125 and parameters: {'num_layers': 2, 'num_trees': 2, 'tree_dim': 47, 'learning_rate': 0.00595620843798485, 'batch_size': 32, 'num_epochs': 67}. Best is trial 5 with value: 26881.154296875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:55:58,216] Trial 7 finished with value: 260174.515625 and parameters: {'num_layers': 4, 'num_trees': 2, 'tree_dim': 46, 'learning_rate': 0.0960624192358449, 'batch_size': 256, 'num_epochs': 21}. Best is trial 5 with value: 26881.154296875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:56:01,146] Trial 8 finished with value: 858469.375 and parameters: {'num_layers': 4, 'num_trees': 4, 'tree_dim': 38, 'learning_rate': 0.006022036664621387, 'batch_size': 128, 'num_epochs': 53}. Best is trial 5 with value: 26881.154296875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\1061925536.py:68: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-08-06 10:56:04,115] Trial 9 finished with value: 47185.21484375 and parameters: {'num_layers': 2, 'num_trees': 5, 'tree_dim': 13, 'learning_rate': 0.03844536838065388, 'batch_size': 128, 'num_epochs': 87}. Best is trial 5 with value: 26881.154296875.\n",
      "[I 2024-08-06 10:56:29,412] A new study created in memory with name: no-name-c9ff0b21-099a-4d9f-ab83-baf38bb7e854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression                     0.0          0.0          0.0       1.0   \n",
      "Ridge                            0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                            0.013676     0.116946     0.096502       1.0   \n",
      "KNN                         161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree                30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest                11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting             8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost                       9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM                     12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost                      6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                         112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                             3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                          11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep                11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN                   1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN                17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN                 10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer                 3346669.75  1829.390503   1536.50708  0.022254   \n",
      "Neural Architecture Search     3838256.75  1959.146973  1625.576172 -0.121365   \n",
      "KAN                             91932.125   303.203094   220.203583  0.973142   \n",
      "NODE                         26062.462891   161.438721    114.89048  0.992386   \n",
      "\n",
      "                           CV Mean RMSE CV Std RMSE  \\\n",
      "Linear Regression                   0.0         0.0   \n",
      "Ridge                          0.798848    0.144414   \n",
      "Lasso                          0.122118    0.006306   \n",
      "KNN                          522.242956   25.290932   \n",
      "Decision Tree                 239.46383   32.541677   \n",
      "Random Forest                163.954712   27.180044   \n",
      "Gradient Boosting            125.340587   11.473233   \n",
      "XGBoost                      123.654731   13.853207   \n",
      "LightGBM                      161.25728    16.98225   \n",
      "CatBoost                     109.085283   17.258991   \n",
      "MLP                          398.072005   21.709468   \n",
      "DNN                         1941.074219   88.739159   \n",
      "DCN                          169.078384   42.784168   \n",
      "Wide_and_Deep                148.798538   41.887012   \n",
      "XGBoost + NN                1495.378174  200.556961   \n",
      "LightGBM + NN                160.771698   26.745409   \n",
      "AutoInt + NN                  54.887157   24.947227   \n",
      "FT-Transformer              1986.821655  173.623123   \n",
      "Neural Architecture Search   1937.61853  116.816948   \n",
      "KAN                          348.740906   42.645691   \n",
      "NODE                         220.164841   45.408577   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.001997   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000996   \n",
      "KNN                                           0.000997   \n",
      "Decision Tree                                 0.004986   \n",
      "Random Forest                                 0.827819   \n",
      "Gradient Boosting                             0.594411   \n",
      "XGBoost                                       0.226393   \n",
      "LightGBM                                      0.134639   \n",
      "CatBoost                                      0.516617   \n",
      "MLP                                           3.516119   \n",
      "DNN                                           5.147236   \n",
      "DCN                                           5.152221   \n",
      "Wide_and_Deep                                 5.563657   \n",
      "XGBoost + NN                                  0.494677   \n",
      "LightGBM + NN                                 3.776898   \n",
      "AutoInt + NN                                  2.099385   \n",
      "FT-Transformer                                4.492984   \n",
      "Neural Architecture Search                    3.009951   \n",
      "KAN                                           3.274243   \n",
      "NODE                                          5.119312   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                               0.0   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002991   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                   0.02191   \n",
      "Gradient Boosting                              0.000998   \n",
      "XGBoost                                        0.001995   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.000998   \n",
      "MLP                                            0.000997   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                   0.000998   \n",
      "LightGBM + NN                                       0.0   \n",
      "AutoInt + NN                                        0.0   \n",
      "FT-Transformer                                 0.000998   \n",
      "Neural Architecture Search                     0.000997   \n",
      "KAN                                                 0.0   \n",
      "NODE                                           0.001996   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                           3.62482   \n",
      "Ridge                                      0.070811   \n",
      "Lasso                                      0.063828   \n",
      "KNN                                        0.192878   \n",
      "Decision Tree                              0.444815   \n",
      "Random Forest                             56.899384   \n",
      "Gradient Boosting                         18.245007   \n",
      "XGBoost                                   10.466175   \n",
      "LightGBM                                  14.049421   \n",
      "CatBoost                                  80.273983   \n",
      "MLP                                      134.738025   \n",
      "DNN                                       44.896956   \n",
      "DCN                                       61.166004   \n",
      "Wide_and_Deep                             52.083259   \n",
      "XGBoost + NN                              23.276744   \n",
      "LightGBM + NN                             37.564108   \n",
      "AutoInt + NN                              39.965675   \n",
      "FT-Transformer                            94.385147   \n",
      "Neural Architecture Search                39.444057   \n",
      "KAN                                       27.943278   \n",
      "NODE                                       57.35708   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                        {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                                   {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting           {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                         {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN                {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 88, 'hidde...  \n",
      "KAN                         {'hidden_dim': 243, 'learning_rate': 0.0095177...  \n",
      "NODE                        {'num_layers': 4, 'num_trees': 4, 'tree_dim': ...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24452210.38356| val_rmse: 4699.63363|  0:00:00s\n",
      "epoch 1  | loss: 24217496.08219| val_rmse: 4654.52826|  0:00:02s\n",
      "epoch 2  | loss: 23688959.72603| val_rmse: 4564.50705|  0:00:03s\n",
      "epoch 3  | loss: 22578614.28767| val_rmse: 4394.94117|  0:00:04s\n",
      "epoch 4  | loss: 20718039.78082| val_rmse: 3963.44347|  0:00:05s\n",
      "epoch 5  | loss: 17054912.93151| val_rmse: 3176.75772|  0:00:06s\n",
      "epoch 6  | loss: 11635838.08219| val_rmse: 1909.47455|  0:00:07s\n",
      "epoch 7  | loss: 5556557.08904| val_rmse: 1018.34752|  0:00:08s\n",
      "epoch 8  | loss: 1772434.12329| val_rmse: 2144.8063|  0:00:09s\n",
      "epoch 9  | loss: 798684.55565| val_rmse: 2773.9313|  0:00:10s\n",
      "epoch 10 | loss: 631880.92808| val_rmse: 1450.50534|  0:00:11s\n",
      "epoch 11 | loss: 547872.94521| val_rmse: 985.79514|  0:00:12s\n",
      "epoch 12 | loss: 339305.11644| val_rmse: 1246.25962|  0:00:14s\n",
      "epoch 13 | loss: 320559.12414| val_rmse: 910.63539|  0:00:15s\n",
      "epoch 14 | loss: 329848.46704| val_rmse: 841.20781|  0:00:16s\n",
      "epoch 15 | loss: 293487.26199| val_rmse: 742.04869|  0:00:17s\n",
      "epoch 16 | loss: 328706.93985| val_rmse: 798.37885|  0:00:18s\n",
      "epoch 17 | loss: 326109.33069| val_rmse: 1260.26016|  0:00:19s\n",
      "epoch 18 | loss: 320951.90668| val_rmse: 1121.51974|  0:00:20s\n",
      "epoch 19 | loss: 341689.56678| val_rmse: 454.58426|  0:00:21s\n",
      "epoch 20 | loss: 395982.99315| val_rmse: 622.70512|  0:00:22s\n",
      "epoch 21 | loss: 265385.3476| val_rmse: 978.17354|  0:00:23s\n",
      "epoch 22 | loss: 280153.45205| val_rmse: 717.76505|  0:00:24s\n",
      "epoch 23 | loss: 250745.23887| val_rmse: 636.96568|  0:00:25s\n",
      "epoch 24 | loss: 315961.75771| val_rmse: 611.00796|  0:00:27s\n",
      "epoch 25 | loss: 321844.2911| val_rmse: 402.03546|  0:00:27s\n",
      "epoch 26 | loss: 312968.11901| val_rmse: 807.12526|  0:00:28s\n",
      "epoch 27 | loss: 255290.07449| val_rmse: 728.22623|  0:00:29s\n",
      "epoch 28 | loss: 274126.25985| val_rmse: 539.41069|  0:00:30s\n",
      "epoch 29 | loss: 265571.66524| val_rmse: 424.67077|  0:00:31s\n",
      "epoch 30 | loss: 221471.00428| val_rmse: 333.5982|  0:00:32s\n",
      "epoch 31 | loss: 179985.16096| val_rmse: 296.98709|  0:00:33s\n",
      "epoch 32 | loss: 253913.69863| val_rmse: 276.04302|  0:00:34s\n",
      "epoch 33 | loss: 198886.73887| val_rmse: 363.22544|  0:00:35s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 32 and best_val_rmse = 276.04302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 10:57:05,736] Trial 0 finished with value: 76199.7500882905 and parameters: {'n_d': 8, 'n_a': 20, 'n_steps': 10, 'gamma': 1.2357305038853414, 'lambda_sparse': 1.2028553619612116e-05, 'learning_rate': 0.07369443477914511, 'batch_size': 64, 'num_epochs': 34}. Best is trial 0 with value: 76199.7500882905.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24413452.35616| val_rmse: 4662.21144|  0:00:00s\n",
      "epoch 1  | loss: 24096747.91781| val_rmse: 4545.17092|  0:00:01s\n",
      "epoch 2  | loss: 23496734.19178| val_rmse: 4458.74753|  0:00:01s\n",
      "epoch 3  | loss: 22488928.32877| val_rmse: 4321.93886|  0:00:02s\n",
      "epoch 4  | loss: 21054356.16438| val_rmse: 4134.86253|  0:00:02s\n",
      "epoch 5  | loss: 19083935.91781| val_rmse: 3873.31716|  0:00:03s\n",
      "epoch 6  | loss: 16472949.28767| val_rmse: 3516.44022|  0:00:04s\n",
      "epoch 7  | loss: 13370234.94521| val_rmse: 3004.28952|  0:00:04s\n",
      "epoch 8  | loss: 10066234.0| val_rmse: 2589.60607|  0:00:05s\n",
      "epoch 9  | loss: 6950763.09589| val_rmse: 1511.68131|  0:00:05s\n",
      "epoch 10 | loss: 4203651.22603| val_rmse: 1165.24406|  0:00:06s\n",
      "epoch 11 | loss: 1837469.48116| val_rmse: 1395.40444|  0:00:06s\n",
      "epoch 12 | loss: 725363.07577| val_rmse: 2289.79228|  0:00:07s\n",
      "epoch 13 | loss: 432445.27269| val_rmse: 2576.39484|  0:00:07s\n",
      "epoch 14 | loss: 410195.5854| val_rmse: 2258.3481|  0:00:08s\n",
      "epoch 15 | loss: 414475.28168| val_rmse: 2017.71208|  0:00:08s\n",
      "epoch 16 | loss: 200178.48673| val_rmse: 1932.87841|  0:00:09s\n",
      "epoch 17 | loss: 214432.42808| val_rmse: 1797.06195|  0:00:10s\n",
      "Stop training because you reached max_epochs = 18 with best_epoch = 10 and best_val_rmse = 1165.24406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 10:57:16,088] Trial 1 finished with value: 1357793.7128748728 and parameters: {'n_d': 14, 'n_a': 55, 'n_steps': 8, 'gamma': 1.1467907717737122, 'lambda_sparse': 0.0008453940954853845, 'learning_rate': 0.078293614917899, 'batch_size': 128, 'num_epochs': 18}. Best is trial 0 with value: 76199.7500882905.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24486133.75342| val_rmse: 4731.08483|  0:00:00s\n",
      "epoch 1  | loss: 24485065.53425| val_rmse: 4731.10512|  0:00:00s\n",
      "epoch 2  | loss: 24484192.30137| val_rmse: 4731.10133|  0:00:00s\n",
      "epoch 3  | loss: 24483564.82192| val_rmse: 4730.96262|  0:00:00s\n",
      "epoch 4  | loss: 24482556.60274| val_rmse: 4730.93655|  0:00:01s\n",
      "epoch 5  | loss: 24482091.94521| val_rmse: 4730.92259|  0:00:01s\n",
      "epoch 6  | loss: 24480974.82192| val_rmse: 4730.88377|  0:00:01s\n",
      "epoch 7  | loss: 24480501.50685| val_rmse: 4730.8717|  0:00:02s\n",
      "epoch 8  | loss: 24479470.73973| val_rmse: 4730.7897|  0:00:02s\n",
      "epoch 9  | loss: 24478917.45205| val_rmse: 4730.67139|  0:00:02s\n",
      "epoch 10 | loss: 24477880.73973| val_rmse: 4730.58492|  0:00:02s\n",
      "epoch 11 | loss: 24477315.53425| val_rmse: 4730.52335|  0:00:02s\n",
      "epoch 12 | loss: 24476212.63014| val_rmse: 4730.49947|  0:00:03s\n",
      "epoch 13 | loss: 24475692.60274| val_rmse: 4730.44644|  0:00:03s\n",
      "epoch 14 | loss: 24474866.82192| val_rmse: 4730.36838|  0:00:03s\n",
      "epoch 15 | loss: 24473854.84932| val_rmse: 4730.27369|  0:00:03s\n",
      "epoch 16 | loss: 24473110.68493| val_rmse: 4730.20592|  0:00:04s\n",
      "epoch 17 | loss: 24471881.17808| val_rmse: 4730.13599|  0:00:04s\n",
      "epoch 18 | loss: 24471367.06849| val_rmse: 4730.04524|  0:00:04s\n",
      "epoch 19 | loss: 24470232.90411| val_rmse: 4729.93458|  0:00:04s\n",
      "epoch 20 | loss: 24469438.32877| val_rmse: 4729.82472|  0:00:05s\n",
      "epoch 21 | loss: 24468584.52055| val_rmse: 4729.73879|  0:00:05s\n",
      "epoch 22 | loss: 24467682.08219| val_rmse: 4729.64595|  0:00:05s\n",
      "epoch 23 | loss: 24466408.9863| val_rmse: 4729.55347|  0:00:05s\n",
      "epoch 24 | loss: 24465440.24658| val_rmse: 4729.46371|  0:00:06s\n",
      "epoch 25 | loss: 24464888.9863| val_rmse: 4729.39719|  0:00:06s\n",
      "epoch 26 | loss: 24463828.41096| val_rmse: 4729.33224|  0:00:06s\n",
      "epoch 27 | loss: 24463027.61644| val_rmse: 4729.23402|  0:00:06s\n",
      "epoch 28 | loss: 24461782.0| val_rmse: 4729.16632|  0:00:07s\n",
      "epoch 29 | loss: 24461265.91781| val_rmse: 4729.09891|  0:00:07s\n",
      "epoch 30 | loss: 24460370.41096| val_rmse: 4728.99218|  0:00:07s\n",
      "epoch 31 | loss: 24459496.90411| val_rmse: 4728.85313|  0:00:07s\n",
      "epoch 32 | loss: 24457903.83562| val_rmse: 4728.79741|  0:00:07s\n",
      "epoch 33 | loss: 24457485.53425| val_rmse: 4728.70064|  0:00:08s\n",
      "epoch 34 | loss: 24456810.9589| val_rmse: 4728.62056|  0:00:08s\n",
      "epoch 35 | loss: 24455428.49315| val_rmse: 4728.50376|  0:00:08s\n",
      "epoch 36 | loss: 24454746.93151| val_rmse: 4728.42491|  0:00:08s\n",
      "epoch 37 | loss: 24454006.79452| val_rmse: 4728.29604|  0:00:09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 10:57:25,681] Trial 2 finished with value: 22355799.065775596 and parameters: {'n_d': 8, 'n_a': 48, 'n_steps': 3, 'gamma': 1.2834916001409709, 'lambda_sparse': 4.920167058145946e-05, 'learning_rate': 0.0005914133483117685, 'batch_size': 128, 'num_epochs': 39}. Best is trial 0 with value: 76199.7500882905.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 | loss: 24453136.10959| val_rmse: 4728.19194|  0:00:09s\n",
      "Stop training because you reached max_epochs = 39 with best_epoch = 38 and best_val_rmse = 4728.19194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24480732.54795| val_rmse: 4729.77081|  0:00:00s\n",
      "epoch 1  | loss: 24467553.75342| val_rmse: 4727.84109|  0:00:01s\n",
      "epoch 2  | loss: 24451686.76712| val_rmse: 4726.90527|  0:00:01s\n",
      "epoch 3  | loss: 24438625.09589| val_rmse: 4725.62572|  0:00:02s\n",
      "epoch 4  | loss: 24427010.9589| val_rmse: 4724.59537|  0:00:02s\n",
      "epoch 5  | loss: 24412820.13699| val_rmse: 4724.0054|  0:00:03s\n",
      "epoch 6  | loss: 24403594.63014| val_rmse: 4722.80717|  0:00:04s\n",
      "epoch 7  | loss: 24392263.34247| val_rmse: 4722.24709|  0:00:04s\n",
      "epoch 8  | loss: 24382890.27397| val_rmse: 4721.12929|  0:00:05s\n",
      "epoch 9  | loss: 24372352.9589| val_rmse: 4719.83091|  0:00:05s\n",
      "epoch 10 | loss: 24362125.9726| val_rmse: 4718.92372|  0:00:06s\n",
      "epoch 11 | loss: 24351390.82192| val_rmse: 4717.92682|  0:00:06s\n",
      "epoch 12 | loss: 24342890.63014| val_rmse: 4717.17622|  0:00:07s\n",
      "epoch 13 | loss: 24338006.13699| val_rmse: 4716.68049|  0:00:08s\n",
      "epoch 14 | loss: 24330807.67123| val_rmse: 4715.95697|  0:00:08s\n",
      "epoch 15 | loss: 24318998.52055| val_rmse: 4714.33363|  0:00:09s\n",
      "epoch 16 | loss: 24310053.20548| val_rmse: 4713.86171|  0:00:09s\n",
      "epoch 17 | loss: 24301666.10959| val_rmse: 4712.79736|  0:00:10s\n",
      "epoch 18 | loss: 24290314.79452| val_rmse: 4711.4072|  0:00:11s\n",
      "epoch 19 | loss: 24277209.42466| val_rmse: 4710.75468|  0:00:11s\n",
      "epoch 20 | loss: 24262757.17808| val_rmse: 4709.60422|  0:00:12s\n",
      "epoch 21 | loss: 24252461.89041| val_rmse: 4708.26654|  0:00:12s\n",
      "epoch 22 | loss: 24238392.87671| val_rmse: 4706.60085|  0:00:13s\n",
      "epoch 23 | loss: 24221015.58904| val_rmse: 4704.9074|  0:00:13s\n",
      "epoch 24 | loss: 24204352.82192| val_rmse: 4704.01095|  0:00:14s\n",
      "epoch 25 | loss: 24188638.79452| val_rmse: 4703.1127|  0:00:15s\n",
      "epoch 26 | loss: 24176361.94521| val_rmse: 4701.68622|  0:00:15s\n",
      "epoch 27 | loss: 24166100.05479| val_rmse: 4699.89225|  0:00:16s\n",
      "Stop training because you reached max_epochs = 28 with best_epoch = 27 and best_val_rmse = 4699.89225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 10:57:42,514] Trial 3 finished with value: 22088987.204420976 and parameters: {'n_d': 28, 'n_a': 10, 'n_steps': 10, 'gamma': 1.5433950905710887, 'lambda_sparse': 4.5040512233346874e-05, 'learning_rate': 0.0030057199503047993, 'batch_size': 128, 'num_epochs': 28}. Best is trial 0 with value: 76199.7500882905.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:38: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\2639847764.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24194045.23288| val_rmse: 4639.36141|  0:00:01s\n",
      "epoch 1  | loss: 23182136.93151| val_rmse: 4484.09677|  0:00:03s\n",
      "epoch 2  | loss: 21296910.19178| val_rmse: 4261.9565|  0:00:05s\n",
      "epoch 3  | loss: 18329377.56164| val_rmse: 3920.79005|  0:00:07s\n",
      "epoch 4  | loss: 14170822.13699| val_rmse: 3229.0747|  0:00:09s\n",
      "epoch 5  | loss: 9001659.46404| val_rmse: 2482.45138|  0:00:11s\n",
      "epoch 6  | loss: 4746198.55137| val_rmse: 1812.74439|  0:00:13s\n",
      "epoch 7  | loss: 1873058.25342| val_rmse: 921.70611|  0:00:14s\n",
      "epoch 8  | loss: 854688.23716| val_rmse: 739.16533|  0:00:16s\n",
      "epoch 9  | loss: 980503.25342| val_rmse: 659.14614|  0:00:18s\n",
      "epoch 10 | loss: 826414.8476| val_rmse: 677.52522|  0:00:20s\n",
      "epoch 11 | loss: 798523.35788| val_rmse: 555.56248|  0:00:22s\n",
      "epoch 12 | loss: 573836.73031| val_rmse: 532.15729|  0:00:23s\n",
      "epoch 13 | loss: 618201.31571| val_rmse: 468.65609|  0:00:25s\n",
      "epoch 14 | loss: 572446.72774| val_rmse: 452.31771|  0:00:27s\n",
      "epoch 15 | loss: 609627.91096| val_rmse: 422.4022|  0:00:29s\n",
      "epoch 16 | loss: 554579.8536| val_rmse: 442.49441|  0:00:31s\n",
      "epoch 17 | loss: 523162.61943| val_rmse: 560.19937|  0:00:33s\n",
      "epoch 18 | loss: 720508.1524| val_rmse: 454.18728|  0:00:35s\n",
      "epoch 19 | loss: 570829.34589| val_rmse: 446.74241|  0:00:37s\n",
      "epoch 20 | loss: 522474.18022| val_rmse: 421.43243|  0:00:38s\n",
      "epoch 21 | loss: 484539.20719| val_rmse: 380.05239|  0:00:40s\n",
      "epoch 22 | loss: 545338.87757| val_rmse: 575.33615|  0:00:42s\n",
      "epoch 23 | loss: 522007.70462| val_rmse: 436.54429|  0:00:44s\n",
      "epoch 24 | loss: 318014.33926| val_rmse: 434.79829|  0:00:46s\n",
      "epoch 25 | loss: 479434.57363| val_rmse: 332.956 |  0:00:48s\n",
      "epoch 26 | loss: 389047.64212| val_rmse: 312.31327|  0:00:50s\n",
      "epoch 27 | loss: 365991.25086| val_rmse: 404.85416|  0:00:53s\n",
      "epoch 28 | loss: 517243.69606| val_rmse: 376.85986|  0:00:55s\n",
      "Stop training because you reached max_epochs = 29 with best_epoch = 26 and best_val_rmse = 312.31327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 10:58:39,802] Trial 4 finished with value: 97539.57682783014 and parameters: {'n_d': 50, 'n_a': 35, 'n_steps': 9, 'gamma': 1.116697820972655, 'lambda_sparse': 2.6334513984166876e-05, 'learning_rate': 0.019131552859410053, 'batch_size': 32, 'num_epochs': 29}. Best is trial 0 with value: 76199.7500882905.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24452210.38356| val_rmse: 4699.63363|  0:00:01s\n",
      "epoch 1  | loss: 24217496.08219| val_rmse: 4654.52826|  0:00:02s\n",
      "epoch 2  | loss: 23688959.72603| val_rmse: 4564.50705|  0:00:03s\n",
      "epoch 3  | loss: 22578614.28767| val_rmse: 4394.94117|  0:00:04s\n",
      "epoch 4  | loss: 20718039.78082| val_rmse: 3963.44347|  0:00:05s\n",
      "epoch 5  | loss: 17054912.93151| val_rmse: 3176.75772|  0:00:05s\n",
      "epoch 6  | loss: 11635838.08219| val_rmse: 1909.47455|  0:00:06s\n",
      "epoch 7  | loss: 5556557.08904| val_rmse: 1018.34752|  0:00:07s\n",
      "epoch 8  | loss: 1772434.12329| val_rmse: 2144.8063|  0:00:08s\n",
      "epoch 9  | loss: 798684.55565| val_rmse: 2773.9313|  0:00:09s\n",
      "epoch 10 | loss: 631880.92808| val_rmse: 1450.50534|  0:00:10s\n",
      "epoch 11 | loss: 547872.94521| val_rmse: 985.79514|  0:00:11s\n",
      "epoch 12 | loss: 339305.11644| val_rmse: 1246.25962|  0:00:12s\n",
      "epoch 13 | loss: 320559.12414| val_rmse: 910.63539|  0:00:13s\n",
      "epoch 14 | loss: 329848.46704| val_rmse: 841.20781|  0:00:14s\n",
      "epoch 15 | loss: 293487.26199| val_rmse: 742.04869|  0:00:15s\n",
      "epoch 16 | loss: 328706.93985| val_rmse: 798.37885|  0:00:16s\n",
      "epoch 17 | loss: 326109.33069| val_rmse: 1260.26016|  0:00:17s\n",
      "epoch 18 | loss: 320951.90668| val_rmse: 1121.51974|  0:00:18s\n",
      "epoch 19 | loss: 341689.56678| val_rmse: 454.58426|  0:00:19s\n",
      "epoch 20 | loss: 395982.99315| val_rmse: 622.70512|  0:00:20s\n",
      "epoch 21 | loss: 265385.3476| val_rmse: 978.17354|  0:00:21s\n",
      "epoch 22 | loss: 280153.45205| val_rmse: 717.76505|  0:00:22s\n",
      "epoch 23 | loss: 250745.23887| val_rmse: 636.96568|  0:00:23s\n",
      "epoch 24 | loss: 315961.75771| val_rmse: 611.00796|  0:00:24s\n",
      "epoch 25 | loss: 321844.2911| val_rmse: 402.03546|  0:00:25s\n",
      "epoch 26 | loss: 312968.11901| val_rmse: 807.12526|  0:00:26s\n",
      "epoch 27 | loss: 255290.07449| val_rmse: 728.22623|  0:00:27s\n",
      "epoch 28 | loss: 274126.25985| val_rmse: 539.41069|  0:00:28s\n",
      "epoch 29 | loss: 265571.66524| val_rmse: 424.67077|  0:00:28s\n",
      "epoch 30 | loss: 221471.00428| val_rmse: 333.5982|  0:00:29s\n",
      "epoch 31 | loss: 179985.16096| val_rmse: 296.98709|  0:00:30s\n",
      "epoch 32 | loss: 253913.69863| val_rmse: 276.04302|  0:00:31s\n",
      "epoch 33 | loss: 198886.73887| val_rmse: 363.22544|  0:00:32s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 32 and best_val_rmse = 276.04302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 25169468.10278| val_rmse: 4634.35733|  0:00:00s\n",
      "epoch 1  | loss: 24999813.30193| val_rmse: 4605.66987|  0:00:01s\n",
      "epoch 2  | loss: 24792012.30407| val_rmse: 4564.68171|  0:00:02s\n",
      "epoch 3  | loss: 24441288.6167| val_rmse: 4504.18548|  0:00:03s\n",
      "epoch 4  | loss: 23889182.68951| val_rmse: 4371.63852|  0:00:03s\n",
      "epoch 5  | loss: 22598617.75161| val_rmse: 4061.71298|  0:00:04s\n",
      "epoch 6  | loss: 20115911.48394| val_rmse: 3570.03648|  0:00:05s\n",
      "epoch 7  | loss: 16286042.11349| val_rmse: 2737.0023|  0:00:06s\n",
      "epoch 8  | loss: 11496987.93576| val_rmse: 1389.00026|  0:00:06s\n",
      "epoch 9  | loss: 7094881.79015| val_rmse: 1240.31717|  0:00:07s\n",
      "epoch 10 | loss: 3389781.08137| val_rmse: 2095.9284|  0:00:08s\n",
      "epoch 11 | loss: 1332766.15538| val_rmse: 2364.6902|  0:00:09s\n",
      "epoch 12 | loss: 804508.03239| val_rmse: 2313.56318|  0:00:09s\n",
      "epoch 13 | loss: 723574.74762| val_rmse: 1531.84139|  0:00:10s\n",
      "epoch 14 | loss: 896832.00187| val_rmse: 1210.2204|  0:00:11s\n",
      "epoch 15 | loss: 939486.89601| val_rmse: 1842.25972|  0:00:12s\n",
      "epoch 16 | loss: 673056.40525| val_rmse: 2304.61094|  0:00:12s\n",
      "epoch 17 | loss: 687539.26365| val_rmse: 1666.07673|  0:00:13s\n",
      "epoch 18 | loss: 513608.54999| val_rmse: 1302.88079|  0:00:14s\n",
      "epoch 19 | loss: 506806.22765| val_rmse: 1341.93995|  0:00:15s\n",
      "epoch 20 | loss: 712914.89441| val_rmse: 1291.65096|  0:00:16s\n",
      "epoch 21 | loss: 492288.70025| val_rmse: 1192.15482|  0:00:17s\n",
      "epoch 22 | loss: 386508.77376| val_rmse: 1219.66973|  0:00:17s\n",
      "epoch 23 | loss: 433946.64267| val_rmse: 1117.50424|  0:00:18s\n",
      "epoch 24 | loss: 596806.2864| val_rmse: 608.19245|  0:00:19s\n",
      "epoch 25 | loss: 378316.0091| val_rmse: 671.9245|  0:00:20s\n",
      "epoch 26 | loss: 438124.15806| val_rmse: 702.96739|  0:00:21s\n",
      "epoch 27 | loss: 383095.23314| val_rmse: 928.73151|  0:00:22s\n",
      "epoch 28 | loss: 379732.64387| val_rmse: 589.92535|  0:00:22s\n",
      "epoch 29 | loss: 386764.7136| val_rmse: 473.72552|  0:00:23s\n",
      "epoch 30 | loss: 403786.41562| val_rmse: 551.10276|  0:00:24s\n",
      "epoch 31 | loss: 327755.91997| val_rmse: 497.68323|  0:00:25s\n",
      "epoch 32 | loss: 380249.27914| val_rmse: 473.89786|  0:00:25s\n",
      "epoch 33 | loss: 250736.0722| val_rmse: 437.09134|  0:00:26s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 33 and best_val_rmse = 437.09134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24022977.05782| val_rmse: 5106.86792|  0:00:00s\n",
      "epoch 1  | loss: 23853603.24197| val_rmse: 5079.44603|  0:00:01s\n",
      "epoch 2  | loss: 23587466.26981| val_rmse: 5009.64075|  0:00:02s\n",
      "epoch 3  | loss: 23071381.4818| val_rmse: 4920.32872|  0:00:03s\n",
      "epoch 4  | loss: 22126108.12848| val_rmse: 4734.16062|  0:00:04s\n",
      "epoch 5  | loss: 20528513.88437| val_rmse: 4385.87282|  0:00:04s\n",
      "epoch 6  | loss: 18140814.62527| val_rmse: 3983.69233|  0:00:05s\n",
      "epoch 7  | loss: 15113480.46467| val_rmse: 3271.50572|  0:00:06s\n",
      "epoch 8  | loss: 11156376.73233| val_rmse: 2394.34973|  0:00:06s\n",
      "epoch 9  | loss: 7068091.76338| val_rmse: 1393.096|  0:00:07s\n",
      "epoch 10 | loss: 3221020.48796| val_rmse: 1060.0737|  0:00:08s\n",
      "epoch 11 | loss: 763399.01164| val_rmse: 1982.5137|  0:00:09s\n",
      "epoch 12 | loss: 739029.53466| val_rmse: 1990.16649|  0:00:11s\n",
      "epoch 13 | loss: 430174.3298| val_rmse: 2193.3237|  0:00:12s\n",
      "epoch 14 | loss: 229050.01077| val_rmse: 2101.11711|  0:00:14s\n",
      "epoch 15 | loss: 311527.60278| val_rmse: 1549.46568|  0:00:15s\n",
      "epoch 16 | loss: 216900.46527| val_rmse: 1300.56906|  0:00:15s\n",
      "epoch 17 | loss: 225284.67361| val_rmse: 1208.3703|  0:00:16s\n",
      "epoch 18 | loss: 287906.79547| val_rmse: 1175.81641|  0:00:17s\n",
      "epoch 19 | loss: 294210.21918| val_rmse: 982.36124|  0:00:18s\n",
      "epoch 20 | loss: 245937.74987| val_rmse: 632.98398|  0:00:19s\n",
      "epoch 21 | loss: 282621.13367| val_rmse: 821.35053|  0:00:19s\n",
      "epoch 22 | loss: 193902.59305| val_rmse: 458.96713|  0:00:20s\n",
      "epoch 23 | loss: 196491.6927| val_rmse: 429.81495|  0:00:21s\n",
      "epoch 24 | loss: 231563.44178| val_rmse: 382.36559|  0:00:22s\n",
      "epoch 25 | loss: 211169.88086| val_rmse: 616.04386|  0:00:23s\n",
      "epoch 26 | loss: 237032.75124| val_rmse: 464.39882|  0:00:24s\n",
      "epoch 27 | loss: 207329.90906| val_rmse: 346.24277|  0:00:24s\n",
      "epoch 28 | loss: 138187.29269| val_rmse: 460.31431|  0:00:25s\n",
      "epoch 29 | loss: 197728.1172| val_rmse: 302.26744|  0:00:26s\n",
      "epoch 30 | loss: 130771.39882| val_rmse: 247.46543|  0:00:27s\n",
      "epoch 31 | loss: 126077.31183| val_rmse: 271.78191|  0:00:29s\n",
      "epoch 32 | loss: 104093.02429| val_rmse: 270.24045|  0:00:31s\n",
      "epoch 33 | loss: 117484.26233| val_rmse: 371.54146|  0:00:32s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 30 and best_val_rmse = 247.46543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24747712.65525| val_rmse: 4811.90522|  0:00:00s\n",
      "epoch 1  | loss: 24557667.4454| val_rmse: 4782.56199|  0:00:01s\n",
      "epoch 2  | loss: 24273730.55246| val_rmse: 4739.589|  0:00:02s\n",
      "epoch 3  | loss: 23649539.01071| val_rmse: 4682.37813|  0:00:03s\n",
      "epoch 4  | loss: 22652369.66167| val_rmse: 4549.31562|  0:00:03s\n",
      "epoch 5  | loss: 21356676.70664| val_rmse: 4348.59452|  0:00:05s\n",
      "epoch 6  | loss: 19334711.32762| val_rmse: 4035.45819|  0:00:05s\n",
      "epoch 7  | loss: 16228246.19486| val_rmse: 3299.68273|  0:00:06s\n",
      "epoch 8  | loss: 12037929.55567| val_rmse: 2453.85656|  0:00:07s\n",
      "epoch 9  | loss: 7195992.21039| val_rmse: 1596.89912|  0:00:08s\n",
      "epoch 10 | loss: 3545655.49518| val_rmse: 1025.51965|  0:00:08s\n",
      "epoch 11 | loss: 1339541.21547| val_rmse: 1576.31228|  0:00:09s\n",
      "epoch 12 | loss: 713916.54216| val_rmse: 3177.95412|  0:00:10s\n",
      "epoch 13 | loss: 871446.77797| val_rmse: 2450.64964|  0:00:11s\n",
      "epoch 14 | loss: 671382.32468| val_rmse: 1300.88421|  0:00:12s\n",
      "epoch 15 | loss: 482878.0273| val_rmse: 1360.5307|  0:00:13s\n",
      "epoch 16 | loss: 366532.7136| val_rmse: 1164.49424|  0:00:14s\n",
      "epoch 17 | loss: 415053.42806| val_rmse: 1290.97191|  0:00:15s\n",
      "epoch 18 | loss: 672110.03433| val_rmse: 865.94279|  0:00:15s\n",
      "epoch 19 | loss: 513824.75776| val_rmse: 816.00147|  0:00:16s\n",
      "epoch 20 | loss: 652675.05173| val_rmse: 986.5171|  0:00:17s\n",
      "epoch 21 | loss: 414546.71922| val_rmse: 1173.64045|  0:00:18s\n",
      "epoch 22 | loss: 291529.63986| val_rmse: 1064.71825|  0:00:19s\n",
      "epoch 23 | loss: 306407.58706| val_rmse: 831.75105|  0:00:19s\n",
      "epoch 24 | loss: 431626.74364| val_rmse: 868.43263|  0:00:20s\n",
      "epoch 25 | loss: 466512.62567| val_rmse: 1035.7058|  0:00:21s\n",
      "epoch 26 | loss: 381584.86386| val_rmse: 1005.79246|  0:00:22s\n",
      "epoch 27 | loss: 369566.37062| val_rmse: 1109.88512|  0:00:23s\n",
      "epoch 28 | loss: 349874.92907| val_rmse: 777.6751|  0:00:23s\n",
      "epoch 29 | loss: 348314.66743| val_rmse: 667.22484|  0:00:24s\n",
      "epoch 30 | loss: 272887.08177| val_rmse: 747.12336|  0:00:25s\n",
      "epoch 31 | loss: 278611.17853| val_rmse: 579.69983|  0:00:26s\n",
      "epoch 32 | loss: 304530.57746| val_rmse: 505.82756|  0:00:26s\n",
      "epoch 33 | loss: 270937.97859| val_rmse: 444.67786|  0:00:27s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 33 and best_val_rmse = 444.67786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 23906812.66381| val_rmse: 5148.89825|  0:00:00s\n",
      "epoch 1  | loss: 23752080.88651| val_rmse: 5110.82522|  0:00:01s\n",
      "epoch 2  | loss: 23499087.45182| val_rmse: 5067.97158|  0:00:02s\n",
      "epoch 3  | loss: 23089232.82227| val_rmse: 5005.11234|  0:00:03s\n",
      "epoch 4  | loss: 22268926.61884| val_rmse: 4838.43999|  0:00:04s\n",
      "epoch 5  | loss: 20586522.10278| val_rmse: 4537.17993|  0:00:05s\n",
      "epoch 6  | loss: 18113000.7409| val_rmse: 4145.85509|  0:00:06s\n",
      "epoch 7  | loss: 14726377.80728| val_rmse: 3287.68825|  0:00:08s\n",
      "epoch 8  | loss: 10221076.97002| val_rmse: 2294.47 |  0:00:09s\n",
      "epoch 9  | loss: 5188571.50749| val_rmse: 1074.07164|  0:00:10s\n",
      "epoch 10 | loss: 1565902.23608| val_rmse: 1276.1466|  0:00:11s\n",
      "epoch 11 | loss: 652345.85934| val_rmse: 3324.45964|  0:00:12s\n",
      "epoch 12 | loss: 459559.01934| val_rmse: 3008.62838|  0:00:13s\n",
      "epoch 13 | loss: 408233.44841| val_rmse: 2649.63594|  0:00:13s\n",
      "epoch 14 | loss: 437759.57317| val_rmse: 2137.59463|  0:00:14s\n",
      "epoch 15 | loss: 411092.74993| val_rmse: 1091.28597|  0:00:15s\n",
      "epoch 16 | loss: 495981.84275| val_rmse: 953.09098|  0:00:16s\n",
      "epoch 17 | loss: 459112.82575| val_rmse: 1392.88625|  0:00:16s\n",
      "epoch 18 | loss: 397760.88357| val_rmse: 1264.72301|  0:00:17s\n",
      "epoch 19 | loss: 457651.44141| val_rmse: 1204.96636|  0:00:18s\n",
      "epoch 20 | loss: 338902.15966| val_rmse: 833.38948|  0:00:19s\n",
      "epoch 21 | loss: 282529.68857| val_rmse: 629.39014|  0:00:20s\n",
      "epoch 22 | loss: 382598.25542| val_rmse: 470.77891|  0:00:20s\n",
      "epoch 23 | loss: 360277.17191| val_rmse: 532.41492|  0:00:21s\n",
      "epoch 24 | loss: 293531.49321| val_rmse: 658.01276|  0:00:25s\n",
      "epoch 25 | loss: 312484.93108| val_rmse: 561.6433|  0:00:26s\n",
      "epoch 26 | loss: 373209.61215| val_rmse: 477.80745|  0:00:27s\n",
      "epoch 27 | loss: 330646.92167| val_rmse: 466.39442|  0:00:28s\n",
      "epoch 28 | loss: 429902.78125| val_rmse: 454.37509|  0:00:29s\n",
      "epoch 29 | loss: 294523.33445| val_rmse: 455.29751|  0:00:29s\n",
      "epoch 30 | loss: 257733.96668| val_rmse: 1031.47922|  0:00:30s\n",
      "epoch 31 | loss: 224431.41047| val_rmse: 536.64629|  0:00:31s\n",
      "epoch 32 | loss: 204056.66515| val_rmse: 581.583 |  0:00:32s\n",
      "epoch 33 | loss: 299034.63467| val_rmse: 351.45978|  0:00:32s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 33 and best_val_rmse = 351.45978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24498725.57265| val_rmse: 4920.85647|  0:00:00s\n",
      "epoch 1  | loss: 24331322.90598| val_rmse: 4870.4998|  0:00:01s\n",
      "epoch 2  | loss: 24052455.7265| val_rmse: 4847.83747|  0:00:02s\n",
      "epoch 3  | loss: 23510567.62393| val_rmse: 4769.44624|  0:00:02s\n",
      "epoch 4  | loss: 22564583.24786| val_rmse: 4569.64282|  0:00:03s\n",
      "epoch 5  | loss: 20996712.18803| val_rmse: 4206.66165|  0:00:04s\n",
      "epoch 6  | loss: 18488050.0| val_rmse: 3727.02999|  0:00:05s\n",
      "epoch 7  | loss: 15182956.94017| val_rmse: 3004.43316|  0:00:05s\n",
      "epoch 8  | loss: 11095453.80342| val_rmse: 1734.46439|  0:00:06s\n",
      "epoch 9  | loss: 6745256.61752| val_rmse: 903.92626|  0:00:07s\n",
      "epoch 10 | loss: 3025149.5| val_rmse: 2414.38468|  0:00:08s\n",
      "epoch 11 | loss: 891748.80101| val_rmse: 3096.08453|  0:00:09s\n",
      "epoch 12 | loss: 618228.76656| val_rmse: 3533.15333|  0:00:09s\n",
      "epoch 13 | loss: 585663.2703| val_rmse: 3127.70878|  0:00:10s\n",
      "epoch 14 | loss: 606631.04808| val_rmse: 2297.85575|  0:00:11s\n",
      "epoch 15 | loss: 479162.47569| val_rmse: 1549.14043|  0:00:12s\n",
      "epoch 16 | loss: 490564.19551| val_rmse: 972.8267|  0:00:12s\n",
      "epoch 17 | loss: 534503.53419| val_rmse: 955.41737|  0:00:13s\n",
      "epoch 18 | loss: 368271.38916| val_rmse: 713.92092|  0:00:14s\n",
      "epoch 19 | loss: 298828.96368| val_rmse: 568.16461|  0:00:15s\n",
      "epoch 20 | loss: 387447.66774| val_rmse: 508.38382|  0:00:16s\n",
      "epoch 21 | loss: 262364.24853| val_rmse: 419.70367|  0:00:17s\n",
      "epoch 22 | loss: 234761.68162| val_rmse: 546.7736|  0:00:18s\n",
      "epoch 23 | loss: 210543.30262| val_rmse: 511.04108|  0:00:18s\n",
      "epoch 24 | loss: 252832.19792| val_rmse: 683.19319|  0:00:19s\n",
      "epoch 25 | loss: 370790.4773| val_rmse: 674.52608|  0:00:20s\n",
      "epoch 26 | loss: 198676.21541| val_rmse: 390.04516|  0:00:21s\n",
      "epoch 27 | loss: 302443.75988| val_rmse: 513.9396|  0:00:21s\n",
      "epoch 28 | loss: 290946.61004| val_rmse: 553.54411|  0:00:22s\n",
      "epoch 29 | loss: 266233.26763| val_rmse: 286.76889|  0:00:23s\n",
      "epoch 30 | loss: 210966.06303| val_rmse: 533.30963|  0:00:24s\n",
      "epoch 31 | loss: 269995.293| val_rmse: 409.54879|  0:00:24s\n",
      "epoch 32 | loss: 269694.45753| val_rmse: 362.66114|  0:00:25s\n",
      "epoch 33 | loss: 175559.16426| val_rmse: 378.24502|  0:00:26s\n",
      "Stop training because you reached max_epochs = 34 with best_epoch = 29 and best_val_rmse = 286.76889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-06 11:01:40,724] A new study created in memory with name: no-name-9b091097-93c8-4796-b545-3b00490a06cd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression                     0.0          0.0          0.0       1.0   \n",
      "Ridge                            0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                            0.013676     0.116946     0.096502       1.0   \n",
      "KNN                         161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree                30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest                11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting             8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost                       9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM                     12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost                      6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                         112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                             3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                          11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep                11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN                   1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN                17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN                 10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer                 3346669.75  1829.390503   1536.50708  0.022254   \n",
      "Neural Architecture Search     3838256.75  1959.146973  1625.576172 -0.121365   \n",
      "KAN                             91932.125   303.203094   220.203583  0.973142   \n",
      "NODE                         26062.462891   161.438721    114.89048  0.992386   \n",
      "TabNet                       76199.750088   276.043022   206.164172  0.977738   \n",
      "\n",
      "                           CV Mean RMSE CV Std RMSE  \\\n",
      "Linear Regression                   0.0         0.0   \n",
      "Ridge                          0.798848    0.144414   \n",
      "Lasso                          0.122118    0.006306   \n",
      "KNN                          522.242956   25.290932   \n",
      "Decision Tree                 239.46383   32.541677   \n",
      "Random Forest                163.954712   27.180044   \n",
      "Gradient Boosting            125.340587   11.473233   \n",
      "XGBoost                      123.654731   13.853207   \n",
      "LightGBM                      161.25728    16.98225   \n",
      "CatBoost                     109.085283   17.258991   \n",
      "MLP                          398.072005   21.709468   \n",
      "DNN                         1941.074219   88.739159   \n",
      "DCN                          169.078384   42.784168   \n",
      "Wide_and_Deep                148.798538   41.887012   \n",
      "XGBoost + NN                1495.378174  200.556961   \n",
      "LightGBM + NN                160.771698   26.745409   \n",
      "AutoInt + NN                  54.887157   24.947227   \n",
      "FT-Transformer              1986.821655  173.623123   \n",
      "Neural Architecture Search   1937.61853  116.816948   \n",
      "KAN                          348.740906   42.645691   \n",
      "NODE                         220.164841   45.408577   \n",
      "TabNet                       353.492661   78.741915   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.001997   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000996   \n",
      "KNN                                           0.000997   \n",
      "Decision Tree                                 0.004986   \n",
      "Random Forest                                 0.827819   \n",
      "Gradient Boosting                             0.594411   \n",
      "XGBoost                                       0.226393   \n",
      "LightGBM                                      0.134639   \n",
      "CatBoost                                      0.516617   \n",
      "MLP                                           3.516119   \n",
      "DNN                                           5.147236   \n",
      "DCN                                           5.152221   \n",
      "Wide_and_Deep                                 5.563657   \n",
      "XGBoost + NN                                  0.494677   \n",
      "LightGBM + NN                                 3.776898   \n",
      "AutoInt + NN                                  2.099385   \n",
      "FT-Transformer                                4.492984   \n",
      "Neural Architecture Search                    3.009951   \n",
      "KAN                                           3.274243   \n",
      "NODE                                          5.119312   \n",
      "TabNet                                       33.054176   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                               0.0   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002991   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                   0.02191   \n",
      "Gradient Boosting                              0.000998   \n",
      "XGBoost                                        0.001995   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.000998   \n",
      "MLP                                            0.000997   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                   0.000998   \n",
      "LightGBM + NN                                       0.0   \n",
      "AutoInt + NN                                        0.0   \n",
      "FT-Transformer                                 0.000998   \n",
      "Neural Architecture Search                     0.000997   \n",
      "KAN                                                 0.0   \n",
      "NODE                                           0.001996   \n",
      "TabNet                                         0.083776   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                           3.62482   \n",
      "Ridge                                      0.070811   \n",
      "Lasso                                      0.063828   \n",
      "KNN                                        0.192878   \n",
      "Decision Tree                              0.444815   \n",
      "Random Forest                             56.899384   \n",
      "Gradient Boosting                         18.245007   \n",
      "XGBoost                                   10.466175   \n",
      "LightGBM                                  14.049421   \n",
      "CatBoost                                  80.273983   \n",
      "MLP                                      134.738025   \n",
      "DNN                                       44.896956   \n",
      "DCN                                       61.166004   \n",
      "Wide_and_Deep                             52.083259   \n",
      "XGBoost + NN                              23.276744   \n",
      "LightGBM + NN                             37.564108   \n",
      "AutoInt + NN                              39.965675   \n",
      "FT-Transformer                            94.385147   \n",
      "Neural Architecture Search                39.444057   \n",
      "KAN                                       27.943278   \n",
      "NODE                                       57.35708   \n",
      "TabNet                                   311.295133   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                        {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                                   {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting           {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                         {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN                {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 88, 'hidde...  \n",
      "KAN                         {'hidden_dim': 243, 'learning_rate': 0.0095177...  \n",
      "NODE                        {'num_layers': 4, 'num_trees': 4, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 8, 'n_a': 20, 'n_steps': 10, 'gamma': ...  \n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 11:01:57,007] Trial 0 finished with value: 5691730.5 and parameters: {'heads': 5, 'dim': 205, 'depth': 5, 'mlp_dim': 245, 'dropout': 0.1675405283586982, 'learning_rate': 0.012337207550717549, 'batch_size': 128, 'num_epochs': 31}. Best is trial 0 with value: 5691730.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 11:02:15,790] Trial 1 finished with value: 3447522.75 and parameters: {'heads': 8, 'dim': 176, 'depth': 6, 'mlp_dim': 182, 'dropout': 0.29067775782058936, 'learning_rate': 0.027944919564903945, 'batch_size': 64, 'num_epochs': 26}. Best is trial 1 with value: 3447522.75.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 11:02:22,351] Trial 2 finished with value: 22112586.0 and parameters: {'heads': 5, 'dim': 205, 'depth': 4, 'mlp_dim': 189, 'dropout': 0.40429882267265616, 'learning_rate': 0.0007160417220854537, 'batch_size': 128, 'num_epochs': 24}. Best is trial 1 with value: 3447522.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 11:02:26,387] Trial 3 finished with value: 16255861.0 and parameters: {'heads': 4, 'dim': 108, 'depth': 5, 'mlp_dim': 229, 'dropout': 0.29721483214698835, 'learning_rate': 0.04360158880489874, 'batch_size': 256, 'num_epochs': 12}. Best is trial 1 with value: 3447522.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_4912\\301764479.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-08-06 11:02:40,309] Trial 4 finished with value: 3443656.0 and parameters: {'heads': 2, 'dim': 118, 'depth': 5, 'mlp_dim': 197, 'dropout': 0.2816817989919019, 'learning_rate': 0.025506298392554246, 'batch_size': 64, 'num_epochs': 30}. Best is trial 4 with value: 3443656.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      MSE         RMSE          MAE  R2 Score  \\\n",
      "Linear Regression                     0.0          0.0          0.0       1.0   \n",
      "Ridge                            0.242336     0.492277     0.398738       1.0   \n",
      "Lasso                            0.013676     0.116946     0.096502       1.0   \n",
      "KNN                         161030.811799   401.286446   296.465121  0.952954   \n",
      "Decision Tree                30628.274734   175.009356   123.978311  0.991052   \n",
      "Random Forest                11178.686815   105.729309    65.787632  0.996734   \n",
      "Gradient Boosting             8478.942997    92.081176      64.2453  0.997523   \n",
      "XGBoost                       9031.190737    95.032577     66.63099  0.997361   \n",
      "LightGBM                     12274.312832   110.789498    72.435986  0.996414   \n",
      "CatBoost                      6256.066985    79.095303    61.673092  0.998172   \n",
      "MLP                         112512.623433   335.429014   264.401499  0.967129   \n",
      "DNN                             3837166.5  1958.868652  1641.655029 -0.121047   \n",
      "DCN                          11457.277344   107.038673    84.923981  0.996653   \n",
      "Wide_and_Deep                11511.054688   107.289581     75.51059  0.996637   \n",
      "XGBoost + NN                   1651738.75  1285.199829    876.83136  0.517436   \n",
      "LightGBM + NN                17473.076172    132.18576    92.866531  0.994895   \n",
      "AutoInt + NN                 10133.049805   100.663048    51.398952   0.99704   \n",
      "FT-Transformer                 3346669.75  1829.390503   1536.50708  0.022254   \n",
      "Neural Architecture Search     3838256.75  1959.146973  1625.576172 -0.121365   \n",
      "KAN                             91932.125   303.203094   220.203583  0.973142   \n",
      "NODE                         26062.462891   161.438721    114.89048  0.992386   \n",
      "TabNet                       76199.750088   276.043022   206.164172  0.977738   \n",
      "SAINT                          3465546.25  1861.597778  1564.615845 -0.012476   \n",
      "\n",
      "                           CV Mean RMSE CV Std RMSE  \\\n",
      "Linear Regression                   0.0         0.0   \n",
      "Ridge                          0.798848    0.144414   \n",
      "Lasso                          0.122118    0.006306   \n",
      "KNN                          522.242956   25.290932   \n",
      "Decision Tree                 239.46383   32.541677   \n",
      "Random Forest                163.954712   27.180044   \n",
      "Gradient Boosting            125.340587   11.473233   \n",
      "XGBoost                      123.654731   13.853207   \n",
      "LightGBM                      161.25728    16.98225   \n",
      "CatBoost                     109.085283   17.258991   \n",
      "MLP                          398.072005   21.709468   \n",
      "DNN                         1941.074219   88.739159   \n",
      "DCN                          169.078384   42.784168   \n",
      "Wide_and_Deep                148.798538   41.887012   \n",
      "XGBoost + NN                1495.378174  200.556961   \n",
      "LightGBM + NN                160.771698   26.745409   \n",
      "AutoInt + NN                  54.887157   24.947227   \n",
      "FT-Transformer              1986.821655  173.623123   \n",
      "Neural Architecture Search   1937.61853  116.816948   \n",
      "KAN                          348.740906   42.645691   \n",
      "NODE                         220.164841   45.408577   \n",
      "TabNet                       353.492661   78.741915   \n",
      "SAINT                       1960.286377  103.876442   \n",
      "\n",
      "                           Training Time (Best Params)  \\\n",
      "Linear Regression                             0.001997   \n",
      "Ridge                                         0.000997   \n",
      "Lasso                                         0.000996   \n",
      "KNN                                           0.000997   \n",
      "Decision Tree                                 0.004986   \n",
      "Random Forest                                 0.827819   \n",
      "Gradient Boosting                             0.594411   \n",
      "XGBoost                                       0.226393   \n",
      "LightGBM                                      0.134639   \n",
      "CatBoost                                      0.516617   \n",
      "MLP                                           3.516119   \n",
      "DNN                                           5.147236   \n",
      "DCN                                           5.152221   \n",
      "Wide_and_Deep                                 5.563657   \n",
      "XGBoost + NN                                  0.494677   \n",
      "LightGBM + NN                                 3.776898   \n",
      "AutoInt + NN                                  2.099385   \n",
      "FT-Transformer                                4.492984   \n",
      "Neural Architecture Search                    3.009951   \n",
      "KAN                                           3.274243   \n",
      "NODE                                          5.119312   \n",
      "TabNet                                       33.054176   \n",
      "SAINT                                        13.324934   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Linear Regression                                   0.0   \n",
      "Ridge                                               0.0   \n",
      "Lasso                                               0.0   \n",
      "KNN                                            0.002991   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                   0.02191   \n",
      "Gradient Boosting                              0.000998   \n",
      "XGBoost                                        0.001995   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.000998   \n",
      "MLP                                            0.000997   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                  0.000997   \n",
      "XGBoost + NN                                   0.000998   \n",
      "LightGBM + NN                                       0.0   \n",
      "AutoInt + NN                                        0.0   \n",
      "FT-Transformer                                 0.000998   \n",
      "Neural Architecture Search                     0.000997   \n",
      "KAN                                                 0.0   \n",
      "NODE                                           0.001996   \n",
      "TabNet                                         0.083776   \n",
      "SAINT                                          0.013962   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Linear Regression                           3.62482   \n",
      "Ridge                                      0.070811   \n",
      "Lasso                                      0.063828   \n",
      "KNN                                        0.192878   \n",
      "Decision Tree                              0.444815   \n",
      "Random Forest                             56.899384   \n",
      "Gradient Boosting                         18.245007   \n",
      "XGBoost                                   10.466175   \n",
      "LightGBM                                  14.049421   \n",
      "CatBoost                                  80.273983   \n",
      "MLP                                      134.738025   \n",
      "DNN                                       44.896956   \n",
      "DCN                                       61.166004   \n",
      "Wide_and_Deep                             52.083259   \n",
      "XGBoost + NN                              23.276744   \n",
      "LightGBM + NN                             37.564108   \n",
      "AutoInt + NN                              39.965675   \n",
      "FT-Transformer                            94.385147   \n",
      "Neural Architecture Search                39.444057   \n",
      "KAN                                       27.943278   \n",
      "NODE                                       57.35708   \n",
      "TabNet                                   311.295133   \n",
      "SAINT                                    128.556877   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Linear Regression                                                          {}  \n",
      "Ridge                                        {'alpha': 0.1, 'solver': 'auto'}  \n",
      "Lasso                                   {'alpha': 0.1, 'selection': 'cyclic'}  \n",
      "KNN                         {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}  \n",
      "Decision Tree               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Random Forest               {'max_depth': 20, 'min_samples_leaf': 2, 'min_...  \n",
      "Gradient Boosting           {'learning_rate': 0.05, 'max_depth': 4, 'n_est...  \n",
      "XGBoost                     {'gamma': 0, 'learning_rate': 0.05, 'max_depth...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 200, 'n...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 400, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.001, 'hidden...  \n",
      "DNN                         {'hidden_dim_0': 122, 'hidden_dim_1': 140, 'hi...  \n",
      "DCN                         {'cross_layers': 1, 'hidden_layer_0': 141, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 176, 'hidden_layer_1': 219,...  \n",
      "XGBoost + NN                {'n_estimators': 142, 'max_depth': 5, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 132, 'max_depth': 3, 'lgb_lea...  \n",
      "AutoInt + NN                {'num_heads': 3, 'embedding_dim': 54, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 8, 'embedding_dim': 24, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 88, 'hidde...  \n",
      "KAN                         {'hidden_dim': 243, 'learning_rate': 0.0095177...  \n",
      "NODE                        {'num_layers': 4, 'num_trees': 4, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 8, 'n_a': 20, 'n_steps': 10, 'gamma': ...  \n",
      "SAINT                       {'heads': 2, 'dim': 118, 'depth': 5, 'mlp_dim'...  \n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"bike\"  # Change this to any word you like\n",
    "df =  pd.read_csv(f'Dataset/{file_prefix}.csv')\n",
    "df = encode_categorical_data(df)\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X, y = apply_robust_transform(X, y)\n",
    "#X, y = apply_synthetic_data_to_training(X, y)\n",
    "\n",
    "result = model_comparison(df, 'Y')\n",
    "print(result)\n",
    "result, best_params = mlp_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = dnn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = dcn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = wide_and_deep_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = xgb_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = lgbm_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = autoint_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = ft_transformer_nn_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = neural_architecture_search(X, y, result)\n",
    "print(result)\n",
    "result, best_params = kan_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = node_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = tabnet_comparison(X, y, result)\n",
    "print(result)\n",
    "result, best_params = saint_comparison(X, y, result)\n",
    "print(result)\n",
    "\n",
    "result.to_csv(f'result/comparison/regression/{file_prefix}_result.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
