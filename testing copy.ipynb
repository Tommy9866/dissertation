{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "df =  pd.read_csv('winequality-red.csv',delimiter=';')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['Y'] = np.where(df['Y'] >= 6, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Y_column = df['Y'].copy()\n",
    "df.drop('Y', axis=1, inplace=True)\n",
    "\n",
    "# Identify categorical data (change this based on your actual data)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Standardize only the continuous (non-categorical) columns\n",
    "continuous_cols = df.columns.difference(categorical_cols)  # Gets the difference, i.e., continuous cols\n",
    "df[continuous_cols] = (df[continuous_cols] - df[continuous_cols].mean()) / df[continuous_cols].std()\n",
    "\n",
    "# Filter out outliers in continuous data (|z-score| > 5)\n",
    "mask = (np.abs(df[continuous_cols]) < 5).all(axis=1)\n",
    "df = df[mask]\n",
    "\n",
    "# Reattach the target variable 'Y' to the DataFrame\n",
    "df['Y'] = Y_column[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for column in categorical_cols:\n",
    "    # Ensure the column is of type object (string) or category\n",
    "    if df[column].dtype == 'object' or df[column].dtype.name == 'category':\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        \n",
    "df['Y'], unique = pd.factorize(df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:15:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:15:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 361, number of negative: 415\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465206 -> initscore=-0.139401\n",
      "[LightGBM] [Info] Start training from score -0.139401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 361, number of negative: 415\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465206 -> initscore=-0.139401\n",
      "[LightGBM] [Info] Start training from score -0.139401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary of models and their reduced hyperparameter grids\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000), {\n",
    "        'C': [0.01, 0.1, 1],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 4]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 4]\n",
    "    }),\n",
    "    'LightGBM': (LGBMClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 50]\n",
    "    }),\n",
    "    'CatBoost': (CatBoostClassifier(verbose=0), {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'depth': [4, 6]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Measure training time for best parameters\n",
    "    best_param_train_start = time.time()\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    best_param_train_time = time.time() - best_param_train_start\n",
    "\n",
    "    # Measure inference time for best parameters\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "    \n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if len(np.unique(y)) == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "    else:  # Multiclass classification\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr', average='macro')\n",
    "\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'AUC Score': auc,\n",
    "        'Training Time (Best Params)': best_param_train_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "result = pd.DataFrame(results).T\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "activation: tanh\n",
      "alpha: 0.01\n",
      "hidden_layer_sizes: (100,)\n",
      "learning_rate: constant\n",
      "solver: adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the MLP model and its hyperparameter grid\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with StratifiedKFold\n",
    "start_time = time.time()\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(mlp, param_grid=param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Measure training time for best parameters\n",
    "best_param_train_start = time.time()\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - best_param_train_start\n",
    "\n",
    "# Measure inference time for best parameters\n",
    "inference_start_time = time.time()\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr', average='macro')\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['MLP'] = [accuracy, auc, training_time, inference_time, computation_time, grid_search.best_params_]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:19:59,336] A new study created in memory with name: no-name-7449ee2f-298a-4134-9e5c-edd4dc861a46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:16,422] Trial 0 finished with value: 0.6717948717948717 and parameters: {'hidden_dim_0': 235, 'hidden_dim_1': 183, 'hidden_dim_2': 182, 'learning_rate': 0.039847770232168886, 'batch_size': 32, 'num_epochs': 66}. Best is trial 0 with value: 0.6717948717948717.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:16,803] Trial 1 finished with value: 0.7333333333333333 and parameters: {'hidden_dim_0': 136, 'hidden_dim_1': 64, 'hidden_dim_2': 253, 'learning_rate': 0.024940195511163176, 'batch_size': 256, 'num_epochs': 10}. Best is trial 1 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:20,911] Trial 2 finished with value: 0.7743589743589744 and parameters: {'hidden_dim_0': 68, 'hidden_dim_1': 215, 'hidden_dim_2': 39, 'learning_rate': 0.00018052008818718822, 'batch_size': 64, 'num_epochs': 57}. Best is trial 2 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:21,859] Trial 3 finished with value: 0.764102564102564 and parameters: {'hidden_dim_0': 250, 'hidden_dim_1': 59, 'hidden_dim_2': 190, 'learning_rate': 0.014049519442907131, 'batch_size': 128, 'num_epochs': 20}. Best is trial 2 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:22,801] Trial 4 finished with value: 0.6974358974358974 and parameters: {'hidden_dim_0': 78, 'hidden_dim_1': 50, 'hidden_dim_2': 150, 'learning_rate': 0.048090155765625614, 'batch_size': 256, 'num_epochs': 31}. Best is trial 2 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:26,474] Trial 5 finished with value: 0.764102564102564 and parameters: {'hidden_dim_0': 239, 'hidden_dim_1': 201, 'hidden_dim_2': 162, 'learning_rate': 0.0007606932271515641, 'batch_size': 32, 'num_epochs': 26}. Best is trial 2 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:28,133] Trial 6 finished with value: 0.7794871794871795 and parameters: {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hidden_dim_2': 247, 'learning_rate': 0.0006736636096398536, 'batch_size': 64, 'num_epochs': 17}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:33,833] Trial 7 finished with value: 0.7794871794871795 and parameters: {'hidden_dim_0': 240, 'hidden_dim_1': 221, 'hidden_dim_2': 221, 'learning_rate': 0.0003605648356529857, 'batch_size': 64, 'num_epochs': 66}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:34,631] Trial 8 finished with value: 0.7487179487179487 and parameters: {'hidden_dim_0': 138, 'hidden_dim_1': 45, 'hidden_dim_2': 219, 'learning_rate': 0.000816090565309488, 'batch_size': 128, 'num_epochs': 18}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:37,600] Trial 9 finished with value: 0.7538461538461538 and parameters: {'hidden_dim_0': 36, 'hidden_dim_1': 48, 'hidden_dim_2': 74, 'learning_rate': 0.0004530657456721047, 'batch_size': 256, 'num_epochs': 97}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:41,222] Trial 10 finished with value: 0.7230769230769231 and parameters: {'hidden_dim_0': 187, 'hidden_dim_1': 253, 'hidden_dim_2': 99, 'learning_rate': 0.0030435186285878834, 'batch_size': 64, 'num_epochs': 43}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:47,932] Trial 11 finished with value: 0.764102564102564 and parameters: {'hidden_dim_0': 185, 'hidden_dim_1': 131, 'hidden_dim_2': 251, 'learning_rate': 0.00010467180054240395, 'batch_size': 64, 'num_epochs': 76}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:54,838] Trial 12 finished with value: 0.7743589743589744 and parameters: {'hidden_dim_0': 191, 'hidden_dim_1': 256, 'hidden_dim_2': 213, 'learning_rate': 0.0024862136896633548, 'batch_size': 64, 'num_epochs': 79}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:20:58,892] Trial 13 finished with value: 0.7487179487179487 and parameters: {'hidden_dim_0': 208, 'hidden_dim_1': 147, 'hidden_dim_2': 226, 'learning_rate': 0.00039489917192484804, 'batch_size': 64, 'num_epochs': 46}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:02,601] Trial 14 finished with value: 0.7282051282051282 and parameters: {'hidden_dim_0': 215, 'hidden_dim_1': 220, 'hidden_dim_2': 126, 'learning_rate': 0.001060212956227316, 'batch_size': 64, 'num_epochs': 40}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:07,424] Trial 15 finished with value: 0.7435897435897436 and parameters: {'hidden_dim_0': 154, 'hidden_dim_1': 160, 'hidden_dim_2': 200, 'learning_rate': 0.0076887549835118205, 'batch_size': 64, 'num_epochs': 60}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:14,915] Trial 16 finished with value: 0.7743589743589744 and parameters: {'hidden_dim_0': 165, 'hidden_dim_1': 116, 'hidden_dim_2': 252, 'learning_rate': 0.000259097551755403, 'batch_size': 64, 'num_epochs': 93}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:20,697] Trial 17 finished with value: 0.7538461538461538 and parameters: {'hidden_dim_0': 107, 'hidden_dim_1': 232, 'hidden_dim_2': 168, 'learning_rate': 0.0016957742875738794, 'batch_size': 128, 'num_epochs': 76}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:28,397] Trial 18 finished with value: 0.7435897435897436 and parameters: {'hidden_dim_0': 218, 'hidden_dim_1': 178, 'hidden_dim_2': 229, 'learning_rate': 0.005829575340070699, 'batch_size': 32, 'num_epochs': 50}. Best is trial 6 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1656234290.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:31,151] Trial 19 finished with value: 0.7589743589743589 and parameters: {'hidden_dim_0': 255, 'hidden_dim_1': 93, 'hidden_dim_2': 132, 'learning_rate': 0.00023006754870535373, 'batch_size': 64, 'num_epochs': 34}. Best is trial 6 with value: 0.7794871794871795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim_0: 193\n",
      "hidden_dim_1: 234\n",
      "hidden_dim_2: 247\n",
      "learning_rate: 0.0006736636096398536\n",
      "batch_size: 64\n",
      "num_epochs: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_dims = [trial.suggest_int(f'hidden_dim_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = DNN(input_dim, hidden_dims, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = DNN(input_dim, [best_params[f'hidden_dim_{i}'] for i in range(3)], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DNN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:21:33,469] A new study created in memory with name: no-name-9b0175e2-7e6f-4a9b-9b07-0d9a97c7e8ab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:36,380] Trial 0 finished with value: 0.6820512820512821 and parameters: {'cross_layers': 2, 'hidden_layer_0': 204, 'hidden_layer_1': 220, 'hidden_layer_2': 80, 'learning_rate': 0.0788376381887241, 'batch_size': 256, 'num_epochs': 45}. Best is trial 0 with value: 0.6820512820512821.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:42,154] Trial 1 finished with value: 0.7435897435897436 and parameters: {'cross_layers': 5, 'hidden_layer_0': 120, 'hidden_layer_1': 54, 'hidden_layer_2': 35, 'learning_rate': 0.007413982251349101, 'batch_size': 64, 'num_epochs': 55}. Best is trial 1 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:43,161] Trial 2 finished with value: 0.7333333333333333 and parameters: {'cross_layers': 4, 'hidden_layer_0': 202, 'hidden_layer_1': 105, 'hidden_layer_2': 152, 'learning_rate': 0.02944352208816394, 'batch_size': 128, 'num_epochs': 14}. Best is trial 1 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:21:56,367] Trial 3 finished with value: 0.7692307692307693 and parameters: {'cross_layers': 1, 'hidden_layer_0': 216, 'hidden_layer_1': 167, 'hidden_layer_2': 71, 'learning_rate': 0.0006254431295577154, 'batch_size': 32, 'num_epochs': 93}. Best is trial 3 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:00,106] Trial 4 finished with value: 0.7025641025641025 and parameters: {'cross_layers': 3, 'hidden_layer_0': 44, 'hidden_layer_1': 234, 'hidden_layer_2': 90, 'learning_rate': 0.00843187520086906, 'batch_size': 256, 'num_epochs': 86}. Best is trial 3 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:07,746] Trial 5 finished with value: 0.764102564102564 and parameters: {'cross_layers': 2, 'hidden_layer_0': 164, 'hidden_layer_1': 234, 'hidden_layer_2': 170, 'learning_rate': 0.0001627609544539298, 'batch_size': 32, 'num_epochs': 48}. Best is trial 3 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:14,312] Trial 6 finished with value: 0.7897435897435897 and parameters: {'cross_layers': 4, 'hidden_layer_0': 85, 'hidden_layer_1': 133, 'hidden_layer_2': 124, 'learning_rate': 0.0045771689455373025, 'batch_size': 128, 'num_epochs': 87}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:18,581] Trial 7 finished with value: 0.7487179487179487 and parameters: {'cross_layers': 2, 'hidden_layer_0': 251, 'hidden_layer_1': 73, 'hidden_layer_2': 140, 'learning_rate': 0.003994206691372569, 'batch_size': 32, 'num_epochs': 23}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:31,066] Trial 8 finished with value: 0.7384615384615385 and parameters: {'cross_layers': 5, 'hidden_layer_0': 166, 'hidden_layer_1': 89, 'hidden_layer_2': 255, 'learning_rate': 0.00961195369431471, 'batch_size': 32, 'num_epochs': 53}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:32,079] Trial 9 finished with value: 0.6615384615384615 and parameters: {'cross_layers': 2, 'hidden_layer_0': 148, 'hidden_layer_1': 102, 'hidden_layer_2': 228, 'learning_rate': 0.00011973800337052514, 'batch_size': 256, 'num_epochs': 19}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:37,562] Trial 10 finished with value: 0.7538461538461538 and parameters: {'cross_layers': 4, 'hidden_layer_0': 77, 'hidden_layer_1': 152, 'hidden_layer_2': 184, 'learning_rate': 0.0008612430346895095, 'batch_size': 128, 'num_epochs': 76}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:43,160] Trial 11 finished with value: 0.7794871794871795 and parameters: {'cross_layers': 1, 'hidden_layer_0': 94, 'hidden_layer_1': 167, 'hidden_layer_2': 97, 'learning_rate': 0.0008906910719136696, 'batch_size': 128, 'num_epochs': 97}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:48,783] Trial 12 finished with value: 0.7794871794871795 and parameters: {'cross_layers': 4, 'hidden_layer_0': 97, 'hidden_layer_1': 180, 'hidden_layer_2': 123, 'learning_rate': 0.0010904816096036485, 'batch_size': 128, 'num_epochs': 75}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:53,993] Trial 13 finished with value: 0.7230769230769231 and parameters: {'cross_layers': 1, 'hidden_layer_0': 38, 'hidden_layer_1': 130, 'hidden_layer_2': 106, 'learning_rate': 0.0017091164653136631, 'batch_size': 128, 'num_epochs': 100}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:22:58,670] Trial 14 finished with value: 0.7743589743589744 and parameters: {'cross_layers': 3, 'hidden_layer_0': 80, 'hidden_layer_1': 201, 'hidden_layer_2': 44, 'learning_rate': 0.0003701028009030686, 'batch_size': 128, 'num_epochs': 76}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:06,316] Trial 15 finished with value: 0.7897435897435897 and parameters: {'cross_layers': 4, 'hidden_layer_0': 117, 'hidden_layer_1': 131, 'hidden_layer_2': 196, 'learning_rate': 0.0029192210106142686, 'batch_size': 64, 'num_epochs': 67}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:14,303] Trial 16 finished with value: 0.7692307692307693 and parameters: {'cross_layers': 4, 'hidden_layer_0': 125, 'hidden_layer_1': 127, 'hidden_layer_2': 203, 'learning_rate': 0.003455253538451431, 'batch_size': 64, 'num_epochs': 66}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:18,290] Trial 17 finished with value: 0.7333333333333333 and parameters: {'cross_layers': 5, 'hidden_layer_0': 54, 'hidden_layer_1': 132, 'hidden_layer_2': 202, 'learning_rate': 0.02181340754426153, 'batch_size': 64, 'num_epochs': 33}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:24,506] Trial 18 finished with value: 0.7538461538461538 and parameters: {'cross_layers': 3, 'hidden_layer_0': 118, 'hidden_layer_1': 44, 'hidden_layer_2': 152, 'learning_rate': 0.0022300645143823765, 'batch_size': 64, 'num_epochs': 66}. Best is trial 6 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3493045964.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:35,116] Trial 19 finished with value: 0.6820512820512821 and parameters: {'cross_layers': 4, 'hidden_layer_0': 65, 'hidden_layer_1': 201, 'hidden_layer_2': 223, 'learning_rate': 0.02017445992103461, 'batch_size': 64, 'num_epochs': 88}. Best is trial 6 with value: 0.7897435897435897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "cross_layers: 4\n",
      "hidden_layer_0: 85\n",
      "hidden_layer_1: 133\n",
      "hidden_layer_2: 124\n",
      "learning_rate: 0.0045771689455373025\n",
      "batch_size: 128\n",
      "num_epochs: 87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x0, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x0 = x0.unsqueeze(2)\n",
    "        interaction = torch.matmul(x0, torch.matmul(x.transpose(1, 2), self.weight))\n",
    "        return x0.squeeze(2) + interaction.squeeze(2) + self.bias.T\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, input_dim, cross_layers, hidden_layers, output_dim):\n",
    "        super(DCN, self).__init__()\n",
    "        self.cross_layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(cross_layers)])\n",
    "        \n",
    "        deep_layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "        self.deep_net = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        self.final_layer = nn.Linear(input_dim + hidden_layers[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cross_out = x\n",
    "        for layer in self.cross_layers:\n",
    "            cross_out = layer(x, cross_out)\n",
    "        deep_out = self.deep_net(x)\n",
    "        concat_out = torch.cat([cross_out, deep_out], dim=1)\n",
    "        return self.final_layer(concat_out)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    cross_layers = trial.suggest_int('cross_layers', 1, 5)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = DCN(input_dim, cross_layers, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = DCN(input_dim, best_params['cross_layers'], \n",
    "                 [best_params[f'hidden_layer_{i}'] for i in range(3)], \n",
    "                 output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DCN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:23:41,009] A new study created in memory with name: no-name-71699cb6-bb7f-474c-9c91-096da33e2d23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:51,254] Trial 0 finished with value: 0.7589743589743589 and parameters: {'hidden_layer_0': 134, 'hidden_layer_1': 83, 'hidden_layer_2': 109, 'learning_rate': 0.0007483649547044918, 'batch_size': 32, 'num_epochs': 83}. Best is trial 0 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:23:52,307] Trial 1 finished with value: 0.7230769230769231 and parameters: {'hidden_layer_0': 42, 'hidden_layer_1': 150, 'hidden_layer_2': 214, 'learning_rate': 0.051399645412128415, 'batch_size': 128, 'num_epochs': 22}. Best is trial 0 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:05,435] Trial 2 finished with value: 0.7333333333333333 and parameters: {'hidden_layer_0': 194, 'hidden_layer_1': 253, 'hidden_layer_2': 209, 'learning_rate': 0.026910724450281875, 'batch_size': 32, 'num_epochs': 69}. Best is trial 0 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:08,347] Trial 3 finished with value: 0.7435897435897436 and parameters: {'hidden_layer_0': 182, 'hidden_layer_1': 120, 'hidden_layer_2': 108, 'learning_rate': 0.00011014497685625177, 'batch_size': 64, 'num_epochs': 35}. Best is trial 0 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:11,085] Trial 4 finished with value: 0.7128205128205128 and parameters: {'hidden_layer_0': 166, 'hidden_layer_1': 212, 'hidden_layer_2': 188, 'learning_rate': 0.032880239335341566, 'batch_size': 32, 'num_epochs': 20}. Best is trial 0 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:12,982] Trial 5 finished with value: 0.764102564102564 and parameters: {'hidden_layer_0': 162, 'hidden_layer_1': 120, 'hidden_layer_2': 144, 'learning_rate': 0.032329893470735546, 'batch_size': 128, 'num_epochs': 35}. Best is trial 5 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:15,048] Trial 6 finished with value: 0.7384615384615385 and parameters: {'hidden_layer_0': 195, 'hidden_layer_1': 256, 'hidden_layer_2': 98, 'learning_rate': 0.014873400825110336, 'batch_size': 128, 'num_epochs': 34}. Best is trial 5 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:21,982] Trial 7 finished with value: 0.7692307692307693 and parameters: {'hidden_layer_0': 251, 'hidden_layer_1': 178, 'hidden_layer_2': 99, 'learning_rate': 0.00039866176797080606, 'batch_size': 32, 'num_epochs': 49}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:25,460] Trial 8 finished with value: 0.7692307692307693 and parameters: {'hidden_layer_0': 141, 'hidden_layer_1': 244, 'hidden_layer_2': 87, 'learning_rate': 0.0003198558067260086, 'batch_size': 64, 'num_epochs': 38}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:33,910] Trial 9 finished with value: 0.7384615384615385 and parameters: {'hidden_layer_0': 180, 'hidden_layer_1': 210, 'hidden_layer_2': 251, 'learning_rate': 0.021091722021390945, 'batch_size': 64, 'num_epochs': 80}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:36,543] Trial 10 finished with value: 0.7589743589743589 and parameters: {'hidden_layer_0': 252, 'hidden_layer_1': 186, 'hidden_layer_2': 42, 'learning_rate': 0.0027382852899923336, 'batch_size': 256, 'num_epochs': 61}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:39,613] Trial 11 finished with value: 0.7333333333333333 and parameters: {'hidden_layer_0': 106, 'hidden_layer_1': 36, 'hidden_layer_2': 49, 'learning_rate': 0.00022928557323933886, 'batch_size': 64, 'num_epochs': 45}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:41,911] Trial 12 finished with value: 0.7589743589743589 and parameters: {'hidden_layer_0': 246, 'hidden_layer_1': 171, 'hidden_layer_2': 77, 'learning_rate': 0.0006123120056787173, 'batch_size': 256, 'num_epochs': 52}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:46,224] Trial 13 finished with value: 0.7435897435897436 and parameters: {'hidden_layer_0': 102, 'hidden_layer_1': 219, 'hidden_layer_2': 150, 'learning_rate': 0.0016592004637607548, 'batch_size': 64, 'num_epochs': 47}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:24:59,600] Trial 14 finished with value: 0.764102564102564 and parameters: {'hidden_layer_0': 223, 'hidden_layer_1': 181, 'hidden_layer_2': 74, 'learning_rate': 0.00027059874747515907, 'batch_size': 32, 'num_epochs': 100}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:04,930] Trial 15 finished with value: 0.7487179487179487 and parameters: {'hidden_layer_0': 121, 'hidden_layer_1': 215, 'hidden_layer_2': 140, 'learning_rate': 0.0009914300017711964, 'batch_size': 64, 'num_epochs': 63}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:06,774] Trial 16 finished with value: 0.7384615384615385 and parameters: {'hidden_layer_0': 73, 'hidden_layer_1': 233, 'hidden_layer_2': 69, 'learning_rate': 0.008057524070379709, 'batch_size': 32, 'num_epochs': 12}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:08,527] Trial 17 finished with value: 0.7076923076923077 and parameters: {'hidden_layer_0': 222, 'hidden_layer_1': 150, 'hidden_layer_2': 124, 'learning_rate': 0.0001066698792846026, 'batch_size': 256, 'num_epochs': 39}. Best is trial 7 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:11,510] Trial 18 finished with value: 0.7743589743589744 and parameters: {'hidden_layer_0': 145, 'hidden_layer_1': 101, 'hidden_layer_2': 171, 'learning_rate': 0.00035917700952223973, 'batch_size': 32, 'num_epochs': 23}. Best is trial 18 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3403281423.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:14,314] Trial 19 finished with value: 0.7333333333333333 and parameters: {'hidden_layer_0': 63, 'hidden_layer_1': 80, 'hidden_layer_2': 151, 'learning_rate': 0.007367510903230665, 'batch_size': 32, 'num_epochs': 24}. Best is trial 18 with value: 0.7743589743589744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_layer_0: 145\n",
      "hidden_layer_1: 101\n",
      "hidden_layer_2: 171\n",
      "learning_rate: 0.00035917700952223973\n",
      "batch_size: 32\n",
      "num_epochs: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class WideAndDeepNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(WideAndDeepNetwork, self).__init__()\n",
    "        \n",
    "        # Wide part\n",
    "        self.wide = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        # Deep part\n",
    "        deep_layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "        deep_layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.deep = nn.Sequential(*deep_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        wide_out = self.wide(x)\n",
    "        deep_out = self.deep(x)\n",
    "        return wide_out + deep_out\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = WideAndDeepNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = WideAndDeepNetwork(input_dim, \n",
    "                                [best_params[f'hidden_layer_{i}'] for i in range(3)], \n",
    "                                output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['Wide_and_Deep'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:25:17,315] A new study created in memory with name: no-name-cc01aa6d-730a-48ca-b724-101c9adc575b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:25,298] Trial 0 finished with value: 0.7076923076923077 and parameters: {'n_estimators': 114, 'max_depth': 10, 'xgb_learning_rate': 0.049553802710495774, 'subsample': 0.9706575962810082, 'colsample_bytree': 0.6392478582249902, 'hidden_layer_0': 248, 'hidden_layer_1': 111, 'hidden_layer_2': 113, 'nn_learning_rate': 0.00048584002658937146, 'batch_size': 64, 'num_epochs': 91}. Best is trial 0 with value: 0.7076923076923077.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:32,940] Trial 1 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 90, 'max_depth': 4, 'xgb_learning_rate': 0.0004138793537150062, 'subsample': 0.5895667985622353, 'colsample_bytree': 0.5910554185102996, 'hidden_layer_0': 132, 'hidden_layer_1': 245, 'hidden_layer_2': 175, 'nn_learning_rate': 0.03957488531437121, 'batch_size': 64, 'num_epochs': 77}. Best is trial 0 with value: 0.7076923076923077.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:36,238] Trial 2 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 57, 'max_depth': 3, 'xgb_learning_rate': 0.08854184482596769, 'subsample': 0.5160908755446671, 'colsample_bytree': 0.9951117736086591, 'hidden_layer_0': 172, 'hidden_layer_1': 166, 'hidden_layer_2': 204, 'nn_learning_rate': 0.02849993000789917, 'batch_size': 256, 'num_epochs': 80}. Best is trial 0 with value: 0.7076923076923077.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:41,629] Trial 3 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 190, 'max_depth': 4, 'xgb_learning_rate': 0.00024825139933168795, 'subsample': 0.5290314198892833, 'colsample_bytree': 0.9827879140515461, 'hidden_layer_0': 47, 'hidden_layer_1': 167, 'hidden_layer_2': 119, 'nn_learning_rate': 0.03993049896352764, 'batch_size': 32, 'num_epochs': 39}. Best is trial 0 with value: 0.7076923076923077.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:46,030] Trial 4 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 92, 'max_depth': 6, 'xgb_learning_rate': 0.00020620904688865727, 'subsample': 0.7469354270947539, 'colsample_bytree': 0.819446138605952, 'hidden_layer_0': 156, 'hidden_layer_1': 140, 'hidden_layer_2': 240, 'nn_learning_rate': 0.07266736450156339, 'batch_size': 64, 'num_epochs': 52}. Best is trial 0 with value: 0.7076923076923077.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:47,178] Trial 5 finished with value: 0.717948717948718 and parameters: {'n_estimators': 266, 'max_depth': 3, 'xgb_learning_rate': 0.08187990008021086, 'subsample': 0.6294507330413578, 'colsample_bytree': 0.9971061459064616, 'hidden_layer_0': 199, 'hidden_layer_1': 116, 'hidden_layer_2': 45, 'nn_learning_rate': 0.01571292039934924, 'batch_size': 128, 'num_epochs': 20}. Best is trial 5 with value: 0.717948717948718.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:49,853] Trial 6 finished with value: 0.6256410256410256 and parameters: {'n_estimators': 153, 'max_depth': 5, 'xgb_learning_rate': 0.03758831937381447, 'subsample': 0.8032503586348406, 'colsample_bytree': 0.8709078907006191, 'hidden_layer_0': 92, 'hidden_layer_1': 215, 'hidden_layer_2': 144, 'nn_learning_rate': 0.007863519055160847, 'batch_size': 128, 'num_epochs': 43}. Best is trial 5 with value: 0.717948717948718.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:25:57,566] Trial 7 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 83, 'max_depth': 10, 'xgb_learning_rate': 0.0032989414937526123, 'subsample': 0.680314835947753, 'colsample_bytree': 0.6215553874740369, 'hidden_layer_0': 140, 'hidden_layer_1': 118, 'hidden_layer_2': 187, 'nn_learning_rate': 0.055979118211061815, 'batch_size': 64, 'num_epochs': 84}. Best is trial 5 with value: 0.717948717948718.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:25:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:01,568] Trial 8 finished with value: 0.7230769230769231 and parameters: {'n_estimators': 204, 'max_depth': 6, 'xgb_learning_rate': 0.009570977222417188, 'subsample': 0.6081193952135282, 'colsample_bytree': 0.976420749164824, 'hidden_layer_0': 200, 'hidden_layer_1': 77, 'hidden_layer_2': 50, 'nn_learning_rate': 0.0023981511859368773, 'batch_size': 128, 'num_epochs': 75}. Best is trial 8 with value: 0.7230769230769231.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:07,305] Trial 9 finished with value: 0.7333333333333333 and parameters: {'n_estimators': 168, 'max_depth': 9, 'xgb_learning_rate': 0.010302040960557288, 'subsample': 0.5765698136962468, 'colsample_bytree': 0.9208261386640324, 'hidden_layer_0': 139, 'hidden_layer_1': 244, 'hidden_layer_2': 184, 'nn_learning_rate': 0.0006863642161653594, 'batch_size': 128, 'num_epochs': 83}. Best is trial 9 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:15,453] Trial 10 finished with value: 0.7282051282051282 and parameters: {'n_estimators': 256, 'max_depth': 8, 'xgb_learning_rate': 0.0018810976155044332, 'subsample': 0.8768399641041789, 'colsample_bytree': 0.7319286785091158, 'hidden_layer_0': 91, 'hidden_layer_1': 35, 'hidden_layer_2': 241, 'nn_learning_rate': 0.00012578554699116943, 'batch_size': 32, 'num_epochs': 64}. Best is trial 9 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:24,138] Trial 11 finished with value: 0.7128205128205128 and parameters: {'n_estimators': 300, 'max_depth': 8, 'xgb_learning_rate': 0.0012784099555498785, 'subsample': 0.8962482633902681, 'colsample_bytree': 0.7180505920095, 'hidden_layer_0': 88, 'hidden_layer_1': 34, 'hidden_layer_2': 256, 'nn_learning_rate': 0.00011555358143628357, 'batch_size': 32, 'num_epochs': 64}. Best is trial 9 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:38,433] Trial 12 finished with value: 0.717948717948718 and parameters: {'n_estimators': 240, 'max_depth': 8, 'xgb_learning_rate': 0.008902267418574774, 'subsample': 0.8543094836575233, 'colsample_bytree': 0.7488500005404615, 'hidden_layer_0': 95, 'hidden_layer_1': 32, 'hidden_layer_2': 222, 'nn_learning_rate': 0.0001101041229408147, 'batch_size': 32, 'num_epochs': 99}. Best is trial 9 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:41,989] Trial 13 finished with value: 0.7282051282051282 and parameters: {'n_estimators': 150, 'max_depth': 8, 'xgb_learning_rate': 0.0014152779960227093, 'subsample': 0.7514860874100999, 'colsample_bytree': 0.8702993488910846, 'hidden_layer_0': 35, 'hidden_layer_1': 208, 'hidden_layer_2': 163, 'nn_learning_rate': 0.0005137764882203291, 'batch_size': 128, 'num_epochs': 59}. Best is trial 9 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:45,311] Trial 14 finished with value: 0.7435897435897436 and parameters: {'n_estimators': 241, 'max_depth': 9, 'xgb_learning_rate': 0.011804878471083257, 'subsample': 0.9797541576977822, 'colsample_bytree': 0.8157941330446775, 'hidden_layer_0': 114, 'hidden_layer_1': 72, 'hidden_layer_2': 218, 'nn_learning_rate': 0.00039813514148554884, 'batch_size': 256, 'num_epochs': 67}. Best is trial 14 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:49,731] Trial 15 finished with value: 0.7230769230769231 and parameters: {'n_estimators': 218, 'max_depth': 9, 'xgb_learning_rate': 0.013088693757403996, 'subsample': 0.9833937813073546, 'colsample_bytree': 0.8896632585807316, 'hidden_layer_0': 124, 'hidden_layer_1': 71, 'hidden_layer_2': 209, 'nn_learning_rate': 0.0011499679952658292, 'batch_size': 256, 'num_epochs': 100}. Best is trial 14 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:53,847] Trial 16 finished with value: 0.7384615384615385 and parameters: {'n_estimators': 163, 'max_depth': 9, 'xgb_learning_rate': 0.005388813412937068, 'subsample': 0.7029088310174938, 'colsample_bytree': 0.8063229515435896, 'hidden_layer_0': 187, 'hidden_layer_1': 250, 'hidden_layer_2': 139, 'nn_learning_rate': 0.00041060959021146963, 'batch_size': 256, 'num_epochs': 70}. Best is trial 14 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:55,410] Trial 17 finished with value: 0.717948717948718 and parameters: {'n_estimators': 134, 'max_depth': 7, 'xgb_learning_rate': 0.003949323316723574, 'subsample': 0.6953751298872851, 'colsample_bytree': 0.6700118810935503, 'hidden_layer_0': 227, 'hidden_layer_1': 196, 'hidden_layer_2': 84, 'nn_learning_rate': 0.00024803736428807355, 'batch_size': 256, 'num_epochs': 27}. Best is trial 14 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:26:58,991] Trial 18 finished with value: 0.7692307692307693 and parameters: {'n_estimators': 295, 'max_depth': 9, 'xgb_learning_rate': 0.022415372047585046, 'subsample': 0.927244647425161, 'colsample_bytree': 0.8065167263666334, 'hidden_layer_0': 182, 'hidden_layer_1': 83, 'hidden_layer_2': 137, 'nn_learning_rate': 0.0020732215195219347, 'batch_size': 256, 'num_epochs': 71}. Best is trial 18 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:26:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2693607433.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:27:01,250] Trial 19 finished with value: 0.7538461538461538 and parameters: {'n_estimators': 300, 'max_depth': 7, 'xgb_learning_rate': 0.023610743702068664, 'subsample': 0.9266084972211004, 'colsample_bytree': 0.8023296504405306, 'hidden_layer_0': 66, 'hidden_layer_1': 73, 'hidden_layer_2': 93, 'nn_learning_rate': 0.0029722741304569152, 'batch_size': 256, 'num_epochs': 50}. Best is trial 18 with value: 0.7692307692307693.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:27:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN         0.697436    0.8209                    3.071482   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "XGBoost + NN                            0.000995               107.399315   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN         {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_estimators: 295\n",
      "max_depth: 9\n",
      "xgb_learning_rate: 0.022415372047585046\n",
      "subsample: 0.927244647425161\n",
      "colsample_bytree: 0.8065167263666334\n",
      "hidden_layer_0: 182\n",
      "hidden_layer_1: 83\n",
      "hidden_layer_2: 137\n",
      "nn_learning_rate: 0.0020732215195219347\n",
      "batch_size: 256\n",
      "num_epochs: 71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for XGBoost\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train XGBoost model\n",
    "    xgb_model = XGBClassifier(**xgb_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using XGBoost\n",
    "    X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "    X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = []\n",
    "    for i in range(3):  # Allow up to 3 hidden layers\n",
    "        if trial.suggest_categorical(f'use_hidden_layer_{i}', [True, False]):\n",
    "            hidden_layers.append(trial.suggest_int(f'hidden_layer_{i}', 32, 256))\n",
    "    \n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final XGBoost model with the best hyperparameters\n",
    "xgb_best_params = {\n",
    "    'n_estimators': best_params['n_estimators'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'learning_rate': best_params['xgb_learning_rate'],\n",
    "    'subsample': best_params['subsample'],\n",
    "    'colsample_bytree': best_params['colsample_bytree']\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using XGBoost\n",
    "X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "\n",
    "# Dynamically determine the number of hidden layers\n",
    "for i in range(3):  # Assuming max 3 hidden layers\n",
    "    if f'use_hidden_layer_{i}' in best_params and best_params[f'use_hidden_layer_{i}']:\n",
    "        nn_best_params['hidden_layers'].append(best_params[f'hidden_layer_{i}'])\n",
    "\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Assuming 'result' DataFrame is defined elsewhere\n",
    "result.loc['XGBoost + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['XGBoost + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:04,788] A new study created in memory with name: no-name-9aafeffd-ea8f-43ea-8934-dd4af9c60c7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:27:11,915] Trial 0 finished with value: 0.6820512820512821 and parameters: {'n_estimators': 171, 'max_depth': 6, 'lgb_learning_rate': 0.0007303950843652628, 'num_leaves': 91, 'subsample': 0.5890687346881405, 'colsample_bytree': 0.8765431388466591, 'hidden_layer_0': 149, 'hidden_layer_1': 251, 'hidden_layer_2': 174, 'nn_learning_rate': 0.07048332057737904, 'batch_size': 64, 'num_epochs': 80}. Best is trial 0 with value: 0.6820512820512821.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:17,370] Trial 1 finished with value: 0.676923076923077 and parameters: {'n_estimators': 155, 'max_depth': 5, 'lgb_learning_rate': 0.0008708741477612304, 'num_leaves': 72, 'subsample': 0.668279713210599, 'colsample_bytree': 0.9867303864548873, 'hidden_layer_0': 175, 'hidden_layer_1': 211, 'hidden_layer_2': 108, 'nn_learning_rate': 0.002780997393517395, 'batch_size': 128, 'num_epochs': 89}. Best is trial 0 with value: 0.6820512820512821.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:25,314] Trial 2 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 99, 'max_depth': 9, 'lgb_learning_rate': 0.00018809740820026834, 'num_leaves': 83, 'subsample': 0.8558071191416066, 'colsample_bytree': 0.6885621058950526, 'hidden_layer_0': 101, 'hidden_layer_1': 249, 'hidden_layer_2': 44, 'nn_learning_rate': 0.04492118955836811, 'batch_size': 32, 'num_epochs': 59}. Best is trial 0 with value: 0.6820512820512821.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:27,128] Trial 3 finished with value: 0.7230769230769231 and parameters: {'n_estimators': 190, 'max_depth': 6, 'lgb_learning_rate': 0.003999048582494491, 'num_leaves': 92, 'subsample': 0.9126899052552253, 'colsample_bytree': 0.6609367499354242, 'hidden_layer_0': 93, 'hidden_layer_1': 153, 'hidden_layer_2': 251, 'nn_learning_rate': 0.01535996206204883, 'batch_size': 256, 'num_epochs': 45}. Best is trial 3 with value: 0.7230769230769231.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:31,697] Trial 4 finished with value: 0.7333333333333333 and parameters: {'n_estimators': 227, 'max_depth': 6, 'lgb_learning_rate': 0.0004287477771335191, 'num_leaves': 34, 'subsample': 0.9079524355272293, 'colsample_bytree': 0.515456175452567, 'hidden_layer_0': 85, 'hidden_layer_1': 35, 'hidden_layer_2': 255, 'nn_learning_rate': 0.035820036633057065, 'batch_size': 128, 'num_epochs': 100}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:27:42,803] Trial 5 finished with value: 0.7076923076923077 and parameters: {'n_estimators': 192, 'max_depth': 9, 'lgb_learning_rate': 0.00894143720380435, 'num_leaves': 40, 'subsample': 0.9554407256290511, 'colsample_bytree': 0.9239478559256313, 'hidden_layer_0': 248, 'hidden_layer_1': 83, 'hidden_layer_2': 76, 'nn_learning_rate': 0.08525988993132252, 'batch_size': 64, 'num_epochs': 57}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:27:59,522] Trial 6 finished with value: 0.7282051282051282 and parameters: {'n_estimators': 274, 'max_depth': 7, 'lgb_learning_rate': 0.006831094283558482, 'num_leaves': 73, 'subsample': 0.8214206558165986, 'colsample_bytree': 0.700787644833244, 'hidden_layer_0': 145, 'hidden_layer_1': 67, 'hidden_layer_2': 145, 'nn_learning_rate': 0.00873366652098684, 'batch_size': 64, 'num_epochs': 63}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:00,662] Trial 7 finished with value: 0.7025641025641025 and parameters: {'n_estimators': 215, 'max_depth': 9, 'lgb_learning_rate': 0.003963805837895997, 'num_leaves': 81, 'subsample': 0.7891176705458761, 'colsample_bytree': 0.8523388048006024, 'hidden_layer_0': 234, 'hidden_layer_1': 189, 'hidden_layer_2': 49, 'nn_learning_rate': 0.0007543437947927383, 'batch_size': 256, 'num_epochs': 25}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:05,124] Trial 8 finished with value: 0.717948717948718 and parameters: {'n_estimators': 161, 'max_depth': 6, 'lgb_learning_rate': 0.0006437790679925271, 'num_leaves': 80, 'subsample': 0.828882209245673, 'colsample_bytree': 0.7228628139116648, 'hidden_layer_0': 153, 'hidden_layer_1': 49, 'hidden_layer_2': 171, 'nn_learning_rate': 0.00023123293141331843, 'batch_size': 128, 'num_epochs': 93}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:07,891] Trial 9 finished with value: 0.6615384615384615 and parameters: {'n_estimators': 280, 'max_depth': 10, 'lgb_learning_rate': 0.0001985706301163377, 'num_leaves': 25, 'subsample': 0.7055449540123271, 'colsample_bytree': 0.5021867428615998, 'hidden_layer_0': 210, 'hidden_layer_1': 140, 'hidden_layer_2': 187, 'nn_learning_rate': 0.00011802097547372629, 'batch_size': 128, 'num_epochs': 50}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:28:09,856] Trial 10 finished with value: 0.7333333333333333 and parameters: {'n_estimators': 239, 'max_depth': 3, 'lgb_learning_rate': 0.09719286093110663, 'num_leaves': 51, 'subsample': 0.9937278019888522, 'colsample_bytree': 0.5323509792326571, 'hidden_layer_0': 42, 'hidden_layer_1': 108, 'hidden_layer_2': 248, 'nn_learning_rate': 0.003201004006439562, 'batch_size': 32, 'num_epochs': 15}. Best is trial 4 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:11,384] Trial 11 finished with value: 0.7435897435897436 and parameters: {'n_estimators': 241, 'max_depth': 3, 'lgb_learning_rate': 0.09485852647530857, 'num_leaves': 48, 'subsample': 0.996469927459809, 'colsample_bytree': 0.5134722418584569, 'hidden_layer_0': 39, 'hidden_layer_1': 107, 'hidden_layer_2': 253, 'nn_learning_rate': 0.0018083302232842315, 'batch_size': 32, 'num_epochs': 10}. Best is trial 11 with value: 0.7435897435897436.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:14,537] Trial 12 finished with value: 0.7487179487179487 and parameters: {'n_estimators': 245, 'max_depth': 3, 'lgb_learning_rate': 0.07112849300838935, 'num_leaves': 34, 'subsample': 0.8964531405074034, 'colsample_bytree': 0.5985622269567084, 'hidden_layer_0': 35, 'hidden_layer_1': 32, 'hidden_layer_2': 222, 'nn_learning_rate': 0.0011237283168162103, 'batch_size': 32, 'num_epochs': 31}. Best is trial 12 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:18,533] Trial 13 finished with value: 0.7589743589743589 and parameters: {'n_estimators': 300, 'max_depth': 3, 'lgb_learning_rate': 0.09233454666079594, 'num_leaves': 55, 'subsample': 0.9889088645886437, 'colsample_bytree': 0.6045886889998339, 'hidden_layer_0': 33, 'hidden_layer_1': 106, 'hidden_layer_2': 204, 'nn_learning_rate': 0.0010399074870392705, 'batch_size': 32, 'num_epochs': 30}. Best is trial 13 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:28:22,352] Trial 14 finished with value: 0.7487179487179487 and parameters: {'n_estimators': 300, 'max_depth': 4, 'lgb_learning_rate': 0.02325585618882458, 'num_leaves': 60, 'subsample': 0.9041381374435047, 'colsample_bytree': 0.6034063335610549, 'hidden_layer_0': 66, 'hidden_layer_1': 95, 'hidden_layer_2': 214, 'nn_learning_rate': 0.0006531631915918229, 'batch_size': 32, 'num_epochs': 32}. Best is trial 13 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:27,531] Trial 15 finished with value: 0.7743589743589744 and parameters: {'n_estimators': 269, 'max_depth': 4, 'lgb_learning_rate': 0.03063915257442697, 'num_leaves': 23, 'subsample': 0.507096330591258, 'colsample_bytree': 0.6170710476981663, 'hidden_layer_0': 116, 'hidden_layer_1': 146, 'hidden_layer_2': 213, 'nn_learning_rate': 0.0007141130417522767, 'batch_size': 32, 'num_epochs': 37}. Best is trial 15 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:32,887] Trial 16 finished with value: 0.7487179487179487 and parameters: {'n_estimators': 120, 'max_depth': 4, 'lgb_learning_rate': 0.033109392180075456, 'num_leaves': 22, 'subsample': 0.5041831774613531, 'colsample_bytree': 0.7823146315500309, 'hidden_layer_0': 114, 'hidden_layer_1': 141, 'hidden_layer_2': 206, 'nn_learning_rate': 0.0003797243442536347, 'batch_size': 32, 'num_epochs': 41}. Best is trial 15 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:35,773] Trial 17 finished with value: 0.7230769230769231 and parameters: {'n_estimators': 51, 'max_depth': 4, 'lgb_learning_rate': 0.02461576887855604, 'num_leaves': 61, 'subsample': 0.6155082398448791, 'colsample_bytree': 0.6103809217280667, 'hidden_layer_0': 187, 'hidden_layer_1': 173, 'hidden_layer_2': 128, 'nn_learning_rate': 0.005676138744619477, 'batch_size': 32, 'num_epochs': 21}. Best is trial 15 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:40,552] Trial 18 finished with value: 0.7538461538461538 and parameters: {'n_estimators': 272, 'max_depth': 5, 'lgb_learning_rate': 0.04308524863136817, 'num_leaves': 50, 'subsample': 0.5239516435239182, 'colsample_bytree': 0.7838011324150067, 'hidden_layer_0': 119, 'hidden_layer_1': 119, 'hidden_layer_2': 154, 'nn_learning_rate': 0.0002648370320288155, 'batch_size': 32, 'num_epochs': 39}. Best is trial 15 with value: 0.7743589743589744.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3458157511.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:43,605] Trial 19 finished with value: 0.7435897435897436 and parameters: {'n_estimators': 293, 'max_depth': 5, 'lgb_learning_rate': 0.012423604624350425, 'num_leaves': 100, 'subsample': 0.7010085370253263, 'colsample_bytree': 0.646119129265326, 'hidden_layer_0': 69, 'hidden_layer_1': 171, 'hidden_layer_2': 197, 'nn_learning_rate': 0.0015715305325937172, 'batch_size': 256, 'num_epochs': 74}. Best is trial 15 with value: 0.7743589743589744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 362, number of negative: 414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 776, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN         0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN        0.753846  0.834954                    5.210468   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "XGBoost + NN                            0.000995               107.399315   \n",
      "LightGBM + NN                           0.000999               104.179843   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN         {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_estimators: 269\n",
      "max_depth: 4\n",
      "lgb_learning_rate: 0.03063915257442697\n",
      "num_leaves: 23\n",
      "subsample: 0.507096330591258\n",
      "colsample_bytree: 0.6170710476981663\n",
      "hidden_layer_0: 116\n",
      "hidden_layer_1: 146\n",
      "hidden_layer_2: 213\n",
      "nn_learning_rate: 0.0007141130417522767\n",
      "batch_size: 32\n",
      "num_epochs: 37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for LightGBM\n",
    "    lgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model\n",
    "    lgb_model = LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using LightGBM\n",
    "    X_train_transformed = lgb_model.predict_proba(X_train_scaled)\n",
    "    X_test_transformed = lgb_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final LightGBM model with the best hyperparameters\n",
    "lgb_best_params = {\n",
    "    'n_estimators': best_params['n_estimators'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'learning_rate': best_params['lgb_learning_rate'],\n",
    "    'num_leaves': best_params['num_leaves'],\n",
    "    'subsample': best_params['subsample'],\n",
    "    'colsample_bytree': best_params['colsample_bytree']\n",
    "}\n",
    "lgb_model = LGBMClassifier(**lgb_best_params)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using LightGBM\n",
    "X_train_transformed = lgb_model.predict_proba(X_train_scaled)\n",
    "X_test_transformed = lgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['LightGBM + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:28:49,038] A new study created in memory with name: no-name-172a0f46-1890-408d-a37b-c702268bd013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:28:51,519] Trial 0 finished with value: 0.5333333333333333 and parameters: {'iterations': 248, 'depth': 6, 'catboost_learning_rate': 0.0001450537062924843, 'l2_leaf_reg': 0.021428272894819977, 'border_count': 161, 'hidden_layer_0': 235, 'hidden_layer_1': 253, 'hidden_layer_2': 34, 'nn_learning_rate': 0.06195799767824983, 'batch_size': 256, 'num_epochs': 45}. Best is trial 0 with value: 0.5333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:28:58,387] Trial 1 finished with value: 0.7384615384615385 and parameters: {'iterations': 300, 'depth': 6, 'catboost_learning_rate': 0.005586729465603519, 'l2_leaf_reg': 0.029012357007480984, 'border_count': 72, 'hidden_layer_0': 249, 'hidden_layer_1': 190, 'hidden_layer_2': 112, 'nn_learning_rate': 0.0002926192563198693, 'batch_size': 64, 'num_epochs': 71}. Best is trial 1 with value: 0.7384615384615385.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:03,010] Trial 2 finished with value: 0.6974358974358974 and parameters: {'iterations': 127, 'depth': 3, 'catboost_learning_rate': 0.00455866940294172, 'l2_leaf_reg': 0.014319463350269714, 'border_count': 251, 'hidden_layer_0': 194, 'hidden_layer_1': 68, 'hidden_layer_2': 33, 'nn_learning_rate': 0.008237071446826186, 'batch_size': 64, 'num_epochs': 75}. Best is trial 1 with value: 0.7384615384615385.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:07,236] Trial 3 finished with value: 0.7589743589743589 and parameters: {'iterations': 251, 'depth': 5, 'catboost_learning_rate': 0.05887152781943333, 'l2_leaf_reg': 0.006455402835523612, 'border_count': 44, 'hidden_layer_0': 121, 'hidden_layer_1': 80, 'hidden_layer_2': 212, 'nn_learning_rate': 0.0071975803194964175, 'batch_size': 128, 'num_epochs': 79}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:11,961] Trial 4 finished with value: 0.5333333333333333 and parameters: {'iterations': 50, 'depth': 6, 'catboost_learning_rate': 0.0013105152403810676, 'l2_leaf_reg': 0.0024125424038355056, 'border_count': 160, 'hidden_layer_0': 90, 'hidden_layer_1': 127, 'hidden_layer_2': 231, 'nn_learning_rate': 0.06470558150590999, 'batch_size': 128, 'num_epochs': 94}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:14,457] Trial 5 finished with value: 0.7435897435897436 and parameters: {'iterations': 157, 'depth': 3, 'catboost_learning_rate': 0.05746498325381256, 'l2_leaf_reg': 0.06642321179763153, 'border_count': 255, 'hidden_layer_0': 49, 'hidden_layer_1': 190, 'hidden_layer_2': 246, 'nn_learning_rate': 0.06983391061030624, 'batch_size': 128, 'num_epochs': 43}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:26,736] Trial 6 finished with value: 0.7589743589743589 and parameters: {'iterations': 296, 'depth': 8, 'catboost_learning_rate': 0.0026580453349773474, 'l2_leaf_reg': 0.008430895165564814, 'border_count': 249, 'hidden_layer_0': 150, 'hidden_layer_1': 150, 'hidden_layer_2': 181, 'nn_learning_rate': 0.010139067742818239, 'batch_size': 32, 'num_epochs': 71}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:32,700] Trial 7 finished with value: 0.5333333333333333 and parameters: {'iterations': 84, 'depth': 10, 'catboost_learning_rate': 0.00023038309457956718, 'l2_leaf_reg': 0.0002854441462326834, 'border_count': 134, 'hidden_layer_0': 105, 'hidden_layer_1': 32, 'hidden_layer_2': 229, 'nn_learning_rate': 0.0453244038580322, 'batch_size': 64, 'num_epochs': 77}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:34,853] Trial 8 finished with value: 0.5333333333333333 and parameters: {'iterations': 170, 'depth': 6, 'catboost_learning_rate': 0.0004545139647531744, 'l2_leaf_reg': 0.0002665442021249304, 'border_count': 95, 'hidden_layer_0': 178, 'hidden_layer_1': 231, 'hidden_layer_2': 244, 'nn_learning_rate': 0.015489995156849611, 'batch_size': 64, 'num_epochs': 17}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:40,490] Trial 9 finished with value: 0.7282051282051282 and parameters: {'iterations': 160, 'depth': 10, 'catboost_learning_rate': 0.002391148103264053, 'l2_leaf_reg': 0.000748727294944362, 'border_count': 215, 'hidden_layer_0': 38, 'hidden_layer_1': 108, 'hidden_layer_2': 209, 'nn_learning_rate': 0.04371519759686334, 'batch_size': 256, 'num_epochs': 69}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:45,051] Trial 10 finished with value: 0.7589743589743589 and parameters: {'iterations': 237, 'depth': 4, 'catboost_learning_rate': 0.08629162379876985, 'l2_leaf_reg': 0.0027925707600541033, 'border_count': 39, 'hidden_layer_0': 109, 'hidden_layer_1': 82, 'hidden_layer_2': 136, 'nn_learning_rate': 0.0007377461010886272, 'batch_size': 128, 'num_epochs': 93}. Best is trial 3 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:29:54,952] Trial 11 finished with value: 0.764102564102564 and parameters: {'iterations': 296, 'depth': 8, 'catboost_learning_rate': 0.01781606362497445, 'l2_leaf_reg': 0.00748077028323346, 'border_count': 194, 'hidden_layer_0': 152, 'hidden_layer_1': 164, 'hidden_layer_2': 181, 'nn_learning_rate': 0.0017915492715922827, 'batch_size': 32, 'num_epochs': 56}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:04,409] Trial 12 finished with value: 0.7589743589743589 and parameters: {'iterations': 239, 'depth': 8, 'catboost_learning_rate': 0.01699308173142604, 'l2_leaf_reg': 0.005356495494608722, 'border_count': 198, 'hidden_layer_0': 145, 'hidden_layer_1': 166, 'hidden_layer_2': 173, 'nn_learning_rate': 0.0016431480914143785, 'batch_size': 32, 'num_epochs': 55}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:09,355] Trial 13 finished with value: 0.7538461538461538 and parameters: {'iterations': 262, 'depth': 8, 'catboost_learning_rate': 0.022859178746291488, 'l2_leaf_reg': 0.0013487008060263568, 'border_count': 123, 'hidden_layer_0': 187, 'hidden_layer_1': 100, 'hidden_layer_2': 177, 'nn_learning_rate': 0.0029568825968866387, 'batch_size': 32, 'num_epochs': 26}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:15,590] Trial 14 finished with value: 0.7435897435897436 and parameters: {'iterations': 205, 'depth': 5, 'catboost_learning_rate': 0.01936444591060856, 'l2_leaf_reg': 0.08922782680400967, 'border_count': 35, 'hidden_layer_0': 83, 'hidden_layer_1': 51, 'hidden_layer_2': 102, 'nn_learning_rate': 0.0011112850946892938, 'batch_size': 32, 'num_epochs': 55}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:20,077] Trial 15 finished with value: 0.764102564102564 and parameters: {'iterations': 206, 'depth': 9, 'catboost_learning_rate': 0.03806957892288325, 'l2_leaf_reg': 0.008001513421910467, 'border_count': 198, 'hidden_layer_0': 137, 'hidden_layer_1': 124, 'hidden_layer_2': 204, 'nn_learning_rate': 0.00016656598850120098, 'batch_size': 128, 'num_epochs': 36}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:24,143] Trial 16 finished with value: 0.7384615384615385 and parameters: {'iterations': 201, 'depth': 9, 'catboost_learning_rate': 0.01129755665581148, 'l2_leaf_reg': 0.043942713316157504, 'border_count': 197, 'hidden_layer_0': 162, 'hidden_layer_1': 184, 'hidden_layer_2': 150, 'nn_learning_rate': 0.00011091424405554205, 'batch_size': 128, 'num_epochs': 33}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:31,940] Trial 17 finished with value: 0.7487179487179487 and parameters: {'iterations': 208, 'depth': 9, 'catboost_learning_rate': 0.03615785172303551, 'l2_leaf_reg': 0.00010259857822517536, 'border_count': 175, 'hidden_layer_0': 221, 'hidden_layer_1': 145, 'hidden_layer_2': 201, 'nn_learning_rate': 0.00034526371320222065, 'batch_size': 32, 'num_epochs': 40}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:32,837] Trial 18 finished with value: 0.717948717948718 and parameters: {'iterations': 133, 'depth': 7, 'catboost_learning_rate': 0.008371865091126415, 'l2_leaf_reg': 0.01307392052040422, 'border_count': 221, 'hidden_layer_0': 123, 'hidden_layer_1': 222, 'hidden_layer_2': 76, 'nn_learning_rate': 0.00010643908759551097, 'batch_size': 256, 'num_epochs': 12}. Best is trial 11 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1954938432.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:36,599] Trial 19 finished with value: 0.764102564102564 and parameters: {'iterations': 277, 'depth': 9, 'catboost_learning_rate': 0.03205025589169537, 'l2_leaf_reg': 0.0009697276834059151, 'border_count': 186, 'hidden_layer_0': 65, 'hidden_layer_1': 125, 'hidden_layer_2': 148, 'nn_learning_rate': 0.00036925552995878603, 'batch_size': 128, 'num_epochs': 25}. Best is trial 11 with value: 0.764102564102564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN         0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN        0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN        0.764103   0.84309                    8.960259   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "XGBoost + NN                            0.000995               107.399315   \n",
      "LightGBM + NN                           0.000999               104.179843   \n",
      "CatBoost + NN                            0.00099               119.045705   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN         {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "iterations: 296\n",
      "depth: 8\n",
      "catboost_learning_rate: 0.01781606362497445\n",
      "l2_leaf_reg: 0.00748077028323346\n",
      "border_count: 194\n",
      "hidden_layer_0: 152\n",
      "hidden_layer_1: 164\n",
      "hidden_layer_2: 181\n",
      "nn_learning_rate: 0.0017915492715922827\n",
      "batch_size: 32\n",
      "num_epochs: 56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for CatBoost\n",
    "    catboost_params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255)\n",
    "    }\n",
    "\n",
    "    # Train CatBoost model\n",
    "    catboost_model = CatBoostClassifier(**catboost_params, verbose=0)\n",
    "    catboost_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using CatBoost\n",
    "    X_train_transformed = catboost_model.predict_proba(X_train_scaled)\n",
    "    X_test_transformed = catboost_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final CatBoost model with the best hyperparameters\n",
    "catboost_best_params = {\n",
    "    'iterations': best_params['iterations'],\n",
    "    'depth': best_params['depth'],\n",
    "    'learning_rate': best_params['catboost_learning_rate'],\n",
    "    'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "    'border_count': best_params['border_count']\n",
    "}\n",
    "catboost_model = CatBoostClassifier(**catboost_best_params, verbose=0)\n",
    "catboost_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using CatBoost\n",
    "X_train_transformed = catboost_model.predict_proba(X_train_scaled)\n",
    "X_test_transformed = catboost_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['CatBoost + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:30:48,199] A new study created in memory with name: no-name-9eccdf7f-7363-40f3-98b7-3f01ae87034a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:56,223] Trial 0 finished with value: 0.7794871794871795 and parameters: {'num_heads': 8, 'embedding_dim': 56, 'num_layers': 3, 'hidden_layer_0': 138, 'hidden_layer_1': 155, 'hidden_layer_2': 68, 'nn_learning_rate': 0.0008061114172516176, 'batch_size': 64, 'num_epochs': 61}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:30:59,681] Trial 1 finished with value: 0.7128205128205128 and parameters: {'num_heads': 3, 'embedding_dim': 57, 'num_layers': 3, 'hidden_layer_0': 234, 'hidden_layer_1': 173, 'hidden_layer_2': 50, 'nn_learning_rate': 0.04208325164012932, 'batch_size': 128, 'num_epochs': 18}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:02,832] Trial 2 finished with value: 0.7743589743589744 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 3, 'hidden_layer_0': 39, 'hidden_layer_1': 38, 'hidden_layer_2': 84, 'nn_learning_rate': 0.042189627025105994, 'batch_size': 128, 'num_epochs': 18}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:06,684] Trial 3 finished with value: 0.717948717948718 and parameters: {'num_heads': 5, 'embedding_dim': 5, 'num_layers': 1, 'hidden_layer_0': 160, 'hidden_layer_1': 58, 'hidden_layer_2': 49, 'nn_learning_rate': 0.006976087081755031, 'batch_size': 256, 'num_epochs': 86}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:18,673] Trial 4 finished with value: 0.6871794871794872 and parameters: {'num_heads': 7, 'embedding_dim': 28, 'num_layers': 1, 'hidden_layer_0': 105, 'hidden_layer_1': 255, 'hidden_layer_2': 255, 'nn_learning_rate': 0.04696297014437602, 'batch_size': 64, 'num_epochs': 90}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:35,550] Trial 5 finished with value: 0.7589743589743589 and parameters: {'num_heads': 7, 'embedding_dim': 49, 'num_layers': 1, 'hidden_layer_0': 230, 'hidden_layer_1': 73, 'hidden_layer_2': 183, 'nn_learning_rate': 0.017226999098722055, 'batch_size': 32, 'num_epochs': 89}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:50,163] Trial 6 finished with value: 0.6358974358974359 and parameters: {'num_heads': 3, 'embedding_dim': 3, 'num_layers': 1, 'hidden_layer_0': 124, 'hidden_layer_1': 207, 'hidden_layer_2': 39, 'nn_learning_rate': 0.039159263980982614, 'batch_size': 32, 'num_epochs': 81}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:31:59,693] Trial 7 finished with value: 0.7487179487179487 and parameters: {'num_heads': 5, 'embedding_dim': 60, 'num_layers': 3, 'hidden_layer_0': 77, 'hidden_layer_1': 232, 'hidden_layer_2': 254, 'nn_learning_rate': 0.0013775132498363724, 'batch_size': 64, 'num_epochs': 68}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:03,048] Trial 8 finished with value: 0.7384615384615385 and parameters: {'num_heads': 5, 'embedding_dim': 30, 'num_layers': 1, 'hidden_layer_0': 96, 'hidden_layer_1': 253, 'hidden_layer_2': 100, 'nn_learning_rate': 0.00021920058799951547, 'batch_size': 128, 'num_epochs': 34}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:07,673] Trial 9 finished with value: 0.7538461538461538 and parameters: {'num_heads': 2, 'embedding_dim': 26, 'num_layers': 2, 'hidden_layer_0': 121, 'hidden_layer_1': 99, 'hidden_layer_2': 251, 'nn_learning_rate': 0.0006528918950330614, 'batch_size': 256, 'num_epochs': 63}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:14,362] Trial 10 finished with value: 0.7487179487179487 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 2, 'hidden_layer_0': 176, 'hidden_layer_1': 136, 'hidden_layer_2': 147, 'nn_learning_rate': 0.00012127368988887861, 'batch_size': 64, 'num_epochs': 45}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:17,474] Trial 11 finished with value: 0.7538461538461538 and parameters: {'num_heads': 8, 'embedding_dim': 48, 'num_layers': 3, 'hidden_layer_0': 36, 'hidden_layer_1': 132, 'hidden_layer_2': 97, 'nn_learning_rate': 0.0037295231100399387, 'batch_size': 128, 'num_epochs': 16}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:22,951] Trial 12 finished with value: 0.7692307692307693 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 3, 'hidden_layer_0': 40, 'hidden_layer_1': 40, 'hidden_layer_2': 102, 'nn_learning_rate': 0.0008223219877385224, 'batch_size': 64, 'num_epochs': 44}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:26,644] Trial 13 finished with value: 0.7282051282051282 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 192, 'hidden_layer_1': 175, 'hidden_layer_2': 143, 'nn_learning_rate': 0.00789910387179344, 'batch_size': 128, 'num_epochs': 30}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:31,347] Trial 14 finished with value: 0.7435897435897436 and parameters: {'num_heads': 8, 'embedding_dim': 16, 'num_layers': 3, 'hidden_layer_0': 67, 'hidden_layer_1': 97, 'hidden_layer_2': 74, 'nn_learning_rate': 0.0003743711029425879, 'batch_size': 128, 'num_epochs': 55}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:40,981] Trial 15 finished with value: 0.7692307692307693 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 3, 'hidden_layer_0': 200, 'hidden_layer_1': 163, 'hidden_layer_2': 133, 'nn_learning_rate': 0.0018578413713095203, 'batch_size': 64, 'num_epochs': 76}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:50,986] Trial 16 finished with value: 0.7282051282051282 and parameters: {'num_heads': 1, 'embedding_dim': 55, 'num_layers': 2, 'hidden_layer_0': 139, 'hidden_layer_1': 101, 'hidden_layer_2': 70, 'nn_learning_rate': 0.015069485348099813, 'batch_size': 32, 'num_epochs': 59}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:56,115] Trial 17 finished with value: 0.5743589743589743 and parameters: {'num_heads': 6, 'embedding_dim': 36, 'num_layers': 3, 'hidden_layer_0': 254, 'hidden_layer_1': 205, 'hidden_layer_2': 175, 'nn_learning_rate': 0.08231088697342877, 'batch_size': 256, 'num_epochs': 48}. Best is trial 0 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:32:59,893] Trial 18 finished with value: 0.7487179487179487 and parameters: {'num_heads': 4, 'embedding_dim': 52, 'num_layers': 2, 'hidden_layer_0': 72, 'hidden_layer_1': 117, 'hidden_layer_2': 75, 'nn_learning_rate': 0.002526387001072188, 'batch_size': 128, 'num_epochs': 29}. Best is trial 0 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\652687641.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:33:08,408] Trial 19 finished with value: 0.7435897435897436 and parameters: {'num_heads': 7, 'embedding_dim': 63, 'num_layers': 3, 'hidden_layer_0': 150, 'hidden_layer_1': 32, 'hidden_layer_2': 114, 'nn_learning_rate': 0.006182486518817999, 'batch_size': 64, 'num_epochs': 71}. Best is trial 0 with value: 0.7794871794871795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN         0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN        0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN        0.764103   0.84309                    8.960259   \n",
      "AutoInt              0.738462  0.831255                    9.652604   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "XGBoost + NN                            0.000995               107.399315   \n",
      "LightGBM + NN                           0.000999               104.179843   \n",
      "CatBoost + NN                            0.00099               119.045705   \n",
      "AutoInt                                 0.001995               154.137457   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN         {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt              {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_heads: 8\n",
      "embedding_dim: 56\n",
      "num_layers: 3\n",
      "hidden_layer_0: 138\n",
      "hidden_layer_1: 155\n",
      "hidden_layer_2: 68\n",
      "nn_learning_rate: 0.0008061114172516176\n",
      "batch_size: 64\n",
      "num_epochs: 61\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class AutoInt(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "        super(AutoInt, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x, _ = attn_layer(x, x, x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for AutoInt\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    \n",
    "    # Train AutoInt model\n",
    "    autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "        autoint_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = autoint_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Extract features using AutoInt\n",
    "    autoint_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final AutoInt model with the best hyperparameters\n",
    "embedding_dim = best_params['embedding_dim']\n",
    "num_heads = best_params['num_heads']\n",
    "num_layers = best_params['num_layers']\n",
    "autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "    autoint_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoint_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract features using AutoInt\n",
    "autoint_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "    X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['AutoInt'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:33:22,387] A new study created in memory with name: no-name-da008a59-395d-4d19-8668-ae5a3353f64b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:33:34,711] Trial 0 finished with value: 0.764102564102564 and parameters: {'num_heads': 4, 'embedding_dim': 40, 'num_layers': 1, 'hidden_layer_0': 194, 'hidden_layer_1': 153, 'hidden_layer_2': 73, 'nn_learning_rate': 0.00039393719638241317, 'batch_size': 64, 'num_epochs': 98}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:33:42,431] Trial 1 finished with value: 0.7487179487179487 and parameters: {'num_heads': 4, 'embedding_dim': 40, 'num_layers': 2, 'hidden_layer_0': 38, 'hidden_layer_1': 233, 'hidden_layer_2': 62, 'nn_learning_rate': 0.010375821295189151, 'batch_size': 128, 'num_epochs': 12}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:33:47,459] Trial 2 finished with value: 0.7282051282051282 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 1, 'hidden_layer_0': 151, 'hidden_layer_1': 44, 'hidden_layer_2': 44, 'nn_learning_rate': 0.00015049363368521655, 'batch_size': 128, 'num_epochs': 33}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:33:50,638] Trial 3 finished with value: 0.7333333333333333 and parameters: {'num_heads': 1, 'embedding_dim': 12, 'num_layers': 1, 'hidden_layer_0': 96, 'hidden_layer_1': 182, 'hidden_layer_2': 69, 'nn_learning_rate': 0.0059579537127015785, 'batch_size': 64, 'num_epochs': 15}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:03,411] Trial 4 finished with value: 0.676923076923077 and parameters: {'num_heads': 2, 'embedding_dim': 4, 'num_layers': 2, 'hidden_layer_0': 235, 'hidden_layer_1': 233, 'hidden_layer_2': 138, 'nn_learning_rate': 0.04152183171490343, 'batch_size': 64, 'num_epochs': 89}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:08,361] Trial 5 finished with value: 0.7333333333333333 and parameters: {'num_heads': 3, 'embedding_dim': 60, 'num_layers': 1, 'hidden_layer_0': 140, 'hidden_layer_1': 94, 'hidden_layer_2': 144, 'nn_learning_rate': 0.00045624610041025335, 'batch_size': 64, 'num_epochs': 17}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:11,707] Trial 6 finished with value: 0.7025641025641025 and parameters: {'num_heads': 4, 'embedding_dim': 16, 'num_layers': 1, 'hidden_layer_0': 153, 'hidden_layer_1': 169, 'hidden_layer_2': 226, 'nn_learning_rate': 0.007086665432673722, 'batch_size': 256, 'num_epochs': 13}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:17,635] Trial 7 finished with value: 0.7025641025641025 and parameters: {'num_heads': 4, 'embedding_dim': 8, 'num_layers': 3, 'hidden_layer_0': 115, 'hidden_layer_1': 222, 'hidden_layer_2': 97, 'nn_learning_rate': 0.00011339692100600956, 'batch_size': 256, 'num_epochs': 12}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:27,872] Trial 8 finished with value: 0.717948717948718 and parameters: {'num_heads': 4, 'embedding_dim': 60, 'num_layers': 2, 'hidden_layer_0': 246, 'hidden_layer_1': 64, 'hidden_layer_2': 171, 'nn_learning_rate': 0.009598430874839144, 'batch_size': 64, 'num_epochs': 59}. Best is trial 0 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:38,173] Trial 9 finished with value: 0.7743589743589744 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 122, 'hidden_layer_1': 142, 'hidden_layer_2': 158, 'nn_learning_rate': 0.0010053232445414262, 'batch_size': 32, 'num_epochs': 42}. Best is trial 9 with value: 0.7743589743589744.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:34:50,615] Trial 10 finished with value: 0.7589743589743589 and parameters: {'num_heads': 7, 'embedding_dim': 28, 'num_layers': 3, 'hidden_layer_0': 53, 'hidden_layer_1': 125, 'hidden_layer_2': 252, 'nn_learning_rate': 0.0014120212588226231, 'batch_size': 32, 'num_epochs': 53}. Best is trial 9 with value: 0.7743589743589744.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:35:09,219] Trial 11 finished with value: 0.7897435897435897 and parameters: {'num_heads': 6, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 199, 'hidden_layer_1': 132, 'hidden_layer_2': 199, 'nn_learning_rate': 0.0009896392229452192, 'batch_size': 32, 'num_epochs': 96}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:35:23,974] Trial 12 finished with value: 0.7846153846153846 and parameters: {'num_heads': 6, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 194, 'hidden_layer_1': 117, 'hidden_layer_2': 199, 'nn_learning_rate': 0.0015622389647064413, 'batch_size': 32, 'num_epochs': 72}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:35:43,239] Trial 13 finished with value: 0.7384615384615385 and parameters: {'num_heads': 6, 'embedding_dim': 54, 'num_layers': 3, 'hidden_layer_0': 194, 'hidden_layer_1': 107, 'hidden_layer_2': 200, 'nn_learning_rate': 0.002504127015374729, 'batch_size': 32, 'num_epochs': 77}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:35:58,223] Trial 14 finished with value: 0.7589743589743589 and parameters: {'num_heads': 8, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 200, 'hidden_layer_1': 91, 'hidden_layer_2': 197, 'nn_learning_rate': 0.0007324118411781066, 'batch_size': 32, 'num_epochs': 71}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:36:20,585] Trial 15 finished with value: 0.7589743589743589 and parameters: {'num_heads': 6, 'embedding_dim': 30, 'num_layers': 2, 'hidden_layer_0': 217, 'hidden_layer_1': 192, 'hidden_layer_2': 196, 'nn_learning_rate': 0.002471420370918606, 'batch_size': 32, 'num_epochs': 81}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:36:49,771] Trial 16 finished with value: 0.6051282051282051 and parameters: {'num_heads': 5, 'embedding_dim': 50, 'num_layers': 3, 'hidden_layer_0': 174, 'hidden_layer_1': 123, 'hidden_layer_2': 242, 'nn_learning_rate': 0.02585541272604184, 'batch_size': 32, 'num_epochs': 100}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:04,582] Trial 17 finished with value: 0.7692307692307693 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 2, 'hidden_layer_0': 256, 'hidden_layer_1': 70, 'hidden_layer_2': 218, 'nn_learning_rate': 0.0002917469688252716, 'batch_size': 32, 'num_epochs': 65}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:23,137] Trial 18 finished with value: 0.7487179487179487 and parameters: {'num_heads': 5, 'embedding_dim': 25, 'num_layers': 3, 'hidden_layer_0': 222, 'hidden_layer_1': 141, 'hidden_layer_2': 111, 'nn_learning_rate': 0.0015521028461857802, 'batch_size': 32, 'num_epochs': 87}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2501686716.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:34,041] Trial 19 finished with value: 0.7333333333333333 and parameters: {'num_heads': 7, 'embedding_dim': 49, 'num_layers': 2, 'hidden_layer_0': 173, 'hidden_layer_1': 203, 'hidden_layer_2': 177, 'nn_learning_rate': 0.0039039140121561173, 'batch_size': 128, 'num_epochs': 89}. Best is trial 11 with value: 0.7897435897435897.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression  0.748718  0.796644                    0.003991   \n",
      "KNN                  0.753846  0.840861                    0.001994   \n",
      "Decision Tree        0.728205  0.749789                    0.004987   \n",
      "Random Forest             0.8  0.864816                    0.727075   \n",
      "Gradient Boosting         0.8  0.861439                    0.513596   \n",
      "XGBoost              0.764103  0.853525                      0.1127   \n",
      "LightGBM             0.789744  0.866083                    0.057846   \n",
      "CatBoost             0.753846  0.843288                    0.245005   \n",
      "MLP                  0.764103  0.838863                    3.545068   \n",
      "DNN                  0.774359  0.836116                    2.187616   \n",
      "DCN                  0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep        0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN         0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN        0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN        0.764103   0.84309                    8.960259   \n",
      "AutoInt              0.738462  0.831255                    9.652604   \n",
      "FT-Transformer       0.789744  0.819738                   13.344369   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                          0.0                 4.049861   \n",
      "KNN                                     0.004881                 0.248299   \n",
      "Decision Tree                                0.0                  0.11569   \n",
      "Random Forest                           0.053861                 9.095487   \n",
      "Gradient Boosting                       0.001995                12.765274   \n",
      "XGBoost                                 0.002992                 1.586735   \n",
      "LightGBM                                0.002992                 6.770629   \n",
      "CatBoost                                0.000997                 10.83189   \n",
      "MLP                                     0.000997               234.750347   \n",
      "DNN                                     0.000997                94.017027   \n",
      "DCN                                     0.000998               127.458129   \n",
      "Wide_and_Deep                           0.000998                96.221561   \n",
      "XGBoost + NN                            0.000995               107.399315   \n",
      "LightGBM + NN                           0.000999               104.179843   \n",
      "CatBoost + NN                            0.00099               119.045705   \n",
      "AutoInt                                 0.001995               154.137457   \n",
      "FT-Transformer                          0.000996               270.296228   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost              {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM             {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost             {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                  {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN         {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt              {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer       {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_heads: 6\n",
      "embedding_dim: 48\n",
      "num_layers: 2\n",
      "hidden_layer_0: 199\n",
      "hidden_layer_1: 132\n",
      "hidden_layer_2: 199\n",
      "nn_learning_rate: 0.0009896392229452192\n",
      "batch_size: 32\n",
      "num_epochs: 96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "        super(FTTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for FT-Transformer\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    \n",
    "    # Train FT-Transformer model\n",
    "    ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "        ft_transformer_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ft_transformer_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Extract features using FT-Transformer\n",
    "    ft_transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final FT-Transformer model with the best hyperparameters\n",
    "embedding_dim = best_params['embedding_dim']\n",
    "num_heads = best_params['num_heads']\n",
    "num_layers = best_params['num_layers']\n",
    "ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "    ft_transformer_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ft_transformer_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract features using FT-Transformer\n",
    "ft_transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "    X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['FT-Transformer'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:37:52,750] A new study created in memory with name: no-name-ecd9127e-b104-49a2-86c9-6f016ad41ae4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:54,045] Trial 0 finished with value: 0.6974358974358974 and parameters: {'num_layers': 5, 'hidden_layer_0': 256, 'hidden_layer_1': 195, 'hidden_layer_2': 65, 'hidden_layer_3': 63, 'hidden_layer_4': 35, 'learning_rate': 0.027251445591806936, 'batch_size': 256, 'num_epochs': 30}. Best is trial 0 with value: 0.6974358974358974.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:58,271] Trial 1 finished with value: 0.7538461538461538 and parameters: {'num_layers': 2, 'hidden_layer_0': 130, 'hidden_layer_1': 203, 'learning_rate': 0.0010551988083198915, 'batch_size': 64, 'num_epochs': 58}. Best is trial 1 with value: 0.7538461538461538.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:37:59,592] Trial 2 finished with value: 0.7692307692307693 and parameters: {'num_layers': 2, 'hidden_layer_0': 245, 'hidden_layer_1': 164, 'learning_rate': 0.0010359563020433974, 'batch_size': 64, 'num_epochs': 21}. Best is trial 2 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:05,872] Trial 3 finished with value: 0.7897435897435897 and parameters: {'num_layers': 2, 'hidden_layer_0': 61, 'hidden_layer_1': 137, 'learning_rate': 0.0004841820541342028, 'batch_size': 32, 'num_epochs': 73}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:10,981] Trial 4 finished with value: 0.7692307692307693 and parameters: {'num_layers': 3, 'hidden_layer_0': 81, 'hidden_layer_1': 129, 'hidden_layer_2': 212, 'learning_rate': 0.004705233729436024, 'batch_size': 32, 'num_epochs': 41}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:11,493] Trial 5 finished with value: 0.7230769230769231 and parameters: {'num_layers': 4, 'hidden_layer_0': 127, 'hidden_layer_1': 109, 'hidden_layer_2': 113, 'hidden_layer_3': 145, 'learning_rate': 0.016336584211395694, 'batch_size': 128, 'num_epochs': 10}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:13,401] Trial 6 finished with value: 0.7589743589743589 and parameters: {'num_layers': 2, 'hidden_layer_0': 126, 'hidden_layer_1': 74, 'learning_rate': 0.007013701581576816, 'batch_size': 128, 'num_epochs': 53}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:16,195] Trial 7 finished with value: 0.7128205128205128 and parameters: {'num_layers': 5, 'hidden_layer_0': 90, 'hidden_layer_1': 221, 'hidden_layer_2': 43, 'hidden_layer_3': 37, 'hidden_layer_4': 90, 'learning_rate': 0.061611505676376535, 'batch_size': 64, 'num_epochs': 33}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:17,007] Trial 8 finished with value: 0.7282051282051282 and parameters: {'num_layers': 1, 'hidden_layer_0': 56, 'learning_rate': 0.0009763683961038683, 'batch_size': 128, 'num_epochs': 41}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:25,724] Trial 9 finished with value: 0.7384615384615385 and parameters: {'num_layers': 5, 'hidden_layer_0': 161, 'hidden_layer_1': 61, 'hidden_layer_2': 155, 'hidden_layer_3': 32, 'hidden_layer_4': 99, 'learning_rate': 0.0004884429037475137, 'batch_size': 32, 'num_epochs': 61}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:29,695] Trial 10 finished with value: 0.7128205128205128 and parameters: {'num_layers': 1, 'hidden_layer_0': 34, 'learning_rate': 0.00011077937875412675, 'batch_size': 32, 'num_epochs': 90}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:35,064] Trial 11 finished with value: 0.7794871794871795 and parameters: {'num_layers': 2, 'hidden_layer_0': 242, 'hidden_layer_1': 162, 'learning_rate': 0.00022394648569916078, 'batch_size': 64, 'num_epochs': 82}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:38,994] Trial 12 finished with value: 0.7384615384615385 and parameters: {'num_layers': 3, 'hidden_layer_0': 198, 'hidden_layer_1': 160, 'hidden_layer_2': 235, 'learning_rate': 0.00011590689361624383, 'batch_size': 256, 'num_epochs': 84}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:46,150] Trial 13 finished with value: 0.7794871794871795 and parameters: {'num_layers': 2, 'hidden_layer_0': 198, 'hidden_layer_1': 111, 'learning_rate': 0.0002801608891560185, 'batch_size': 32, 'num_epochs': 76}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:38:49,615] Trial 14 finished with value: 0.7487179487179487 and parameters: {'num_layers': 1, 'hidden_layer_0': 215, 'learning_rate': 0.00027511501111797675, 'batch_size': 64, 'num_epochs': 72}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:04,515] Trial 15 finished with value: 0.7692307692307693 and parameters: {'num_layers': 3, 'hidden_layer_0': 165, 'hidden_layer_1': 168, 'hidden_layer_2': 167, 'learning_rate': 0.0017578676810928468, 'batch_size': 32, 'num_epochs': 98}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:11,713] Trial 16 finished with value: 0.7487179487179487 and parameters: {'num_layers': 4, 'hidden_layer_0': 84, 'hidden_layer_1': 90, 'hidden_layer_2': 98, 'hidden_layer_3': 253, 'learning_rate': 0.0003254655410292667, 'batch_size': 64, 'num_epochs': 77}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:14,583] Trial 17 finished with value: 0.7589743589743589 and parameters: {'num_layers': 2, 'hidden_layer_0': 226, 'hidden_layer_1': 238, 'learning_rate': 0.002639014619712713, 'batch_size': 256, 'num_epochs': 68}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:24,063] Trial 18 finished with value: 0.7128205128205128 and parameters: {'num_layers': 4, 'hidden_layer_0': 104, 'hidden_layer_1': 138, 'hidden_layer_2': 188, 'hidden_layer_3': 252, 'learning_rate': 0.0005552613012323991, 'batch_size': 64, 'num_epochs': 89}. Best is trial 3 with value: 0.7897435897435897.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\3158987854.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:29,038] Trial 19 finished with value: 0.7384615384615385 and parameters: {'num_layers': 1, 'hidden_layer_0': 169, 'learning_rate': 0.00019909418632052292, 'batch_size': 32, 'num_epochs': 100}. Best is trial 3 with value: 0.7897435897435897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_layers: 2\n",
      "hidden_layer_0: 61\n",
      "hidden_layer_1: 137\n",
      "learning_rate: 0.0004841820541342028\n",
      "batch_size: 32\n",
      "num_epochs: 73\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(num_layers)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, \n",
    "                           [best_params[f'hidden_layer_{i}'] for i in range(best_params['num_layers'])], \n",
    "                           output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['Neural Architecture Search'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:39:35,442] A new study created in memory with name: no-name-41b9623f-8eba-40ce-8cca-f15a80fda323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:39:56,396] Trial 0 finished with value: 0.7487179487179487 and parameters: {'num_layers': 4, 'num_trees': 8, 'tree_dim': 23, 'learning_rate': 0.051167047104067796, 'batch_size': 128, 'num_epochs': 90}. Best is trial 0 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:40:28,571] Trial 1 finished with value: 0.7589743589743589 and parameters: {'num_layers': 4, 'num_trees': 8, 'tree_dim': 42, 'learning_rate': 0.00017034100178255545, 'batch_size': 32, 'num_epochs': 56}. Best is trial 1 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:40:58,759] Trial 2 finished with value: 0.7538461538461538 and parameters: {'num_layers': 4, 'num_trees': 6, 'tree_dim': 39, 'learning_rate': 0.0019947496524994153, 'batch_size': 32, 'num_epochs': 58}. Best is trial 1 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:41:10,264] Trial 3 finished with value: 0.717948717948718 and parameters: {'num_layers': 4, 'num_trees': 8, 'tree_dim': 14, 'learning_rate': 0.0002980567359901975, 'batch_size': 128, 'num_epochs': 43}. Best is trial 1 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:41:32,950] Trial 4 finished with value: 0.7435897435897436 and parameters: {'num_layers': 3, 'num_trees': 10, 'tree_dim': 61, 'learning_rate': 0.0102954117102854, 'batch_size': 256, 'num_epochs': 94}. Best is trial 1 with value: 0.7589743589743589.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:41:45,563] Trial 5 finished with value: 0.7846153846153846 and parameters: {'num_layers': 1, 'num_trees': 8, 'tree_dim': 42, 'learning_rate': 0.0008319240313573611, 'batch_size': 32, 'num_epochs': 65}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:10,430] Trial 6 finished with value: 0.7538461538461538 and parameters: {'num_layers': 2, 'num_trees': 10, 'tree_dim': 49, 'learning_rate': 0.057717987997203665, 'batch_size': 32, 'num_epochs': 61}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:18,646] Trial 7 finished with value: 0.7333333333333333 and parameters: {'num_layers': 1, 'num_trees': 3, 'tree_dim': 56, 'learning_rate': 0.00012929616792811858, 'batch_size': 32, 'num_epochs': 80}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:21,545] Trial 8 finished with value: 0.6974358974358974 and parameters: {'num_layers': 3, 'num_trees': 3, 'tree_dim': 35, 'learning_rate': 0.0003288889342485084, 'batch_size': 256, 'num_epochs': 41}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:22,107] Trial 9 finished with value: 0.6512820512820513 and parameters: {'num_layers': 1, 'num_trees': 2, 'tree_dim': 13, 'learning_rate': 0.001471730087451588, 'batch_size': 128, 'num_epochs': 13}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:24,790] Trial 10 finished with value: 0.7743589743589744 and parameters: {'num_layers': 2, 'num_trees': 5, 'tree_dim': 26, 'learning_rate': 0.008058287304031799, 'batch_size': 64, 'num_epochs': 19}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:26,182] Trial 11 finished with value: 0.764102564102564 and parameters: {'num_layers': 2, 'num_trees': 5, 'tree_dim': 27, 'learning_rate': 0.008562141593295361, 'batch_size': 64, 'num_epochs': 11}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:31,037] Trial 12 finished with value: 0.7435897435897436 and parameters: {'num_layers': 2, 'num_trees': 6, 'tree_dim': 29, 'learning_rate': 0.0008695521701528763, 'batch_size': 64, 'num_epochs': 29}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:37,230] Trial 13 finished with value: 0.7384615384615385 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 45, 'learning_rate': 0.007192587055978946, 'batch_size': 64, 'num_epochs': 76}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:42:48,977] Trial 14 finished with value: 0.7538461538461538 and parameters: {'num_layers': 2, 'num_trees': 7, 'tree_dim': 19, 'learning_rate': 0.02144267320188767, 'batch_size': 64, 'num_epochs': 72}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:43:01,447] Trial 15 finished with value: 0.7794871794871795 and parameters: {'num_layers': 5, 'num_trees': 4, 'tree_dim': 32, 'learning_rate': 0.0034787873581959327, 'batch_size': 32, 'num_epochs': 31}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:43:05,878] Trial 16 finished with value: 0.7435897435897436 and parameters: {'num_layers': 3, 'num_trees': 1, 'tree_dim': 34, 'learning_rate': 0.0007164863489390134, 'batch_size': 32, 'num_epochs': 41}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:43:16,622] Trial 17 finished with value: 0.7384615384615385 and parameters: {'num_layers': 5, 'num_trees': 4, 'tree_dim': 50, 'learning_rate': 0.002895324564700662, 'batch_size': 32, 'num_epochs': 25}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:43:44,960] Trial 18 finished with value: 0.7589743589743589 and parameters: {'num_layers': 5, 'num_trees': 9, 'tree_dim': 32, 'learning_rate': 0.0007300647324146597, 'batch_size': 32, 'num_epochs': 33}. Best is trial 5 with value: 0.7846153846153846.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2066003851.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:44:18,897] Trial 19 finished with value: 0.7487179487179487 and parameters: {'num_layers': 5, 'num_trees': 7, 'tree_dim': 51, 'learning_rate': 0.0035760246958902236, 'batch_size': 32, 'num_epochs': 48}. Best is trial 5 with value: 0.7846153846153846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "NODE                        0.779487  0.847105                   12.640264   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "NODE                                           0.000998   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "NODE                                     296.128333   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "NODE                        {'num_layers': 1, 'num_trees': 8, 'tree_dim': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_layers: 1\n",
      "num_trees: 8\n",
      "tree_dim: 42\n",
      "learning_rate: 0.0008319240313573611\n",
      "batch_size: 32\n",
      "num_epochs: 65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, num_trees, tree_dim, output_dim):\n",
    "        super(NODE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = nn.ModuleList()\n",
    "            for _ in range(num_trees):\n",
    "                tree = nn.Sequential(\n",
    "                    nn.Linear(input_dim, tree_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(tree_dim, 1)\n",
    "                )\n",
    "                layer.append(tree)\n",
    "            self.layers.append(layer)\n",
    "        self.output = nn.Linear(num_layers * num_trees, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tree_outputs = []\n",
    "        for layer in self.layers:\n",
    "            layer_outputs = []\n",
    "            for tree in layer:\n",
    "                layer_outputs.append(tree(x))\n",
    "            layer_output = torch.cat(layer_outputs, dim=1)\n",
    "            tree_outputs.append(layer_output)\n",
    "        x = torch.cat(tree_outputs, dim=1)\n",
    "        return self.output(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for NODE\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    num_trees = trial.suggest_int('num_trees', 1, 10)\n",
    "    tree_dim = trial.suggest_int('tree_dim', 8, 64)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the NODE model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NODE(input_dim, num_layers, num_trees, tree_dim, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final NODE model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NODE(input_dim, \n",
    "                  best_params['num_layers'], \n",
    "                  best_params['num_trees'], \n",
    "                  best_params['tree_dim'], \n",
    "                  output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['NODE'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:44:31,670] A new study created in memory with name: no-name-8ee6defe-9ae0-49e3-84b8-860d75358470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.3455  | val_accuracy: 0.53846 |  0:00:02s\n",
      "epoch 1  | loss: 1.23687 | val_accuracy: 0.54872 |  0:00:04s\n",
      "epoch 2  | loss: 1.10571 | val_accuracy: 0.6     |  0:00:06s\n",
      "epoch 3  | loss: 1.05679 | val_accuracy: 0.56923 |  0:00:09s\n",
      "epoch 4  | loss: 1.11113 | val_accuracy: 0.58974 |  0:00:11s\n",
      "epoch 5  | loss: 1.04627 | val_accuracy: 0.61538 |  0:00:13s\n",
      "epoch 6  | loss: 0.92673 | val_accuracy: 0.57949 |  0:00:15s\n",
      "epoch 7  | loss: 0.8847  | val_accuracy: 0.58974 |  0:00:18s\n",
      "epoch 8  | loss: 0.91821 | val_accuracy: 0.62564 |  0:00:20s\n",
      "epoch 9  | loss: 0.9308  | val_accuracy: 0.58974 |  0:00:22s\n",
      "epoch 10 | loss: 0.87777 | val_accuracy: 0.59487 |  0:00:24s\n",
      "epoch 11 | loss: 0.8088  | val_accuracy: 0.61026 |  0:00:27s\n",
      "epoch 12 | loss: 0.84667 | val_accuracy: 0.61538 |  0:00:29s\n",
      "epoch 13 | loss: 0.80574 | val_accuracy: 0.61026 |  0:00:31s\n",
      "epoch 14 | loss: 0.82285 | val_accuracy: 0.61538 |  0:00:33s\n",
      "epoch 15 | loss: 0.78058 | val_accuracy: 0.64103 |  0:00:36s\n",
      "epoch 16 | loss: 0.82244 | val_accuracy: 0.6359  |  0:00:38s\n",
      "epoch 17 | loss: 0.77913 | val_accuracy: 0.61538 |  0:00:40s\n",
      "epoch 18 | loss: 0.78167 | val_accuracy: 0.62564 |  0:00:42s\n",
      "epoch 19 | loss: 0.78936 | val_accuracy: 0.61538 |  0:00:45s\n",
      "epoch 20 | loss: 0.75009 | val_accuracy: 0.60513 |  0:00:47s\n",
      "epoch 21 | loss: 0.75638 | val_accuracy: 0.58462 |  0:00:50s\n",
      "epoch 22 | loss: 0.7473  | val_accuracy: 0.58462 |  0:00:52s\n",
      "epoch 23 | loss: 0.81205 | val_accuracy: 0.60513 |  0:00:54s\n",
      "epoch 24 | loss: 0.71691 | val_accuracy: 0.56923 |  0:00:56s\n",
      "epoch 25 | loss: 0.72492 | val_accuracy: 0.63077 |  0:00:59s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_accuracy = 0.64103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:45:32,038] Trial 0 finished with value: 0.6410256410256411 and parameters: {'n_d': 59, 'n_a': 33, 'n_steps': 6, 'gamma': 1.5476170306282893, 'lambda_sparse': 8.999343538588935e-06, 'learning_rate': 0.00039879252081584095, 'batch_size': 32, 'num_epochs': 75}. Best is trial 0 with value: 0.6410256410256411.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.18003 | val_accuracy: 0.57949 |  0:00:00s\n",
      "epoch 1  | loss: 0.91123 | val_accuracy: 0.55897 |  0:00:01s\n",
      "epoch 2  | loss: 0.84461 | val_accuracy: 0.61538 |  0:00:03s\n",
      "epoch 3  | loss: 0.76336 | val_accuracy: 0.59487 |  0:00:04s\n",
      "epoch 4  | loss: 0.70471 | val_accuracy: 0.66667 |  0:00:05s\n",
      "epoch 5  | loss: 0.67972 | val_accuracy: 0.6359  |  0:00:06s\n",
      "epoch 6  | loss: 0.65276 | val_accuracy: 0.61538 |  0:00:07s\n",
      "epoch 7  | loss: 0.69059 | val_accuracy: 0.65128 |  0:00:08s\n",
      "epoch 8  | loss: 0.6712  | val_accuracy: 0.6359  |  0:00:10s\n",
      "epoch 9  | loss: 0.60784 | val_accuracy: 0.64615 |  0:00:11s\n",
      "epoch 10 | loss: 0.60861 | val_accuracy: 0.65641 |  0:00:12s\n",
      "epoch 11 | loss: 0.59797 | val_accuracy: 0.67179 |  0:00:13s\n",
      "epoch 12 | loss: 0.59042 | val_accuracy: 0.67179 |  0:00:14s\n",
      "epoch 13 | loss: 0.58979 | val_accuracy: 0.68205 |  0:00:15s\n",
      "epoch 14 | loss: 0.58307 | val_accuracy: 0.67692 |  0:00:16s\n",
      "epoch 15 | loss: 0.59343 | val_accuracy: 0.67179 |  0:00:16s\n",
      "epoch 16 | loss: 0.55876 | val_accuracy: 0.65641 |  0:00:17s\n",
      "epoch 17 | loss: 0.59614 | val_accuracy: 0.6359  |  0:00:18s\n",
      "epoch 18 | loss: 0.60143 | val_accuracy: 0.66154 |  0:00:19s\n",
      "epoch 19 | loss: 0.56845 | val_accuracy: 0.69744 |  0:00:20s\n",
      "epoch 20 | loss: 0.55636 | val_accuracy: 0.66667 |  0:00:22s\n",
      "epoch 21 | loss: 0.57648 | val_accuracy: 0.65128 |  0:00:23s\n",
      "epoch 22 | loss: 0.58397 | val_accuracy: 0.65128 |  0:00:24s\n",
      "epoch 23 | loss: 0.54571 | val_accuracy: 0.65128 |  0:00:25s\n",
      "epoch 24 | loss: 0.55495 | val_accuracy: 0.66154 |  0:00:27s\n",
      "epoch 25 | loss: 0.54144 | val_accuracy: 0.68718 |  0:00:28s\n",
      "epoch 26 | loss: 0.55905 | val_accuracy: 0.68718 |  0:00:29s\n",
      "epoch 27 | loss: 0.54261 | val_accuracy: 0.66667 |  0:00:30s\n",
      "epoch 28 | loss: 0.54422 | val_accuracy: 0.67692 |  0:00:31s\n",
      "epoch 29 | loss: 0.52449 | val_accuracy: 0.68718 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_accuracy = 0.69744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:46:05,056] Trial 1 finished with value: 0.6974358974358974 and parameters: {'n_d': 40, 'n_a': 57, 'n_steps': 4, 'gamma': 1.3414110165602908, 'lambda_sparse': 0.0008024543100408923, 'learning_rate': 0.0013123960913818625, 'batch_size': 64, 'num_epochs': 38}. Best is trial 1 with value: 0.6974358974358974.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.65466 | val_accuracy: 0.49744 |  0:00:00s\n",
      "epoch 1  | loss: 1.61414 | val_accuracy: 0.49231 |  0:00:01s\n",
      "epoch 2  | loss: 1.6684  | val_accuracy: 0.44615 |  0:00:01s\n",
      "epoch 3  | loss: 1.63902 | val_accuracy: 0.44103 |  0:00:02s\n",
      "epoch 4  | loss: 1.70561 | val_accuracy: 0.52821 |  0:00:02s\n",
      "epoch 5  | loss: 1.54254 | val_accuracy: 0.49744 |  0:00:03s\n",
      "epoch 6  | loss: 1.5023  | val_accuracy: 0.48205 |  0:00:04s\n",
      "epoch 7  | loss: 1.58504 | val_accuracy: 0.48718 |  0:00:04s\n",
      "epoch 8  | loss: 1.62683 | val_accuracy: 0.48718 |  0:00:05s\n",
      "epoch 9  | loss: 1.52458 | val_accuracy: 0.47692 |  0:00:05s\n",
      "epoch 10 | loss: 1.5991  | val_accuracy: 0.47179 |  0:00:06s\n",
      "epoch 11 | loss: 1.54844 | val_accuracy: 0.48205 |  0:00:06s\n",
      "epoch 12 | loss: 1.5282  | val_accuracy: 0.48718 |  0:00:07s\n",
      "epoch 13 | loss: 1.43934 | val_accuracy: 0.52308 |  0:00:07s\n",
      "epoch 14 | loss: 1.43686 | val_accuracy: 0.47692 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.52821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:46:13,824] Trial 2 finished with value: 0.5282051282051282 and parameters: {'n_d': 19, 'n_a': 13, 'n_steps': 10, 'gamma': 1.5509700315695467, 'lambda_sparse': 2.3398906937090753e-06, 'learning_rate': 0.00018917775478587736, 'batch_size': 256, 'num_epochs': 28}. Best is trial 1 with value: 0.6974358974358974.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.04211 | val_accuracy: 0.62051 |  0:00:01s\n",
      "epoch 1  | loss: 0.64577 | val_accuracy: 0.65641 |  0:00:02s\n",
      "epoch 2  | loss: 0.63166 | val_accuracy: 0.61538 |  0:00:04s\n",
      "epoch 3  | loss: 0.66746 | val_accuracy: 0.63077 |  0:00:05s\n",
      "epoch 4  | loss: 0.60867 | val_accuracy: 0.65641 |  0:00:05s\n",
      "epoch 5  | loss: 0.58108 | val_accuracy: 0.62051 |  0:00:06s\n",
      "epoch 6  | loss: 0.56772 | val_accuracy: 0.6     |  0:00:07s\n",
      "epoch 7  | loss: 0.55775 | val_accuracy: 0.6359  |  0:00:08s\n",
      "epoch 8  | loss: 0.56667 | val_accuracy: 0.68205 |  0:00:10s\n",
      "epoch 9  | loss: 0.57901 | val_accuracy: 0.69744 |  0:00:11s\n",
      "epoch 10 | loss: 0.56908 | val_accuracy: 0.71795 |  0:00:12s\n",
      "epoch 11 | loss: 0.56374 | val_accuracy: 0.69744 |  0:00:14s\n",
      "epoch 12 | loss: 0.56633 | val_accuracy: 0.70256 |  0:00:15s\n",
      "epoch 13 | loss: 0.54468 | val_accuracy: 0.72821 |  0:00:16s\n",
      "epoch 14 | loss: 0.54174 | val_accuracy: 0.70769 |  0:00:18s\n",
      "epoch 15 | loss: 0.55033 | val_accuracy: 0.67179 |  0:00:19s\n",
      "epoch 16 | loss: 0.57272 | val_accuracy: 0.68718 |  0:00:21s\n",
      "epoch 17 | loss: 0.56568 | val_accuracy: 0.69744 |  0:00:22s\n",
      "epoch 18 | loss: 0.54869 | val_accuracy: 0.66667 |  0:00:23s\n",
      "epoch 19 | loss: 0.54219 | val_accuracy: 0.66667 |  0:00:24s\n",
      "epoch 20 | loss: 0.55808 | val_accuracy: 0.67179 |  0:00:26s\n",
      "epoch 21 | loss: 0.53928 | val_accuracy: 0.67179 |  0:00:27s\n",
      "epoch 22 | loss: 0.53204 | val_accuracy: 0.68205 |  0:00:28s\n",
      "epoch 23 | loss: 0.53013 | val_accuracy: 0.68718 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_accuracy = 0.72821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:46:45,352] Trial 3 finished with value: 0.7282051282051282 and parameters: {'n_d': 11, 'n_a': 16, 'n_steps': 9, 'gamma': 1.404230164236873, 'lambda_sparse': 1.2337829675273215e-06, 'learning_rate': 0.07134761793748194, 'batch_size': 64, 'num_epochs': 70}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.29735 | val_accuracy: 0.5641  |  0:00:03s\n",
      "epoch 1  | loss: 1.11579 | val_accuracy: 0.55385 |  0:00:06s\n",
      "epoch 2  | loss: 1.06315 | val_accuracy: 0.57436 |  0:00:10s\n",
      "epoch 3  | loss: 0.88803 | val_accuracy: 0.57949 |  0:00:13s\n",
      "epoch 4  | loss: 0.90835 | val_accuracy: 0.6     |  0:00:17s\n",
      "epoch 5  | loss: 0.91372 | val_accuracy: 0.58974 |  0:00:19s\n",
      "epoch 6  | loss: 0.86397 | val_accuracy: 0.61026 |  0:00:23s\n",
      "epoch 7  | loss: 0.86803 | val_accuracy: 0.61538 |  0:00:26s\n",
      "epoch 8  | loss: 0.82269 | val_accuracy: 0.62051 |  0:00:29s\n",
      "epoch 9  | loss: 0.83786 | val_accuracy: 0.6359  |  0:00:31s\n",
      "epoch 10 | loss: 0.81276 | val_accuracy: 0.68718 |  0:00:34s\n",
      "epoch 11 | loss: 0.78183 | val_accuracy: 0.68205 |  0:00:36s\n",
      "epoch 12 | loss: 0.71076 | val_accuracy: 0.64103 |  0:00:38s\n",
      "epoch 13 | loss: 0.80223 | val_accuracy: 0.62564 |  0:00:41s\n",
      "epoch 14 | loss: 0.73746 | val_accuracy: 0.65641 |  0:00:43s\n",
      "epoch 15 | loss: 0.77867 | val_accuracy: 0.64103 |  0:00:46s\n",
      "epoch 16 | loss: 0.77091 | val_accuracy: 0.59487 |  0:00:48s\n",
      "epoch 17 | loss: 0.69572 | val_accuracy: 0.62051 |  0:00:50s\n",
      "epoch 18 | loss: 0.78115 | val_accuracy: 0.61026 |  0:00:53s\n",
      "epoch 19 | loss: 0.77375 | val_accuracy: 0.67179 |  0:00:55s\n",
      "epoch 20 | loss: 0.72565 | val_accuracy: 0.63077 |  0:00:58s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.68718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:47:44,925] Trial 4 finished with value: 0.6871794871794872 and parameters: {'n_d': 56, 'n_a': 39, 'n_steps': 7, 'gamma': 1.8213532806930595, 'lambda_sparse': 3.980811905742596e-06, 'learning_rate': 0.0013708519968903326, 'batch_size': 32, 'num_epochs': 94}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.36585 | val_accuracy: 0.65128 |  0:00:00s\n",
      "epoch 1  | loss: 1.02572 | val_accuracy: 0.66667 |  0:00:01s\n",
      "epoch 2  | loss: 0.90372 | val_accuracy: 0.60513 |  0:00:02s\n",
      "epoch 3  | loss: 0.7925  | val_accuracy: 0.62564 |  0:00:03s\n",
      "epoch 4  | loss: 0.80796 | val_accuracy: 0.65641 |  0:00:04s\n",
      "epoch 5  | loss: 0.82694 | val_accuracy: 0.64615 |  0:00:05s\n",
      "epoch 6  | loss: 0.78873 | val_accuracy: 0.64103 |  0:00:06s\n",
      "epoch 7  | loss: 0.70785 | val_accuracy: 0.63077 |  0:00:07s\n",
      "epoch 8  | loss: 0.84992 | val_accuracy: 0.58974 |  0:00:08s\n",
      "epoch 9  | loss: 0.92667 | val_accuracy: 0.64103 |  0:00:09s\n",
      "epoch 10 | loss: 0.78721 | val_accuracy: 0.62564 |  0:00:09s\n",
      "epoch 11 | loss: 0.8261  | val_accuracy: 0.62564 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.66667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:47:56,309] Trial 5 finished with value: 0.6666666666666666 and parameters: {'n_d': 42, 'n_a': 30, 'n_steps': 9, 'gamma': 1.6142740263713264, 'lambda_sparse': 1.2647803192835822e-06, 'learning_rate': 0.004554398523193363, 'batch_size': 128, 'num_epochs': 56}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.94312 | val_accuracy: 0.47692 |  0:00:00s\n",
      "epoch 1  | loss: 1.77209 | val_accuracy: 0.48205 |  0:00:01s\n",
      "epoch 2  | loss: 1.63438 | val_accuracy: 0.46667 |  0:00:02s\n",
      "epoch 3  | loss: 1.44056 | val_accuracy: 0.46154 |  0:00:03s\n",
      "epoch 4  | loss: 1.37958 | val_accuracy: 0.50769 |  0:00:04s\n",
      "epoch 5  | loss: 1.44444 | val_accuracy: 0.47692 |  0:00:05s\n",
      "epoch 6  | loss: 1.29807 | val_accuracy: 0.48718 |  0:00:05s\n",
      "epoch 7  | loss: 1.27879 | val_accuracy: 0.50769 |  0:00:06s\n",
      "epoch 8  | loss: 1.19025 | val_accuracy: 0.51282 |  0:00:07s\n",
      "epoch 9  | loss: 1.16078 | val_accuracy: 0.50256 |  0:00:08s\n",
      "epoch 10 | loss: 1.15595 | val_accuracy: 0.52821 |  0:00:09s\n",
      "epoch 11 | loss: 1.21282 | val_accuracy: 0.54872 |  0:00:10s\n",
      "epoch 12 | loss: 1.00202 | val_accuracy: 0.54359 |  0:00:10s\n",
      "epoch 13 | loss: 1.11962 | val_accuracy: 0.53333 |  0:00:11s\n",
      "epoch 14 | loss: 1.023   | val_accuracy: 0.54359 |  0:00:12s\n",
      "epoch 15 | loss: 1.07079 | val_accuracy: 0.53333 |  0:00:13s\n",
      "epoch 16 | loss: 0.98627 | val_accuracy: 0.58462 |  0:00:14s\n",
      "epoch 17 | loss: 1.01873 | val_accuracy: 0.56923 |  0:00:15s\n",
      "epoch 18 | loss: 0.9479  | val_accuracy: 0.58462 |  0:00:16s\n",
      "epoch 19 | loss: 1.01087 | val_accuracy: 0.55385 |  0:00:17s\n",
      "epoch 20 | loss: 1.00105 | val_accuracy: 0.57949 |  0:00:18s\n",
      "epoch 21 | loss: 0.95811 | val_accuracy: 0.57436 |  0:00:19s\n",
      "epoch 22 | loss: 0.95011 | val_accuracy: 0.55897 |  0:00:20s\n",
      "epoch 23 | loss: 0.93886 | val_accuracy: 0.5641  |  0:00:21s\n",
      "epoch 24 | loss: 0.95328 | val_accuracy: 0.57949 |  0:00:22s\n",
      "epoch 25 | loss: 0.91465 | val_accuracy: 0.52821 |  0:00:22s\n",
      "epoch 26 | loss: 0.87962 | val_accuracy: 0.53333 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_accuracy = 0.58462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:48:20,554] Trial 6 finished with value: 0.5846153846153846 and parameters: {'n_d': 17, 'n_a': 13, 'n_steps': 10, 'gamma': 1.9909554932764073, 'lambda_sparse': 1.1833416998961726e-05, 'learning_rate': 0.0012965475065764526, 'batch_size': 128, 'num_epochs': 43}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.89265 | val_accuracy: 0.53846 |  0:00:01s\n",
      "epoch 1  | loss: 2.40044 | val_accuracy: 0.52308 |  0:00:03s\n",
      "epoch 2  | loss: 1.64817 | val_accuracy: 0.68205 |  0:00:05s\n",
      "epoch 3  | loss: 0.64844 | val_accuracy: 0.55897 |  0:00:07s\n",
      "epoch 4  | loss: 0.67787 | val_accuracy: 0.67692 |  0:00:09s\n",
      "epoch 5  | loss: 0.60283 | val_accuracy: 0.66667 |  0:00:11s\n",
      "epoch 6  | loss: 0.55835 | val_accuracy: 0.65128 |  0:00:13s\n",
      "epoch 7  | loss: 0.59571 | val_accuracy: 0.69231 |  0:00:14s\n",
      "epoch 8  | loss: 0.55819 | val_accuracy: 0.65641 |  0:00:16s\n",
      "epoch 9  | loss: 0.57792 | val_accuracy: 0.71282 |  0:00:18s\n",
      "epoch 10 | loss: 0.5819  | val_accuracy: 0.72308 |  0:00:20s\n",
      "epoch 11 | loss: 0.58184 | val_accuracy: 0.70769 |  0:00:22s\n",
      "epoch 12 | loss: 0.59785 | val_accuracy: 0.67179 |  0:00:24s\n",
      "Stop training because you reached max_epochs = 13 with best_epoch = 10 and best_val_accuracy = 0.72308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:48:45,508] Trial 7 finished with value: 0.7230769230769231 and parameters: {'n_d': 63, 'n_a': 54, 'n_steps': 8, 'gamma': 1.8129968435339547, 'lambda_sparse': 2.9241409019451484e-05, 'learning_rate': 0.03411868127500712, 'batch_size': 64, 'num_epochs': 13}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.78655 | val_accuracy: 0.56923 |  0:00:01s\n",
      "epoch 1  | loss: 1.55849 | val_accuracy: 0.58462 |  0:00:02s\n",
      "epoch 2  | loss: 1.50347 | val_accuracy: 0.58462 |  0:00:04s\n",
      "epoch 3  | loss: 1.38113 | val_accuracy: 0.55385 |  0:00:05s\n",
      "epoch 4  | loss: 1.28113 | val_accuracy: 0.58462 |  0:00:07s\n",
      "epoch 5  | loss: 1.18666 | val_accuracy: 0.57436 |  0:00:08s\n",
      "epoch 6  | loss: 1.08573 | val_accuracy: 0.61026 |  0:00:10s\n",
      "epoch 7  | loss: 1.1432  | val_accuracy: 0.58974 |  0:00:11s\n",
      "epoch 8  | loss: 1.06433 | val_accuracy: 0.56923 |  0:00:12s\n",
      "epoch 9  | loss: 1.03129 | val_accuracy: 0.63077 |  0:00:14s\n",
      "epoch 10 | loss: 0.93909 | val_accuracy: 0.62051 |  0:00:15s\n",
      "epoch 11 | loss: 0.94224 | val_accuracy: 0.62051 |  0:00:17s\n",
      "epoch 12 | loss: 0.91405 | val_accuracy: 0.63077 |  0:00:18s\n",
      "epoch 13 | loss: 0.97575 | val_accuracy: 0.6359  |  0:00:19s\n",
      "epoch 14 | loss: 0.87155 | val_accuracy: 0.61538 |  0:00:21s\n",
      "epoch 15 | loss: 0.87649 | val_accuracy: 0.64615 |  0:00:22s\n",
      "epoch 16 | loss: 0.84449 | val_accuracy: 0.65641 |  0:00:23s\n",
      "epoch 17 | loss: 0.844   | val_accuracy: 0.6359  |  0:00:25s\n",
      "epoch 18 | loss: 0.88427 | val_accuracy: 0.66154 |  0:00:26s\n",
      "epoch 19 | loss: 0.89101 | val_accuracy: 0.67692 |  0:00:28s\n",
      "epoch 20 | loss: 0.89455 | val_accuracy: 0.65128 |  0:00:29s\n",
      "epoch 21 | loss: 0.80759 | val_accuracy: 0.63077 |  0:00:30s\n",
      "epoch 22 | loss: 0.77633 | val_accuracy: 0.66154 |  0:00:32s\n",
      "epoch 23 | loss: 0.8095  | val_accuracy: 0.65641 |  0:00:33s\n",
      "epoch 24 | loss: 0.85607 | val_accuracy: 0.68205 |  0:00:35s\n",
      "epoch 25 | loss: 0.73896 | val_accuracy: 0.6359  |  0:00:36s\n",
      "epoch 26 | loss: 0.86602 | val_accuracy: 0.63077 |  0:00:37s\n",
      "epoch 27 | loss: 0.83554 | val_accuracy: 0.68205 |  0:00:39s\n",
      "epoch 28 | loss: 0.75187 | val_accuracy: 0.68718 |  0:00:40s\n",
      "epoch 29 | loss: 0.81324 | val_accuracy: 0.67692 |  0:00:42s\n",
      "epoch 30 | loss: 0.83155 | val_accuracy: 0.68718 |  0:00:44s\n",
      "epoch 31 | loss: 0.7445  | val_accuracy: 0.67179 |  0:00:45s\n",
      "epoch 32 | loss: 0.73323 | val_accuracy: 0.66667 |  0:00:47s\n",
      "epoch 33 | loss: 0.74203 | val_accuracy: 0.68205 |  0:00:48s\n",
      "epoch 34 | loss: 0.75863 | val_accuracy: 0.67179 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 35 with best_epoch = 28 and best_val_accuracy = 0.68718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:49:36,284] Trial 8 finished with value: 0.6871794871794872 and parameters: {'n_d': 28, 'n_a': 42, 'n_steps': 7, 'gamma': 1.1794628755577543, 'lambda_sparse': 0.0006027494074851571, 'learning_rate': 0.00023648338170391018, 'batch_size': 64, 'num_epochs': 35}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.12046 | val_accuracy: 0.62051 |  0:00:00s\n",
      "epoch 1  | loss: 1.47043 | val_accuracy: 0.57949 |  0:00:01s\n",
      "epoch 2  | loss: 1.56357 | val_accuracy: 0.5641  |  0:00:01s\n",
      "epoch 3  | loss: 1.9029  | val_accuracy: 0.52308 |  0:00:02s\n",
      "epoch 4  | loss: 2.07697 | val_accuracy: 0.66154 |  0:00:02s\n",
      "epoch 5  | loss: 0.92697 | val_accuracy: 0.60513 |  0:00:03s\n",
      "epoch 6  | loss: 0.70518 | val_accuracy: 0.66154 |  0:00:03s\n",
      "epoch 7  | loss: 0.77203 | val_accuracy: 0.66667 |  0:00:04s\n",
      "epoch 8  | loss: 0.65628 | val_accuracy: 0.64103 |  0:00:04s\n",
      "epoch 9  | loss: 0.61985 | val_accuracy: 0.67692 |  0:00:05s\n",
      "epoch 10 | loss: 0.58931 | val_accuracy: 0.66667 |  0:00:05s\n",
      "epoch 11 | loss: 0.55519 | val_accuracy: 0.67692 |  0:00:06s\n",
      "epoch 12 | loss: 0.5697  | val_accuracy: 0.62564 |  0:00:06s\n",
      "epoch 13 | loss: 0.55774 | val_accuracy: 0.65128 |  0:00:07s\n",
      "epoch 14 | loss: 0.69059 | val_accuracy: 0.67692 |  0:00:07s\n",
      "epoch 15 | loss: 0.56876 | val_accuracy: 0.66667 |  0:00:08s\n",
      "epoch 16 | loss: 0.56167 | val_accuracy: 0.67692 |  0:00:08s\n",
      "epoch 17 | loss: 0.53819 | val_accuracy: 0.68205 |  0:00:09s\n",
      "epoch 18 | loss: 0.54411 | val_accuracy: 0.70256 |  0:00:10s\n",
      "epoch 19 | loss: 0.5309  | val_accuracy: 0.64103 |  0:00:10s\n",
      "epoch 20 | loss: 0.53468 | val_accuracy: 0.69231 |  0:00:11s\n",
      "Stop training because you reached max_epochs = 21 with best_epoch = 18 and best_val_accuracy = 0.70256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:49:47,936] Trial 9 finished with value: 0.7025641025641025 and parameters: {'n_d': 48, 'n_a': 41, 'n_steps': 7, 'gamma': 1.4511936041145181, 'lambda_sparse': 0.0005605219569625983, 'learning_rate': 0.03800782640949999, 'batch_size': 256, 'num_epochs': 21}. Best is trial 3 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.725   | val_accuracy: 0.62051 |  0:00:00s\n",
      "epoch 1  | loss: 0.61806 | val_accuracy: 0.64103 |  0:00:01s\n",
      "epoch 2  | loss: 0.5714  | val_accuracy: 0.61538 |  0:00:01s\n",
      "epoch 3  | loss: 0.60143 | val_accuracy: 0.64615 |  0:00:02s\n",
      "epoch 4  | loss: 0.58444 | val_accuracy: 0.74872 |  0:00:02s\n",
      "epoch 5  | loss: 0.59563 | val_accuracy: 0.67179 |  0:00:03s\n",
      "epoch 6  | loss: 0.55921 | val_accuracy: 0.67179 |  0:00:04s\n",
      "epoch 7  | loss: 0.55897 | val_accuracy: 0.69231 |  0:00:05s\n",
      "epoch 8  | loss: 0.54446 | val_accuracy: 0.68718 |  0:00:05s\n",
      "epoch 9  | loss: 0.56299 | val_accuracy: 0.67692 |  0:00:06s\n",
      "epoch 10 | loss: 0.57437 | val_accuracy: 0.68205 |  0:00:07s\n",
      "epoch 11 | loss: 0.56951 | val_accuracy: 0.68718 |  0:00:07s\n",
      "epoch 12 | loss: 0.55318 | val_accuracy: 0.69231 |  0:00:08s\n",
      "epoch 13 | loss: 0.53782 | val_accuracy: 0.69231 |  0:00:08s\n",
      "epoch 14 | loss: 0.56136 | val_accuracy: 0.67692 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.74872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:49:58,694] Trial 10 finished with value: 0.7487179487179487 and parameters: {'n_d': 11, 'n_a': 23, 'n_steps': 3, 'gamma': 1.0028132741310292, 'lambda_sparse': 0.00011931663455668031, 'learning_rate': 0.08997719231506193, 'batch_size': 64, 'num_epochs': 72}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73411 | val_accuracy: 0.55897 |  0:00:01s\n",
      "epoch 1  | loss: 0.60479 | val_accuracy: 0.70769 |  0:00:02s\n",
      "epoch 2  | loss: 0.57832 | val_accuracy: 0.60513 |  0:00:03s\n",
      "epoch 3  | loss: 0.56852 | val_accuracy: 0.65128 |  0:00:04s\n",
      "epoch 4  | loss: 0.57539 | val_accuracy: 0.70769 |  0:00:07s\n",
      "epoch 5  | loss: 0.55346 | val_accuracy: 0.71282 |  0:00:08s\n",
      "epoch 6  | loss: 0.60003 | val_accuracy: 0.71282 |  0:00:09s\n",
      "epoch 7  | loss: 0.58335 | val_accuracy: 0.67179 |  0:00:11s\n",
      "epoch 8  | loss: 0.57541 | val_accuracy: 0.67692 |  0:00:12s\n",
      "epoch 9  | loss: 0.55548 | val_accuracy: 0.68718 |  0:00:13s\n",
      "epoch 10 | loss: 0.5339  | val_accuracy: 0.70769 |  0:00:14s\n",
      "epoch 11 | loss: 0.5391  | val_accuracy: 0.69744 |  0:00:15s\n",
      "epoch 12 | loss: 0.55718 | val_accuracy: 0.68205 |  0:00:15s\n",
      "epoch 13 | loss: 0.54926 | val_accuracy: 0.71795 |  0:00:16s\n",
      "epoch 14 | loss: 0.52958 | val_accuracy: 0.72308 |  0:00:16s\n",
      "epoch 15 | loss: 0.52479 | val_accuracy: 0.71282 |  0:00:17s\n",
      "epoch 16 | loss: 0.53687 | val_accuracy: 0.71795 |  0:00:18s\n",
      "epoch 17 | loss: 0.53582 | val_accuracy: 0.68205 |  0:00:18s\n",
      "epoch 18 | loss: 0.56563 | val_accuracy: 0.67179 |  0:00:19s\n",
      "epoch 19 | loss: 0.54038 | val_accuracy: 0.67692 |  0:00:20s\n",
      "epoch 20 | loss: 0.52796 | val_accuracy: 0.73333 |  0:00:20s\n",
      "epoch 21 | loss: 0.51764 | val_accuracy: 0.72308 |  0:00:21s\n",
      "epoch 22 | loss: 0.52753 | val_accuracy: 0.70769 |  0:00:21s\n",
      "epoch 23 | loss: 0.50967 | val_accuracy: 0.70256 |  0:00:22s\n",
      "epoch 24 | loss: 0.51017 | val_accuracy: 0.68718 |  0:00:23s\n",
      "epoch 25 | loss: 0.55346 | val_accuracy: 0.70769 |  0:00:24s\n",
      "epoch 26 | loss: 0.53847 | val_accuracy: 0.70769 |  0:00:24s\n",
      "epoch 27 | loss: 0.56121 | val_accuracy: 0.65128 |  0:00:25s\n",
      "epoch 28 | loss: 0.55407 | val_accuracy: 0.72821 |  0:00:25s\n",
      "epoch 29 | loss: 0.55355 | val_accuracy: 0.66154 |  0:00:26s\n",
      "epoch 30 | loss: 0.55985 | val_accuracy: 0.72821 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:50:26,526] Trial 11 finished with value: 0.7333333333333333 and parameters: {'n_d': 10, 'n_a': 24, 'n_steps': 3, 'gamma': 1.019914815326508, 'lambda_sparse': 0.00011787027972966686, 'learning_rate': 0.06500824374847447, 'batch_size': 64, 'num_epochs': 69}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.85509 | val_accuracy: 0.60513 |  0:00:00s\n",
      "epoch 1  | loss: 0.66413 | val_accuracy: 0.68205 |  0:00:01s\n",
      "epoch 2  | loss: 0.61612 | val_accuracy: 0.66667 |  0:00:01s\n",
      "epoch 3  | loss: 0.57155 | val_accuracy: 0.6359  |  0:00:02s\n",
      "epoch 4  | loss: 0.54151 | val_accuracy: 0.66154 |  0:00:02s\n",
      "epoch 5  | loss: 0.55906 | val_accuracy: 0.67179 |  0:00:03s\n",
      "epoch 6  | loss: 0.54475 | val_accuracy: 0.64103 |  0:00:04s\n",
      "epoch 7  | loss: 0.53734 | val_accuracy: 0.71282 |  0:00:05s\n",
      "epoch 8  | loss: 0.52914 | val_accuracy: 0.68718 |  0:00:05s\n",
      "epoch 9  | loss: 0.53314 | val_accuracy: 0.70256 |  0:00:06s\n",
      "epoch 10 | loss: 0.54296 | val_accuracy: 0.68718 |  0:00:07s\n",
      "epoch 11 | loss: 0.50815 | val_accuracy: 0.66667 |  0:00:07s\n",
      "epoch 12 | loss: 0.51784 | val_accuracy: 0.67179 |  0:00:08s\n",
      "epoch 13 | loss: 0.52516 | val_accuracy: 0.66667 |  0:00:09s\n",
      "epoch 14 | loss: 0.51352 | val_accuracy: 0.68205 |  0:00:09s\n",
      "epoch 15 | loss: 0.53195 | val_accuracy: 0.68718 |  0:00:10s\n",
      "epoch 16 | loss: 0.53236 | val_accuracy: 0.69231 |  0:00:10s\n",
      "epoch 17 | loss: 0.5216  | val_accuracy: 0.70769 |  0:00:11s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.71282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:50:38,306] Trial 12 finished with value: 0.7128205128205128 and parameters: {'n_d': 10, 'n_a': 23, 'n_steps': 3, 'gamma': 1.007226909344278, 'lambda_sparse': 0.00014435615750468147, 'learning_rate': 0.013181071475732711, 'batch_size': 64, 'num_epochs': 83}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.01223 | val_accuracy: 0.63077 |  0:00:01s\n",
      "epoch 1  | loss: 0.74535 | val_accuracy: 0.73333 |  0:00:02s\n",
      "epoch 2  | loss: 0.63896 | val_accuracy: 0.69744 |  0:00:03s\n",
      "epoch 3  | loss: 0.58618 | val_accuracy: 0.73333 |  0:00:04s\n",
      "epoch 4  | loss: 0.58325 | val_accuracy: 0.70769 |  0:00:05s\n",
      "epoch 5  | loss: 0.56194 | val_accuracy: 0.69744 |  0:00:06s\n",
      "epoch 6  | loss: 0.5624  | val_accuracy: 0.64103 |  0:00:07s\n",
      "epoch 7  | loss: 0.55993 | val_accuracy: 0.68718 |  0:00:08s\n",
      "epoch 8  | loss: 0.55517 | val_accuracy: 0.70769 |  0:00:09s\n",
      "epoch 9  | loss: 0.54822 | val_accuracy: 0.70769 |  0:00:10s\n",
      "epoch 10 | loss: 0.56334 | val_accuracy: 0.71795 |  0:00:12s\n",
      "epoch 11 | loss: 0.56111 | val_accuracy: 0.71282 |  0:00:14s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:50:53,475] Trial 13 finished with value: 0.7333333333333333 and parameters: {'n_d': 28, 'n_a': 24, 'n_steps': 5, 'gamma': 1.0012248384119773, 'lambda_sparse': 0.0001245041106542963, 'learning_rate': 0.09313275025245546, 'batch_size': 64, 'num_epochs': 59}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.98997 | val_accuracy: 0.60513 |  0:00:00s\n",
      "epoch 1  | loss: 0.6521  | val_accuracy: 0.56923 |  0:00:02s\n",
      "epoch 2  | loss: 0.59444 | val_accuracy: 0.62564 |  0:00:03s\n",
      "epoch 3  | loss: 0.59227 | val_accuracy: 0.62564 |  0:00:04s\n",
      "epoch 4  | loss: 0.58246 | val_accuracy: 0.65641 |  0:00:04s\n",
      "epoch 5  | loss: 0.57851 | val_accuracy: 0.62051 |  0:00:05s\n",
      "epoch 6  | loss: 0.57358 | val_accuracy: 0.64103 |  0:00:05s\n",
      "epoch 7  | loss: 0.56627 | val_accuracy: 0.66154 |  0:00:06s\n",
      "epoch 8  | loss: 0.56012 | val_accuracy: 0.67179 |  0:00:07s\n",
      "epoch 9  | loss: 0.5514  | val_accuracy: 0.67692 |  0:00:07s\n",
      "epoch 10 | loss: 0.52606 | val_accuracy: 0.68718 |  0:00:08s\n",
      "epoch 11 | loss: 0.52105 | val_accuracy: 0.69744 |  0:00:08s\n",
      "epoch 12 | loss: 0.51747 | val_accuracy: 0.69231 |  0:00:09s\n",
      "epoch 13 | loss: 0.53074 | val_accuracy: 0.71795 |  0:00:10s\n",
      "epoch 14 | loss: 0.50838 | val_accuracy: 0.70769 |  0:00:10s\n",
      "epoch 15 | loss: 0.52697 | val_accuracy: 0.69231 |  0:00:11s\n",
      "epoch 16 | loss: 0.54188 | val_accuracy: 0.70769 |  0:00:11s\n",
      "epoch 17 | loss: 0.52013 | val_accuracy: 0.69231 |  0:00:12s\n",
      "epoch 18 | loss: 0.52023 | val_accuracy: 0.71795 |  0:00:13s\n",
      "epoch 19 | loss: 0.50828 | val_accuracy: 0.71282 |  0:00:13s\n",
      "epoch 20 | loss: 0.50566 | val_accuracy: 0.69231 |  0:00:14s\n",
      "epoch 21 | loss: 0.48468 | val_accuracy: 0.68205 |  0:00:14s\n",
      "epoch 22 | loss: 0.53683 | val_accuracy: 0.72821 |  0:00:15s\n",
      "epoch 23 | loss: 0.52241 | val_accuracy: 0.71795 |  0:00:16s\n",
      "epoch 24 | loss: 0.52084 | val_accuracy: 0.73333 |  0:00:16s\n",
      "epoch 25 | loss: 0.51196 | val_accuracy: 0.74359 |  0:00:17s\n",
      "epoch 26 | loss: 0.49877 | val_accuracy: 0.71795 |  0:00:17s\n",
      "epoch 27 | loss: 0.50161 | val_accuracy: 0.73846 |  0:00:18s\n",
      "epoch 28 | loss: 0.4705  | val_accuracy: 0.73846 |  0:00:18s\n",
      "epoch 29 | loss: 0.49428 | val_accuracy: 0.69744 |  0:00:19s\n",
      "epoch 30 | loss: 0.49773 | val_accuracy: 0.65641 |  0:00:20s\n",
      "epoch 31 | loss: 0.48971 | val_accuracy: 0.67179 |  0:00:20s\n",
      "epoch 32 | loss: 0.48195 | val_accuracy: 0.70769 |  0:00:21s\n",
      "epoch 33 | loss: 0.51767 | val_accuracy: 0.67179 |  0:00:21s\n",
      "epoch 34 | loss: 0.49899 | val_accuracy: 0.71282 |  0:00:22s\n",
      "epoch 35 | loss: 0.50257 | val_accuracy: 0.74359 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_accuracy = 0.74359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:51:16,973] Trial 14 finished with value: 0.7435897435897436 and parameters: {'n_d': 26, 'n_a': 22, 'n_steps': 3, 'gamma': 1.1770541747833232, 'lambda_sparse': 0.00011336981078064036, 'learning_rate': 0.012671095734378352, 'batch_size': 64, 'num_epochs': 98}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.19408 | val_accuracy: 0.61538 |  0:00:00s\n",
      "epoch 1  | loss: 0.7105  | val_accuracy: 0.65641 |  0:00:01s\n",
      "epoch 2  | loss: 0.68093 | val_accuracy: 0.67179 |  0:00:02s\n",
      "epoch 3  | loss: 0.67994 | val_accuracy: 0.70256 |  0:00:03s\n",
      "epoch 4  | loss: 0.62574 | val_accuracy: 0.68718 |  0:00:03s\n",
      "epoch 5  | loss: 0.61391 | val_accuracy: 0.66154 |  0:00:04s\n",
      "epoch 6  | loss: 0.6218  | val_accuracy: 0.66154 |  0:00:05s\n",
      "epoch 7  | loss: 0.59396 | val_accuracy: 0.68718 |  0:00:05s\n",
      "epoch 8  | loss: 0.60163 | val_accuracy: 0.65641 |  0:00:06s\n",
      "epoch 9  | loss: 0.59205 | val_accuracy: 0.69231 |  0:00:07s\n",
      "epoch 10 | loss: 0.56455 | val_accuracy: 0.70769 |  0:00:07s\n",
      "epoch 11 | loss: 0.56053 | val_accuracy: 0.67692 |  0:00:08s\n",
      "epoch 12 | loss: 0.55965 | val_accuracy: 0.69744 |  0:00:10s\n",
      "epoch 13 | loss: 0.54866 | val_accuracy: 0.74359 |  0:00:11s\n",
      "epoch 14 | loss: 0.54653 | val_accuracy: 0.72308 |  0:00:12s\n",
      "epoch 15 | loss: 0.53961 | val_accuracy: 0.71282 |  0:00:14s\n",
      "epoch 16 | loss: 0.54778 | val_accuracy: 0.69231 |  0:00:15s\n",
      "epoch 17 | loss: 0.53747 | val_accuracy: 0.69231 |  0:00:16s\n",
      "epoch 18 | loss: 0.51857 | val_accuracy: 0.71795 |  0:00:17s\n",
      "epoch 19 | loss: 0.53718 | val_accuracy: 0.68718 |  0:00:19s\n",
      "epoch 20 | loss: 0.5222  | val_accuracy: 0.69744 |  0:00:20s\n",
      "epoch 21 | loss: 0.53271 | val_accuracy: 0.68205 |  0:00:21s\n",
      "epoch 22 | loss: 0.50848 | val_accuracy: 0.66667 |  0:00:22s\n",
      "epoch 23 | loss: 0.51038 | val_accuracy: 0.71282 |  0:00:22s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_accuracy = 0.74359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:51:40,210] Trial 15 finished with value: 0.7435897435897436 and parameters: {'n_d': 28, 'n_a': 8, 'n_steps': 4, 'gamma': 1.2276909650001775, 'lambda_sparse': 4.609742620157796e-05, 'learning_rate': 0.01046104598396773, 'batch_size': 64, 'num_epochs': 99}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.07586 | val_accuracy: 0.5641  |  0:00:00s\n",
      "epoch 1  | loss: 0.81028 | val_accuracy: 0.64103 |  0:00:01s\n",
      "epoch 2  | loss: 0.70333 | val_accuracy: 0.63077 |  0:00:02s\n",
      "epoch 3  | loss: 0.63239 | val_accuracy: 0.60513 |  0:00:04s\n",
      "epoch 4  | loss: 0.6014  | val_accuracy: 0.69744 |  0:00:05s\n",
      "epoch 5  | loss: 0.56139 | val_accuracy: 0.66667 |  0:00:06s\n",
      "epoch 6  | loss: 0.56018 | val_accuracy: 0.67179 |  0:00:06s\n",
      "epoch 7  | loss: 0.54414 | val_accuracy: 0.65128 |  0:00:07s\n",
      "epoch 8  | loss: 0.52857 | val_accuracy: 0.68205 |  0:00:07s\n",
      "epoch 9  | loss: 0.52169 | val_accuracy: 0.70256 |  0:00:08s\n",
      "epoch 10 | loss: 0.51725 | val_accuracy: 0.69231 |  0:00:08s\n",
      "epoch 11 | loss: 0.53137 | val_accuracy: 0.72821 |  0:00:09s\n",
      "epoch 12 | loss: 0.52413 | val_accuracy: 0.70256 |  0:00:09s\n",
      "epoch 13 | loss: 0.52285 | val_accuracy: 0.69231 |  0:00:10s\n",
      "epoch 14 | loss: 0.49649 | val_accuracy: 0.68718 |  0:00:10s\n",
      "epoch 15 | loss: 0.49719 | val_accuracy: 0.71282 |  0:00:11s\n",
      "epoch 16 | loss: 0.49657 | val_accuracy: 0.70769 |  0:00:12s\n",
      "epoch 17 | loss: 0.51579 | val_accuracy: 0.70256 |  0:00:12s\n",
      "epoch 18 | loss: 0.52394 | val_accuracy: 0.68205 |  0:00:13s\n",
      "epoch 19 | loss: 0.51551 | val_accuracy: 0.66154 |  0:00:13s\n",
      "epoch 20 | loss: 0.49483 | val_accuracy: 0.66667 |  0:00:14s\n",
      "epoch 21 | loss: 0.4779  | val_accuracy: 0.69744 |  0:00:14s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.72821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:51:55,510] Trial 16 finished with value: 0.7282051282051282 and parameters: {'n_d': 22, 'n_a': 21, 'n_steps': 5, 'gamma': 1.180796063007985, 'lambda_sparse': 0.00024659741018132205, 'learning_rate': 0.015507565148104318, 'batch_size': 128, 'num_epochs': 89}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.18468 | val_accuracy: 0.64103 |  0:00:00s\n",
      "epoch 1  | loss: 0.8381  | val_accuracy: 0.62564 |  0:00:00s\n",
      "epoch 2  | loss: 0.7023  | val_accuracy: 0.63077 |  0:00:00s\n",
      "epoch 3  | loss: 0.62516 | val_accuracy: 0.58462 |  0:00:01s\n",
      "epoch 4  | loss: 0.61198 | val_accuracy: 0.64103 |  0:00:01s\n",
      "epoch 5  | loss: 0.55528 | val_accuracy: 0.65128 |  0:00:01s\n",
      "epoch 6  | loss: 0.54548 | val_accuracy: 0.63077 |  0:00:02s\n",
      "epoch 7  | loss: 0.54303 | val_accuracy: 0.6359  |  0:00:02s\n",
      "epoch 8  | loss: 0.54383 | val_accuracy: 0.63077 |  0:00:02s\n",
      "epoch 9  | loss: 0.54389 | val_accuracy: 0.66667 |  0:00:03s\n",
      "epoch 10 | loss: 0.55261 | val_accuracy: 0.68205 |  0:00:03s\n",
      "epoch 11 | loss: 0.49234 | val_accuracy: 0.68205 |  0:00:04s\n",
      "epoch 12 | loss: 0.53854 | val_accuracy: 0.6359  |  0:00:04s\n",
      "epoch 13 | loss: 0.50797 | val_accuracy: 0.68205 |  0:00:04s\n",
      "epoch 14 | loss: 0.49345 | val_accuracy: 0.69231 |  0:00:05s\n",
      "epoch 15 | loss: 0.50098 | val_accuracy: 0.65641 |  0:00:05s\n",
      "epoch 16 | loss: 0.48586 | val_accuracy: 0.63077 |  0:00:05s\n",
      "epoch 17 | loss: 0.48756 | val_accuracy: 0.66667 |  0:00:06s\n",
      "epoch 18 | loss: 0.46337 | val_accuracy: 0.66154 |  0:00:06s\n",
      "epoch 19 | loss: 0.4685  | val_accuracy: 0.69231 |  0:00:06s\n",
      "epoch 20 | loss: 0.48354 | val_accuracy: 0.70769 |  0:00:07s\n",
      "epoch 21 | loss: 0.46731 | val_accuracy: 0.71795 |  0:00:07s\n",
      "epoch 22 | loss: 0.45807 | val_accuracy: 0.69744 |  0:00:07s\n",
      "epoch 23 | loss: 0.44542 | val_accuracy: 0.71795 |  0:00:08s\n",
      "epoch 24 | loss: 0.42598 | val_accuracy: 0.71282 |  0:00:08s\n",
      "epoch 25 | loss: 0.41976 | val_accuracy: 0.68718 |  0:00:09s\n",
      "epoch 26 | loss: 0.42918 | val_accuracy: 0.70256 |  0:00:09s\n",
      "epoch 27 | loss: 0.43472 | val_accuracy: 0.72821 |  0:00:09s\n",
      "epoch 28 | loss: 0.45624 | val_accuracy: 0.71282 |  0:00:10s\n",
      "epoch 29 | loss: 0.43575 | val_accuracy: 0.70256 |  0:00:10s\n",
      "epoch 30 | loss: 0.43849 | val_accuracy: 0.72308 |  0:00:10s\n",
      "epoch 31 | loss: 0.44522 | val_accuracy: 0.69744 |  0:00:11s\n",
      "epoch 32 | loss: 0.4231  | val_accuracy: 0.69231 |  0:00:11s\n",
      "epoch 33 | loss: 0.42647 | val_accuracy: 0.70769 |  0:00:11s\n",
      "epoch 34 | loss: 0.43113 | val_accuracy: 0.70256 |  0:00:12s\n",
      "epoch 35 | loss: 0.42977 | val_accuracy: 0.70769 |  0:00:12s\n",
      "epoch 36 | loss: 0.42306 | val_accuracy: 0.70256 |  0:00:13s\n",
      "epoch 37 | loss: 0.41472 | val_accuracy: 0.67179 |  0:00:13s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_accuracy = 0.72821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:52:09,397] Trial 17 finished with value: 0.7282051282051282 and parameters: {'n_d': 34, 'n_a': 47, 'n_steps': 4, 'gamma': 1.1213119598089212, 'lambda_sparse': 4.5657879639362306e-05, 'learning_rate': 0.007294674115382554, 'batch_size': 256, 'num_epochs': 82}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.74272 | val_accuracy: 0.66154 |  0:00:01s\n",
      "epoch 1  | loss: 0.60172 | val_accuracy: 0.65641 |  0:00:02s\n",
      "epoch 2  | loss: 0.5981  | val_accuracy: 0.64103 |  0:00:03s\n",
      "epoch 3  | loss: 0.59535 | val_accuracy: 0.66667 |  0:00:05s\n",
      "epoch 4  | loss: 0.57755 | val_accuracy: 0.68718 |  0:00:07s\n",
      "epoch 5  | loss: 0.56265 | val_accuracy: 0.72821 |  0:00:08s\n",
      "epoch 6  | loss: 0.54351 | val_accuracy: 0.71282 |  0:00:09s\n",
      "epoch 7  | loss: 0.57042 | val_accuracy: 0.69744 |  0:00:10s\n",
      "epoch 8  | loss: 0.54942 | val_accuracy: 0.69744 |  0:00:12s\n",
      "epoch 9  | loss: 0.5601  | val_accuracy: 0.70256 |  0:00:13s\n",
      "epoch 10 | loss: 0.52986 | val_accuracy: 0.73333 |  0:00:15s\n",
      "epoch 11 | loss: 0.55152 | val_accuracy: 0.70256 |  0:00:16s\n",
      "epoch 12 | loss: 0.51986 | val_accuracy: 0.70256 |  0:00:17s\n",
      "epoch 13 | loss: 0.56096 | val_accuracy: 0.71282 |  0:00:19s\n",
      "epoch 14 | loss: 0.54058 | val_accuracy: 0.72821 |  0:00:20s\n",
      "epoch 15 | loss: 0.5455  | val_accuracy: 0.69744 |  0:00:21s\n",
      "epoch 16 | loss: 0.53142 | val_accuracy: 0.68718 |  0:00:22s\n",
      "epoch 17 | loss: 0.51739 | val_accuracy: 0.68718 |  0:00:24s\n",
      "epoch 18 | loss: 0.5461  | val_accuracy: 0.71795 |  0:00:25s\n",
      "epoch 19 | loss: 0.52312 | val_accuracy: 0.69744 |  0:00:28s\n",
      "epoch 20 | loss: 0.55126 | val_accuracy: 0.70769 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:52:41,086] Trial 18 finished with value: 0.7333333333333333 and parameters: {'n_d': 18, 'n_a': 30, 'n_steps': 3, 'gamma': 1.3014580853472242, 'lambda_sparse': 0.00032963018224346937, 'learning_rate': 0.027742108297274918, 'batch_size': 32, 'num_epochs': 100}. Best is trial 10 with value: 0.7487179487179487.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\1744437722.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.06468 | val_accuracy: 0.6359  |  0:00:01s\n",
      "epoch 1  | loss: 0.70026 | val_accuracy: 0.68205 |  0:00:03s\n",
      "epoch 2  | loss: 0.72526 | val_accuracy: 0.67692 |  0:00:05s\n",
      "epoch 3  | loss: 0.67881 | val_accuracy: 0.69231 |  0:00:07s\n",
      "epoch 4  | loss: 0.6566  | val_accuracy: 0.65128 |  0:00:09s\n",
      "epoch 5  | loss: 0.64935 | val_accuracy: 0.67692 |  0:00:11s\n",
      "epoch 6  | loss: 0.58683 | val_accuracy: 0.65641 |  0:00:12s\n",
      "epoch 7  | loss: 0.58752 | val_accuracy: 0.68205 |  0:00:16s\n",
      "epoch 8  | loss: 0.61817 | val_accuracy: 0.69231 |  0:00:17s\n",
      "epoch 9  | loss: 0.60115 | val_accuracy: 0.70769 |  0:00:20s\n",
      "epoch 10 | loss: 0.57936 | val_accuracy: 0.68718 |  0:00:21s\n",
      "epoch 11 | loss: 0.58927 | val_accuracy: 0.68718 |  0:00:23s\n",
      "epoch 12 | loss: 0.558   | val_accuracy: 0.70769 |  0:00:25s\n",
      "epoch 13 | loss: 0.56991 | val_accuracy: 0.70769 |  0:00:28s\n",
      "epoch 14 | loss: 0.56246 | val_accuracy: 0.70769 |  0:00:30s\n",
      "epoch 15 | loss: 0.54717 | val_accuracy: 0.69231 |  0:00:32s\n",
      "epoch 16 | loss: 0.5657  | val_accuracy: 0.72308 |  0:00:33s\n",
      "epoch 17 | loss: 0.57048 | val_accuracy: 0.69744 |  0:00:34s\n",
      "epoch 18 | loss: 0.55802 | val_accuracy: 0.71282 |  0:00:35s\n",
      "epoch 19 | loss: 0.53583 | val_accuracy: 0.69744 |  0:00:36s\n",
      "epoch 20 | loss: 0.54346 | val_accuracy: 0.71795 |  0:00:37s\n",
      "epoch 21 | loss: 0.51854 | val_accuracy: 0.67692 |  0:00:39s\n",
      "epoch 22 | loss: 0.52626 | val_accuracy: 0.70256 |  0:00:40s\n",
      "epoch 23 | loss: 0.51412 | val_accuracy: 0.72821 |  0:00:41s\n",
      "epoch 24 | loss: 0.50564 | val_accuracy: 0.70769 |  0:00:42s\n",
      "epoch 25 | loss: 0.55439 | val_accuracy: 0.69744 |  0:00:43s\n",
      "epoch 26 | loss: 0.52572 | val_accuracy: 0.68718 |  0:00:44s\n",
      "epoch 27 | loss: 0.5318  | val_accuracy: 0.67179 |  0:00:45s\n",
      "epoch 28 | loss: 0.51573 | val_accuracy: 0.70256 |  0:00:45s\n",
      "epoch 29 | loss: 0.53286 | val_accuracy: 0.72308 |  0:00:46s\n",
      "epoch 30 | loss: 0.54232 | val_accuracy: 0.75385 |  0:00:47s\n",
      "epoch 31 | loss: 0.54035 | val_accuracy: 0.72821 |  0:00:48s\n",
      "epoch 32 | loss: 0.53974 | val_accuracy: 0.76923 |  0:00:49s\n",
      "epoch 33 | loss: 0.50732 | val_accuracy: 0.70769 |  0:00:50s\n",
      "epoch 34 | loss: 0.52145 | val_accuracy: 0.77949 |  0:00:51s\n",
      "epoch 35 | loss: 0.49793 | val_accuracy: 0.72821 |  0:00:52s\n",
      "epoch 36 | loss: 0.52031 | val_accuracy: 0.76923 |  0:00:53s\n",
      "epoch 37 | loss: 0.49557 | val_accuracy: 0.73333 |  0:00:54s\n",
      "epoch 38 | loss: 0.50394 | val_accuracy: 0.73333 |  0:00:55s\n",
      "epoch 39 | loss: 0.49746 | val_accuracy: 0.73846 |  0:00:55s\n",
      "epoch 40 | loss: 0.49052 | val_accuracy: 0.70769 |  0:00:56s\n",
      "epoch 41 | loss: 0.48731 | val_accuracy: 0.71795 |  0:00:57s\n",
      "epoch 42 | loss: 0.47479 | val_accuracy: 0.73333 |  0:00:58s\n",
      "epoch 43 | loss: 0.5271  | val_accuracy: 0.75385 |  0:00:59s\n",
      "epoch 44 | loss: 0.48923 | val_accuracy: 0.74359 |  0:01:00s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_accuracy = 0.77949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 21:53:41,798] Trial 19 finished with value: 0.7794871794871795 and parameters: {'n_d': 33, 'n_a': 8, 'n_steps': 5, 'gamma': 1.108918887339927, 'lambda_sparse': 6.34631345516932e-05, 'learning_rate': 0.0034942897183386835, 'batch_size': 64, 'num_epochs': 63}. Best is trial 19 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.06468 | val_accuracy: 0.6359  |  0:00:00s\n",
      "epoch 1  | loss: 0.70026 | val_accuracy: 0.68205 |  0:00:01s\n",
      "epoch 2  | loss: 0.72526 | val_accuracy: 0.67692 |  0:00:02s\n",
      "epoch 3  | loss: 0.67881 | val_accuracy: 0.69231 |  0:00:03s\n",
      "epoch 4  | loss: 0.6566  | val_accuracy: 0.65128 |  0:00:04s\n",
      "epoch 5  | loss: 0.64935 | val_accuracy: 0.67692 |  0:00:05s\n",
      "epoch 6  | loss: 0.58683 | val_accuracy: 0.65641 |  0:00:05s\n",
      "epoch 7  | loss: 0.58752 | val_accuracy: 0.68205 |  0:00:06s\n",
      "epoch 8  | loss: 0.61817 | val_accuracy: 0.69231 |  0:00:07s\n",
      "epoch 9  | loss: 0.60115 | val_accuracy: 0.70769 |  0:00:08s\n",
      "epoch 10 | loss: 0.57936 | val_accuracy: 0.68718 |  0:00:09s\n",
      "epoch 11 | loss: 0.58927 | val_accuracy: 0.68718 |  0:00:09s\n",
      "epoch 12 | loss: 0.558   | val_accuracy: 0.70769 |  0:00:10s\n",
      "epoch 13 | loss: 0.56991 | val_accuracy: 0.70769 |  0:00:11s\n",
      "epoch 14 | loss: 0.56246 | val_accuracy: 0.70769 |  0:00:12s\n",
      "epoch 15 | loss: 0.54717 | val_accuracy: 0.69231 |  0:00:13s\n",
      "epoch 16 | loss: 0.5657  | val_accuracy: 0.72308 |  0:00:13s\n",
      "epoch 17 | loss: 0.57048 | val_accuracy: 0.69744 |  0:00:14s\n",
      "epoch 18 | loss: 0.55802 | val_accuracy: 0.71282 |  0:00:16s\n",
      "epoch 19 | loss: 0.53583 | val_accuracy: 0.69744 |  0:00:18s\n",
      "epoch 20 | loss: 0.54346 | val_accuracy: 0.71795 |  0:00:19s\n",
      "epoch 21 | loss: 0.51854 | val_accuracy: 0.67692 |  0:00:20s\n",
      "epoch 22 | loss: 0.52626 | val_accuracy: 0.70256 |  0:00:22s\n",
      "epoch 23 | loss: 0.51412 | val_accuracy: 0.72821 |  0:00:23s\n",
      "epoch 24 | loss: 0.50564 | val_accuracy: 0.70769 |  0:00:25s\n",
      "epoch 25 | loss: 0.55439 | val_accuracy: 0.69744 |  0:00:27s\n",
      "epoch 26 | loss: 0.52572 | val_accuracy: 0.68718 |  0:00:29s\n",
      "epoch 27 | loss: 0.5318  | val_accuracy: 0.67179 |  0:00:31s\n",
      "epoch 28 | loss: 0.51573 | val_accuracy: 0.70256 |  0:00:32s\n",
      "epoch 29 | loss: 0.53286 | val_accuracy: 0.72308 |  0:00:33s\n",
      "epoch 30 | loss: 0.54232 | val_accuracy: 0.75385 |  0:00:35s\n",
      "epoch 31 | loss: 0.54035 | val_accuracy: 0.72821 |  0:00:37s\n",
      "epoch 32 | loss: 0.53974 | val_accuracy: 0.76923 |  0:00:39s\n",
      "epoch 33 | loss: 0.50732 | val_accuracy: 0.70769 |  0:00:40s\n",
      "epoch 34 | loss: 0.52145 | val_accuracy: 0.77949 |  0:00:41s\n",
      "epoch 35 | loss: 0.49793 | val_accuracy: 0.72821 |  0:00:43s\n",
      "epoch 36 | loss: 0.52031 | val_accuracy: 0.76923 |  0:00:45s\n",
      "epoch 37 | loss: 0.49557 | val_accuracy: 0.73333 |  0:00:47s\n",
      "epoch 38 | loss: 0.50394 | val_accuracy: 0.73333 |  0:00:49s\n",
      "epoch 39 | loss: 0.49746 | val_accuracy: 0.73846 |  0:00:50s\n",
      "epoch 40 | loss: 0.49052 | val_accuracy: 0.70769 |  0:00:51s\n",
      "epoch 41 | loss: 0.48731 | val_accuracy: 0.71795 |  0:00:53s\n",
      "epoch 42 | loss: 0.47479 | val_accuracy: 0.73333 |  0:00:54s\n",
      "epoch 43 | loss: 0.5271  | val_accuracy: 0.75385 |  0:00:55s\n",
      "epoch 44 | loss: 0.48923 | val_accuracy: 0.74359 |  0:00:56s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_accuracy = 0.77949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "NODE                        0.779487  0.847105                   12.640264   \n",
      "TabNet                      0.779487  0.814349                   56.761422   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.080219   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "NODE                                     296.128333   \n",
      "TabNet                                   607.092467   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "NODE                        {'num_layers': 1, 'num_trees': 8, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 33, 'n_a': 8, 'n_steps': 5, 'gamma': 1...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_d: 33\n",
      "n_a: 8\n",
      "n_steps: 5\n",
      "gamma: 1.108918887339927\n",
      "lambda_sparse: 6.34631345516932e-05\n",
      "learning_rate: 0.0034942897183386835\n",
      "batch_size: 64\n",
      "num_epochs: 63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for TabNet\n",
    "    n_d = trial.suggest_int('n_d', 8, 64)\n",
    "    n_a = trial.suggest_int('n_a', 8, 64)\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the TabNet model\n",
    "    model = TabNetClassifier(\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        device_name=device\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        X_train=X_train_scaled, y_train=y_train.values,\n",
    "        eval_set=[(X_test_scaled, y_test.values)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=10,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=batch_size // 2,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final TabNet model with the best hyperparameters\n",
    "best_model = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "    device_name=device\n",
    ")\n",
    "\n",
    "training_start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train=X_train_scaled, y_train=y_train.values,\n",
    "    eval_set=[(X_test_scaled, y_test.values)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=best_params['num_epochs'],\n",
    "    patience=10,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    virtual_batch_size=best_params['batch_size'] // 2,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_start_time = time.time()\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['TabNet'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:54:38,885] A new study created in memory with name: no-name-d6b10731-f73c-45c5-ad13-a68318d79bfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:41,019] Trial 0 finished with value: 0.7230769230769231 and parameters: {'hidden_dim': 186, 'learning_rate': 0.0001924859163305034, 'batch_size': 128, 'num_epochs': 54}. Best is trial 0 with value: 0.7230769230769231.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:41,940] Trial 1 finished with value: 0.7282051282051282 and parameters: {'hidden_dim': 63, 'learning_rate': 0.0007027299733553533, 'batch_size': 32, 'num_epochs': 19}. Best is trial 1 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:43,477] Trial 2 finished with value: 0.7333333333333333 and parameters: {'hidden_dim': 51, 'learning_rate': 0.07606912126560084, 'batch_size': 256, 'num_epochs': 63}. Best is trial 2 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:44,876] Trial 3 finished with value: 0.7794871794871795 and parameters: {'hidden_dim': 218, 'learning_rate': 0.013114386161707112, 'batch_size': 128, 'num_epochs': 35}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:50,335] Trial 4 finished with value: 0.7743589743589744 and parameters: {'hidden_dim': 223, 'learning_rate': 0.026634880131191866, 'batch_size': 32, 'num_epochs': 99}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:52,672] Trial 5 finished with value: 0.7333333333333333 and parameters: {'hidden_dim': 187, 'learning_rate': 0.027528708566708793, 'batch_size': 128, 'num_epochs': 65}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:55,840] Trial 6 finished with value: 0.7384615384615385 and parameters: {'hidden_dim': 72, 'learning_rate': 0.00019380322303866868, 'batch_size': 64, 'num_epochs': 83}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:58,764] Trial 7 finished with value: 0.7333333333333333 and parameters: {'hidden_dim': 37, 'learning_rate': 0.0006466793167901032, 'batch_size': 32, 'num_epochs': 56}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:54:59,332] Trial 8 finished with value: 0.764102564102564 and parameters: {'hidden_dim': 84, 'learning_rate': 0.014303955483287296, 'batch_size': 128, 'num_epochs': 20}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:02,247] Trial 9 finished with value: 0.764102564102564 and parameters: {'hidden_dim': 136, 'learning_rate': 0.005787443389955483, 'batch_size': 128, 'num_epochs': 83}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:03,185] Trial 10 finished with value: 0.7435897435897436 and parameters: {'hidden_dim': 249, 'learning_rate': 0.0027355940917073215, 'batch_size': 256, 'num_epochs': 36}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:08,624] Trial 11 finished with value: 0.7076923076923077 and parameters: {'hidden_dim': 255, 'learning_rate': 0.0642104909348104, 'batch_size': 32, 'num_epochs': 99}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:10,543] Trial 12 finished with value: 0.7743589743589744 and parameters: {'hidden_dim': 221, 'learning_rate': 0.01093626492006712, 'batch_size': 64, 'num_epochs': 37}. Best is trial 3 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:12,809] Trial 13 finished with value: 0.7948717948717948 and parameters: {'hidden_dim': 195, 'learning_rate': 0.003086615507422138, 'batch_size': 32, 'num_epochs': 39}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:15,145] Trial 14 finished with value: 0.7794871794871795 and parameters: {'hidden_dim': 158, 'learning_rate': 0.0020769244439194246, 'batch_size': 32, 'num_epochs': 35}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:16,729] Trial 15 finished with value: 0.7538461538461538 and parameters: {'hidden_dim': 120, 'learning_rate': 0.0011132657005950592, 'batch_size': 128, 'num_epochs': 45}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:17,520] Trial 16 finished with value: 0.7333333333333333 and parameters: {'hidden_dim': 191, 'learning_rate': 0.005494441691893018, 'batch_size': 256, 'num_epochs': 26}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:18,070] Trial 17 finished with value: 0.7743589743589744 and parameters: {'hidden_dim': 219, 'learning_rate': 0.00672656991223021, 'batch_size': 64, 'num_epochs': 10}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:20,334] Trial 18 finished with value: 0.7692307692307693 and parameters: {'hidden_dim': 164, 'learning_rate': 0.0014880517333288813, 'batch_size': 32, 'num_epochs': 44}. Best is trial 13 with value: 0.7948717948717948.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\2557614817.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 21:55:21,255] Trial 19 finished with value: 0.7435897435897436 and parameters: {'hidden_dim': 109, 'learning_rate': 0.030794317945776934, 'batch_size': 128, 'num_epochs': 27}. Best is trial 13 with value: 0.7948717948717948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "NODE                        0.779487  0.847105                   12.640264   \n",
      "TabNet                      0.779487  0.814349                   56.761422   \n",
      "KAN                         0.789744  0.846154                    2.070088   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.080219   \n",
      "KAN                                                 0.0   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "NODE                                     296.128333   \n",
      "TabNet                                   607.092467   \n",
      "KAN                                        44.48086   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "NODE                        {'num_layers': 1, 'num_trees': 8, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 33, 'n_a': 8, 'n_steps': 5, 'gamma': 1...  \n",
      "KAN                         {'hidden_dim': 195, 'learning_rate': 0.0030866...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim: 195\n",
      "learning_rate: 0.003086615507422138\n",
      "batch_size: 32\n",
      "num_epochs: 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(KAN, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden_layer(x))\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for KAN\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 32, 256)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the KAN model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = KAN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final KAN model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = KAN(input_dim, \n",
    "                 best_params['hidden_dim'], \n",
    "                 output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['KAN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 21:55:23,406] A new study created in memory with name: no-name-ef99e6f9-4a27-4d29-b141-0008901e9a55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:55:28,225] Trial 0 finished with value: 0.4666666666666667 and parameters: {'heads': 7, 'dim': 245, 'depth': 2, 'mlp_dim': 66, 'dropout': 0.38122637242381197, 'learning_rate': 0.03438537350792628, 'batch_size': 128, 'num_epochs': 10}. Best is trial 0 with value: 0.4666666666666667.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:56:07,752] Trial 1 finished with value: 0.5897435897435898 and parameters: {'heads': 5, 'dim': 255, 'depth': 6, 'mlp_dim': 52, 'dropout': 0.27613722450196104, 'learning_rate': 0.001214249767907352, 'batch_size': 256, 'num_epochs': 66}. Best is trial 1 with value: 0.5897435897435898.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:56:56,549] Trial 2 finished with value: 0.7282051282051282 and parameters: {'heads': 7, 'dim': 91, 'depth': 6, 'mlp_dim': 140, 'dropout': 0.3209336072844951, 'learning_rate': 0.0017045722245191241, 'batch_size': 32, 'num_epochs': 47}. Best is trial 2 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:57:33,679] Trial 3 finished with value: 0.6410256410256411 and parameters: {'heads': 1, 'dim': 245, 'depth': 4, 'mlp_dim': 38, 'dropout': 0.273156367991753, 'learning_rate': 0.0009428079062719181, 'batch_size': 256, 'num_epochs': 100}. Best is trial 2 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:57:38,719] Trial 4 finished with value: 0.7435897435897436 and parameters: {'heads': 4, 'dim': 160, 'depth': 2, 'mlp_dim': 230, 'dropout': 0.1674448775704599, 'learning_rate': 0.0012690214951424372, 'batch_size': 256, 'num_epochs': 24}. Best is trial 4 with value: 0.7435897435897436.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:57:50,187] Trial 5 finished with value: 0.6974358974358974 and parameters: {'heads': 5, 'dim': 60, 'depth': 6, 'mlp_dim': 150, 'dropout': 0.3722911636547843, 'learning_rate': 0.00454899960678212, 'batch_size': 256, 'num_epochs': 18}. Best is trial 4 with value: 0.7435897435897436.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:58:35,337] Trial 6 finished with value: 0.764102564102564 and parameters: {'heads': 5, 'dim': 65, 'depth': 5, 'mlp_dim': 201, 'dropout': 0.02436514538504686, 'learning_rate': 0.0007645762402609027, 'batch_size': 64, 'num_epochs': 83}. Best is trial 6 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:58:39,682] Trial 7 finished with value: 0.6820512820512821 and parameters: {'heads': 2, 'dim': 72, 'depth': 1, 'mlp_dim': 117, 'dropout': 0.46761950739709873, 'learning_rate': 0.0051183299694754015, 'batch_size': 256, 'num_epochs': 67}. Best is trial 6 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:58:51,950] Trial 8 finished with value: 0.7384615384615385 and parameters: {'heads': 1, 'dim': 148, 'depth': 1, 'mlp_dim': 169, 'dropout': 0.3898326822951089, 'learning_rate': 0.00013255429013876873, 'batch_size': 32, 'num_epochs': 44}. Best is trial 6 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:59:04,208] Trial 9 finished with value: 0.7538461538461538 and parameters: {'heads': 6, 'dim': 60, 'depth': 1, 'mlp_dim': 148, 'dropout': 0.3073131751365997, 'learning_rate': 0.00013952323406799666, 'batch_size': 32, 'num_epochs': 55}. Best is trial 6 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 21:59:41,087] Trial 10 finished with value: 0.5333333333333333 and parameters: {'heads': 3, 'dim': 33, 'depth': 4, 'mlp_dim': 225, 'dropout': 0.0023336648169608454, 'learning_rate': 0.06330970134289203, 'batch_size': 64, 'num_epochs': 97}. Best is trial 6 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:00:17,440] Trial 11 finished with value: 0.7435897435897436 and parameters: {'heads': 6, 'dim': 18, 'depth': 5, 'mlp_dim': 192, 'dropout': 0.15900604875306393, 'learning_rate': 0.00011044380270562376, 'batch_size': 64, 'num_epochs': 79}. Best is trial 6 with value: 0.764102564102564.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:01:05,110] Trial 12 finished with value: 0.6974358974358974 and parameters: {'heads': 6, 'dim': 114, 'depth': 3, 'mlp_dim': 99, 'dropout': 0.016701774209600673, 'learning_rate': 0.00032292797013119953, 'batch_size': 32, 'num_epochs': 81}. Best is trial 6 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:01:22,909] Trial 13 finished with value: 0.7230769230769231 and parameters: {'heads': 8, 'dim': 8, 'depth': 5, 'mlp_dim': 188, 'dropout': 0.16180845856741088, 'learning_rate': 0.00038107122629889626, 'batch_size': 64, 'num_epochs': 36}. Best is trial 6 with value: 0.764102564102564.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:01:43,077] Trial 14 finished with value: 0.7692307692307693 and parameters: {'heads': 4, 'dim': 192, 'depth': 3, 'mlp_dim': 206, 'dropout': 0.11200006855545801, 'learning_rate': 0.00037710393194967005, 'batch_size': 128, 'num_epochs': 61}. Best is trial 14 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:02:17,943] Trial 15 finished with value: 0.5333333333333333 and parameters: {'heads': 4, 'dim': 196, 'depth': 3, 'mlp_dim': 256, 'dropout': 0.07857934124966146, 'learning_rate': 0.010084090529706956, 'batch_size': 128, 'num_epochs': 86}. Best is trial 14 with value: 0.7692307692307693.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:02:53,660] Trial 16 finished with value: 0.7435897435897436 and parameters: {'heads': 3, 'dim': 189, 'depth': 5, 'mlp_dim': 204, 'dropout': 0.09583860629547589, 'learning_rate': 0.0005576111377852556, 'batch_size': 128, 'num_epochs': 64}. Best is trial 14 with value: 0.7692307692307693.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:03:46,920] Trial 17 finished with value: 0.7794871794871795 and parameters: {'heads': 3, 'dim': 198, 'depth': 4, 'mlp_dim': 251, 'dropout': 0.07214721908172474, 'learning_rate': 0.00028514321652179313, 'batch_size': 64, 'num_epochs': 90}. Best is trial 17 with value: 0.7794871794871795.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:04:14,335] Trial 18 finished with value: 0.7435897435897436 and parameters: {'heads': 3, 'dim': 213, 'depth': 3, 'mlp_dim': 255, 'dropout': 0.21591831782532284, 'learning_rate': 0.00039120955592932227, 'batch_size': 128, 'num_epochs': 74}. Best is trial 17 with value: 0.7794871794871795.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\168735295.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 22:05:09,437] Trial 19 finished with value: 0.7846153846153846 and parameters: {'heads': 2, 'dim': 168, 'depth': 4, 'mlp_dim': 229, 'dropout': 0.07894633591246165, 'learning_rate': 0.0002299258147500175, 'batch_size': 64, 'num_epochs': 92}. Best is trial 19 with value: 0.7846153846153846.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "NODE                        0.779487  0.847105                   12.640264   \n",
      "TabNet                      0.779487  0.814349                   56.761422   \n",
      "KAN                         0.789744  0.846154                    2.070088   \n",
      "SAINT                       0.733333  0.807904                   52.492634   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.080219   \n",
      "KAN                                                 0.0   \n",
      "SAINT                                          0.012965   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "NODE                                     296.128333   \n",
      "TabNet                                   607.092467   \n",
      "KAN                                        44.48086   \n",
      "SAINT                                    638.582521   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "NODE                        {'num_layers': 1, 'num_trees': 8, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 33, 'n_a': 8, 'n_steps': 5, 'gamma': 1...  \n",
      "KAN                         {'hidden_dim': 195, 'learning_rate': 0.0030866...  \n",
      "SAINT                       {'heads': 2, 'dim': 168, 'depth': 4, 'mlp_dim'...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "heads: 2\n",
      "dim: 168\n",
      "depth: 4\n",
      "mlp_dim: 229\n",
      "dropout: 0.07894633591246165\n",
      "learning_rate: 0.0002299258147500175\n",
      "batch_size: 64\n",
      "num_epochs: 92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.embeds = nn.Linear(input_dim, dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeds(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for SAINT\n",
    "    heads = trial.suggest_int('heads', 1, 8)\n",
    "    dim = trial.suggest_int('dim', heads, 256, step=heads)  # Ensure dim is divisible by heads\n",
    "    depth = trial.suggest_int('depth', 1, 6)\n",
    "    mlp_dim = trial.suggest_int('mlp_dim', 32, 256)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the SAINT model\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = SAINT(input_dim, num_classes, dim, depth, heads, mlp_dim, dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final SAINT model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "best_model = SAINT(input_dim, num_classes, \n",
    "                   best_params['dim'], \n",
    "                   best_params['depth'], \n",
    "                   best_params['heads'], \n",
    "                   best_params['mlp_dim'], \n",
    "                   best_params['dropout']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['SAINT'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 22:06:02,049] A new study created in memory with name: no-name-4e434e35-c773-4e31-bf25-2bb6ea7b287c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.88328 | val_accuracy: 0.51795 |  0:00:01s\n",
      "epoch 1  | loss: 1.15528 | val_accuracy: 0.6     |  0:00:01s\n",
      "epoch 2  | loss: 1.03478 | val_accuracy: 0.5641  |  0:00:03s\n",
      "epoch 3  | loss: 0.94845 | val_accuracy: 0.60513 |  0:00:04s\n",
      "epoch 4  | loss: 0.82715 | val_accuracy: 0.61538 |  0:00:05s\n",
      "epoch 5  | loss: 0.62127 | val_accuracy: 0.62564 |  0:00:05s\n",
      "epoch 6  | loss: 0.65437 | val_accuracy: 0.60513 |  0:00:06s\n",
      "epoch 7  | loss: 0.64205 | val_accuracy: 0.68718 |  0:00:07s\n",
      "epoch 8  | loss: 0.67313 | val_accuracy: 0.59487 |  0:00:08s\n",
      "epoch 9  | loss: 0.70581 | val_accuracy: 0.64615 |  0:00:09s\n",
      "epoch 10 | loss: 0.74422 | val_accuracy: 0.64615 |  0:00:10s\n",
      "epoch 11 | loss: 0.60619 | val_accuracy: 0.66667 |  0:00:11s\n",
      "epoch 12 | loss: 0.58805 | val_accuracy: 0.61538 |  0:00:13s\n",
      "epoch 13 | loss: 0.58355 | val_accuracy: 0.65128 |  0:00:14s\n",
      "epoch 14 | loss: 0.57404 | val_accuracy: 0.66154 |  0:00:15s\n",
      "epoch 15 | loss: 0.58828 | val_accuracy: 0.68205 |  0:00:16s\n",
      "epoch 16 | loss: 0.60357 | val_accuracy: 0.62564 |  0:00:17s\n",
      "epoch 17 | loss: 0.60701 | val_accuracy: 0.68718 |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.68718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:06:21,717] Trial 0 finished with value: 0.6871794871794872 and parameters: {'n_d': 25, 'n_a': 55, 'n_steps': 9, 'gamma': 1.9612932175244775, 'lambda_sparse': 1.5699959648691783e-05, 'learning_rate': 0.03553586705153356, 'batch_size': 128, 'num_epochs': 34}. Best is trial 0 with value: 0.6871794871794872.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.47676 | val_accuracy: 0.50769 |  0:00:00s\n",
      "epoch 1  | loss: 0.86558 | val_accuracy: 0.55385 |  0:00:00s\n",
      "epoch 2  | loss: 0.71355 | val_accuracy: 0.59487 |  0:00:01s\n",
      "epoch 3  | loss: 0.7631  | val_accuracy: 0.60513 |  0:00:01s\n",
      "epoch 4  | loss: 0.80486 | val_accuracy: 0.6359  |  0:00:02s\n",
      "epoch 5  | loss: 0.63896 | val_accuracy: 0.60513 |  0:00:02s\n",
      "epoch 6  | loss: 0.63563 | val_accuracy: 0.66667 |  0:00:03s\n",
      "epoch 7  | loss: 0.58952 | val_accuracy: 0.68718 |  0:00:03s\n",
      "epoch 8  | loss: 0.56929 | val_accuracy: 0.65128 |  0:00:04s\n",
      "epoch 9  | loss: 0.60126 | val_accuracy: 0.67692 |  0:00:04s\n",
      "epoch 10 | loss: 0.58348 | val_accuracy: 0.69231 |  0:00:05s\n",
      "epoch 11 | loss: 0.61497 | val_accuracy: 0.6359  |  0:00:05s\n",
      "epoch 12 | loss: 0.62353 | val_accuracy: 0.67179 |  0:00:05s\n",
      "epoch 13 | loss: 0.54518 | val_accuracy: 0.65128 |  0:00:06s\n",
      "epoch 14 | loss: 0.58905 | val_accuracy: 0.66667 |  0:00:06s\n",
      "epoch 15 | loss: 0.54842 | val_accuracy: 0.65128 |  0:00:07s\n",
      "epoch 16 | loss: 0.59893 | val_accuracy: 0.64615 |  0:00:07s\n",
      "epoch 17 | loss: 0.54404 | val_accuracy: 0.64615 |  0:00:08s\n",
      "epoch 18 | loss: 0.52583 | val_accuracy: 0.67179 |  0:00:08s\n",
      "epoch 19 | loss: 0.55132 | val_accuracy: 0.68205 |  0:00:09s\n",
      "epoch 20 | loss: 0.53135 | val_accuracy: 0.65641 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.69231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:06:31,568] Trial 1 finished with value: 0.6923076923076923 and parameters: {'n_d': 62, 'n_a': 49, 'n_steps': 5, 'gamma': 1.654980627930387, 'lambda_sparse': 2.4182017114232987e-06, 'learning_rate': 0.002657059212656804, 'batch_size': 256, 'num_epochs': 52}. Best is trial 1 with value: 0.6923076923076923.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.70057 | val_accuracy: 0.56923 |  0:00:00s\n",
      "epoch 1  | loss: 1.40543 | val_accuracy: 0.61538 |  0:00:00s\n",
      "epoch 2  | loss: 1.19291 | val_accuracy: 0.58974 |  0:00:01s\n",
      "epoch 3  | loss: 0.76889 | val_accuracy: 0.58462 |  0:00:01s\n",
      "epoch 4  | loss: 0.63891 | val_accuracy: 0.58462 |  0:00:02s\n",
      "epoch 5  | loss: 0.58485 | val_accuracy: 0.61538 |  0:00:02s\n",
      "epoch 6  | loss: 0.57306 | val_accuracy: 0.66667 |  0:00:03s\n",
      "epoch 7  | loss: 0.56439 | val_accuracy: 0.6     |  0:00:03s\n",
      "epoch 8  | loss: 0.54318 | val_accuracy: 0.67179 |  0:00:04s\n",
      "epoch 9  | loss: 0.54122 | val_accuracy: 0.64615 |  0:00:04s\n",
      "epoch 10 | loss: 0.55174 | val_accuracy: 0.6     |  0:00:05s\n",
      "epoch 11 | loss: 0.61805 | val_accuracy: 0.57436 |  0:00:05s\n",
      "epoch 12 | loss: 0.60409 | val_accuracy: 0.60513 |  0:00:06s\n",
      "epoch 13 | loss: 0.60081 | val_accuracy: 0.63077 |  0:00:06s\n",
      "epoch 14 | loss: 0.55015 | val_accuracy: 0.6     |  0:00:07s\n",
      "epoch 15 | loss: 0.56708 | val_accuracy: 0.65128 |  0:00:07s\n",
      "epoch 16 | loss: 0.54928 | val_accuracy: 0.65641 |  0:00:08s\n",
      "epoch 17 | loss: 0.54704 | val_accuracy: 0.61026 |  0:00:08s\n",
      "epoch 18 | loss: 0.62563 | val_accuracy: 0.66154 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_accuracy = 0.67179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:06:40,767] Trial 2 finished with value: 0.6717948717948717 and parameters: {'n_d': 63, 'n_a': 43, 'n_steps': 5, 'gamma': 1.223088639494534, 'lambda_sparse': 1.2437835631239294e-06, 'learning_rate': 0.06787040404699463, 'batch_size': 256, 'num_epochs': 39}. Best is trial 1 with value: 0.6923076923076923.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08794 | val_accuracy: 0.48718 |  0:00:01s\n",
      "epoch 1  | loss: 1.04908 | val_accuracy: 0.47179 |  0:00:03s\n",
      "epoch 2  | loss: 0.89947 | val_accuracy: 0.47692 |  0:00:04s\n",
      "epoch 3  | loss: 0.90516 | val_accuracy: 0.49744 |  0:00:06s\n",
      "epoch 4  | loss: 0.88597 | val_accuracy: 0.52821 |  0:00:08s\n",
      "epoch 5  | loss: 0.82173 | val_accuracy: 0.49231 |  0:00:09s\n",
      "epoch 6  | loss: 0.80319 | val_accuracy: 0.52821 |  0:00:10s\n",
      "epoch 7  | loss: 0.81732 | val_accuracy: 0.51795 |  0:00:12s\n",
      "epoch 8  | loss: 0.75448 | val_accuracy: 0.53846 |  0:00:13s\n",
      "epoch 9  | loss: 0.77094 | val_accuracy: 0.57949 |  0:00:15s\n",
      "epoch 10 | loss: 0.75104 | val_accuracy: 0.62564 |  0:00:16s\n",
      "epoch 11 | loss: 0.78321 | val_accuracy: 0.62051 |  0:00:18s\n",
      "epoch 12 | loss: 0.75977 | val_accuracy: 0.58462 |  0:00:19s\n",
      "epoch 13 | loss: 0.74527 | val_accuracy: 0.60513 |  0:00:21s\n",
      "epoch 14 | loss: 0.72776 | val_accuracy: 0.55897 |  0:00:22s\n",
      "epoch 15 | loss: 0.74338 | val_accuracy: 0.53846 |  0:00:23s\n",
      "epoch 16 | loss: 0.74885 | val_accuracy: 0.58462 |  0:00:25s\n",
      "epoch 17 | loss: 0.71208 | val_accuracy: 0.57436 |  0:00:26s\n",
      "epoch 18 | loss: 0.72978 | val_accuracy: 0.57436 |  0:00:28s\n",
      "epoch 19 | loss: 0.67967 | val_accuracy: 0.59487 |  0:00:29s\n",
      "epoch 20 | loss: 0.70358 | val_accuracy: 0.62051 |  0:00:31s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.62564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:07:12,619] Trial 3 finished with value: 0.6256410256410256 and parameters: {'n_d': 8, 'n_a': 22, 'n_steps': 6, 'gamma': 1.836387208302698, 'lambda_sparse': 0.00027972725077287237, 'learning_rate': 0.000722995582126127, 'batch_size': 32, 'num_epochs': 98}. Best is trial 1 with value: 0.6923076923076923.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08711 | val_accuracy: 0.52821 |  0:00:00s\n",
      "epoch 1  | loss: 0.95701 | val_accuracy: 0.58462 |  0:00:00s\n",
      "epoch 2  | loss: 0.82448 | val_accuracy: 0.66154 |  0:00:01s\n",
      "epoch 3  | loss: 0.80476 | val_accuracy: 0.67692 |  0:00:01s\n",
      "epoch 4  | loss: 0.762   | val_accuracy: 0.67179 |  0:00:02s\n",
      "epoch 5  | loss: 0.77394 | val_accuracy: 0.69231 |  0:00:02s\n",
      "epoch 6  | loss: 0.7314  | val_accuracy: 0.68205 |  0:00:03s\n",
      "epoch 7  | loss: 0.70217 | val_accuracy: 0.68205 |  0:00:03s\n",
      "epoch 8  | loss: 0.68799 | val_accuracy: 0.67692 |  0:00:04s\n",
      "epoch 9  | loss: 0.70371 | val_accuracy: 0.69231 |  0:00:04s\n",
      "epoch 10 | loss: 0.63584 | val_accuracy: 0.69231 |  0:00:04s\n",
      "epoch 11 | loss: 0.62409 | val_accuracy: 0.71282 |  0:00:05s\n",
      "epoch 12 | loss: 0.66376 | val_accuracy: 0.69231 |  0:00:05s\n",
      "epoch 13 | loss: 0.62097 | val_accuracy: 0.70256 |  0:00:06s\n",
      "epoch 14 | loss: 0.62068 | val_accuracy: 0.70769 |  0:00:06s\n",
      "epoch 15 | loss: 0.62996 | val_accuracy: 0.71282 |  0:00:06s\n",
      "epoch 16 | loss: 0.59327 | val_accuracy: 0.69744 |  0:00:07s\n",
      "epoch 17 | loss: 0.58584 | val_accuracy: 0.71795 |  0:00:07s\n",
      "epoch 18 | loss: 0.60661 | val_accuracy: 0.71282 |  0:00:08s\n",
      "epoch 19 | loss: 0.58391 | val_accuracy: 0.72821 |  0:00:08s\n",
      "epoch 20 | loss: 0.56518 | val_accuracy: 0.71795 |  0:00:08s\n",
      "epoch 21 | loss: 0.5662  | val_accuracy: 0.71282 |  0:00:09s\n",
      "epoch 22 | loss: 0.59099 | val_accuracy: 0.71282 |  0:00:09s\n",
      "epoch 23 | loss: 0.6002  | val_accuracy: 0.72308 |  0:00:10s\n",
      "epoch 24 | loss: 0.57606 | val_accuracy: 0.72821 |  0:00:10s\n",
      "epoch 25 | loss: 0.5682  | val_accuracy: 0.71795 |  0:00:11s\n",
      "epoch 26 | loss: 0.5594  | val_accuracy: 0.71282 |  0:00:11s\n",
      "epoch 27 | loss: 0.56501 | val_accuracy: 0.72308 |  0:00:11s\n",
      "epoch 28 | loss: 0.54402 | val_accuracy: 0.72821 |  0:00:12s\n",
      "Stop training because you reached max_epochs = 29 with best_epoch = 19 and best_val_accuracy = 0.72821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:07:25,244] Trial 4 finished with value: 0.7282051282051282 and parameters: {'n_d': 58, 'n_a': 23, 'n_steps': 3, 'gamma': 1.4949923485646366, 'lambda_sparse': 0.00022078454573570624, 'learning_rate': 0.0004491821337873118, 'batch_size': 128, 'num_epochs': 29}. Best is trial 4 with value: 0.7282051282051282.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.32642 | val_accuracy: 0.64615 |  0:00:01s\n",
      "epoch 1  | loss: 0.70338 | val_accuracy: 0.61026 |  0:00:03s\n",
      "epoch 2  | loss: 0.61983 | val_accuracy: 0.66667 |  0:00:05s\n",
      "epoch 3  | loss: 0.58908 | val_accuracy: 0.6359  |  0:00:07s\n",
      "epoch 4  | loss: 0.60174 | val_accuracy: 0.67692 |  0:00:09s\n",
      "epoch 5  | loss: 0.63881 | val_accuracy: 0.70256 |  0:00:11s\n",
      "epoch 6  | loss: 0.57826 | val_accuracy: 0.68718 |  0:00:14s\n",
      "epoch 7  | loss: 0.57884 | val_accuracy: 0.71282 |  0:00:16s\n",
      "epoch 8  | loss: 0.56155 | val_accuracy: 0.68718 |  0:00:18s\n",
      "epoch 9  | loss: 0.55539 | val_accuracy: 0.65641 |  0:00:20s\n",
      "epoch 10 | loss: 0.57132 | val_accuracy: 0.70256 |  0:00:23s\n",
      "epoch 11 | loss: 0.55171 | val_accuracy: 0.71282 |  0:00:24s\n",
      "epoch 12 | loss: 0.56378 | val_accuracy: 0.71795 |  0:00:26s\n",
      "epoch 13 | loss: 0.56245 | val_accuracy: 0.72308 |  0:00:28s\n",
      "epoch 14 | loss: 0.55521 | val_accuracy: 0.73333 |  0:00:30s\n",
      "epoch 15 | loss: 0.54629 | val_accuracy: 0.71795 |  0:00:31s\n",
      "epoch 16 | loss: 0.5386  | val_accuracy: 0.67179 |  0:00:33s\n",
      "epoch 17 | loss: 0.57097 | val_accuracy: 0.67692 |  0:00:35s\n",
      "epoch 18 | loss: 0.59817 | val_accuracy: 0.67692 |  0:00:38s\n",
      "epoch 19 | loss: 0.56042 | val_accuracy: 0.62564 |  0:00:40s\n",
      "epoch 20 | loss: 0.55914 | val_accuracy: 0.69744 |  0:00:42s\n",
      "epoch 21 | loss: 0.5555  | val_accuracy: 0.70256 |  0:00:43s\n",
      "epoch 22 | loss: 0.55099 | val_accuracy: 0.71795 |  0:00:45s\n",
      "epoch 23 | loss: 0.5504  | val_accuracy: 0.71282 |  0:00:47s\n",
      "epoch 24 | loss: 0.5545  | val_accuracy: 0.70769 |  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:08:15,496] Trial 5 finished with value: 0.7333333333333333 and parameters: {'n_d': 48, 'n_a': 50, 'n_steps': 5, 'gamma': 1.4128893552268842, 'lambda_sparse': 5.779193004449135e-05, 'learning_rate': 0.05854127294781648, 'batch_size': 32, 'num_epochs': 85}. Best is trial 5 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.07406 | val_accuracy: 0.57949 |  0:00:01s\n",
      "epoch 1  | loss: 1.84265 | val_accuracy: 0.55897 |  0:00:03s\n",
      "epoch 2  | loss: 1.77555 | val_accuracy: 0.54359 |  0:00:05s\n",
      "epoch 3  | loss: 1.64133 | val_accuracy: 0.52821 |  0:00:07s\n",
      "epoch 4  | loss: 1.48044 | val_accuracy: 0.55897 |  0:00:09s\n",
      "epoch 5  | loss: 1.30863 | val_accuracy: 0.54359 |  0:00:12s\n",
      "epoch 6  | loss: 1.27771 | val_accuracy: 0.5641  |  0:00:14s\n",
      "epoch 7  | loss: 1.24047 | val_accuracy: 0.57436 |  0:00:16s\n",
      "epoch 8  | loss: 1.11613 | val_accuracy: 0.57949 |  0:00:18s\n",
      "epoch 9  | loss: 1.10209 | val_accuracy: 0.5641  |  0:00:20s\n",
      "epoch 10 | loss: 1.02838 | val_accuracy: 0.56923 |  0:00:22s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.57949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:08:39,255] Trial 6 finished with value: 0.5794871794871795 and parameters: {'n_d': 54, 'n_a': 42, 'n_steps': 6, 'gamma': 1.3347420752920536, 'lambda_sparse': 4.536029794521018e-06, 'learning_rate': 0.00011509941764771661, 'batch_size': 32, 'num_epochs': 92}. Best is trial 5 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.74543 | val_accuracy: 0.51795 |  0:00:02s\n",
      "epoch 1  | loss: 1.54756 | val_accuracy: 0.50769 |  0:00:05s\n",
      "epoch 2  | loss: 1.60461 | val_accuracy: 0.53846 |  0:00:08s\n",
      "epoch 3  | loss: 1.39158 | val_accuracy: 0.54359 |  0:00:10s\n",
      "epoch 4  | loss: 1.38244 | val_accuracy: 0.54872 |  0:00:13s\n",
      "epoch 5  | loss: 1.33098 | val_accuracy: 0.5641  |  0:00:17s\n",
      "epoch 6  | loss: 1.42078 | val_accuracy: 0.53846 |  0:00:21s\n",
      "epoch 7  | loss: 1.27209 | val_accuracy: 0.5641  |  0:00:23s\n",
      "epoch 8  | loss: 1.21233 | val_accuracy: 0.53333 |  0:00:26s\n",
      "epoch 9  | loss: 1.26424 | val_accuracy: 0.5641  |  0:00:29s\n",
      "epoch 10 | loss: 1.18271 | val_accuracy: 0.57436 |  0:00:32s\n",
      "epoch 11 | loss: 1.14847 | val_accuracy: 0.54872 |  0:00:34s\n",
      "epoch 12 | loss: 1.12524 | val_accuracy: 0.52308 |  0:00:37s\n",
      "epoch 13 | loss: 1.06377 | val_accuracy: 0.51282 |  0:00:41s\n",
      "epoch 14 | loss: 1.12152 | val_accuracy: 0.55897 |  0:00:44s\n",
      "epoch 15 | loss: 1.1613  | val_accuracy: 0.54872 |  0:00:47s\n",
      "epoch 16 | loss: 1.1666  | val_accuracy: 0.57436 |  0:00:50s\n",
      "epoch 17 | loss: 1.12573 | val_accuracy: 0.58974 |  0:00:53s\n",
      "epoch 18 | loss: 1.06035 | val_accuracy: 0.57949 |  0:00:55s\n",
      "epoch 19 | loss: 0.96991 | val_accuracy: 0.55897 |  0:00:58s\n",
      "epoch 20 | loss: 1.05616 | val_accuracy: 0.57436 |  0:01:01s\n",
      "epoch 21 | loss: 1.06919 | val_accuracy: 0.54359 |  0:01:03s\n",
      "epoch 22 | loss: 0.97368 | val_accuracy: 0.5641  |  0:01:06s\n",
      "epoch 23 | loss: 0.98202 | val_accuracy: 0.55385 |  0:01:09s\n",
      "epoch 24 | loss: 1.02493 | val_accuracy: 0.62564 |  0:01:12s\n",
      "epoch 25 | loss: 1.08683 | val_accuracy: 0.65641 |  0:01:15s\n",
      "epoch 26 | loss: 0.96561 | val_accuracy: 0.58974 |  0:01:18s\n",
      "Stop training because you reached max_epochs = 27 with best_epoch = 25 and best_val_accuracy = 0.65641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:09:58,687] Trial 7 finished with value: 0.6564102564102564 and parameters: {'n_d': 31, 'n_a': 47, 'n_steps': 9, 'gamma': 1.7107384275359525, 'lambda_sparse': 2.19497404424172e-06, 'learning_rate': 0.00011607507600617685, 'batch_size': 32, 'num_epochs': 27}. Best is trial 5 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.59291 | val_accuracy: 0.46667 |  0:00:02s\n",
      "epoch 1  | loss: 1.39128 | val_accuracy: 0.44615 |  0:00:04s\n",
      "epoch 2  | loss: 1.09584 | val_accuracy: 0.50256 |  0:00:07s\n",
      "epoch 3  | loss: 1.12521 | val_accuracy: 0.50769 |  0:00:09s\n",
      "epoch 4  | loss: 0.97416 | val_accuracy: 0.57949 |  0:00:11s\n",
      "epoch 5  | loss: 1.00793 | val_accuracy: 0.55385 |  0:00:14s\n",
      "epoch 6  | loss: 0.83649 | val_accuracy: 0.56923 |  0:00:16s\n",
      "epoch 7  | loss: 0.84505 | val_accuracy: 0.57436 |  0:00:18s\n",
      "epoch 8  | loss: 0.85968 | val_accuracy: 0.58462 |  0:00:21s\n",
      "epoch 9  | loss: 0.7496  | val_accuracy: 0.59487 |  0:00:24s\n",
      "epoch 10 | loss: 0.73191 | val_accuracy: 0.61026 |  0:00:26s\n",
      "epoch 11 | loss: 0.74879 | val_accuracy: 0.60513 |  0:00:29s\n",
      "epoch 12 | loss: 0.76167 | val_accuracy: 0.62564 |  0:00:32s\n",
      "epoch 13 | loss: 0.73047 | val_accuracy: 0.61026 |  0:00:35s\n",
      "epoch 14 | loss: 0.7209  | val_accuracy: 0.64103 |  0:00:37s\n",
      "epoch 15 | loss: 0.72402 | val_accuracy: 0.67179 |  0:00:40s\n",
      "epoch 16 | loss: 0.75449 | val_accuracy: 0.66154 |  0:00:43s\n",
      "epoch 17 | loss: 0.71701 | val_accuracy: 0.69231 |  0:00:45s\n",
      "epoch 18 | loss: 0.67838 | val_accuracy: 0.65641 |  0:00:48s\n",
      "epoch 19 | loss: 0.73991 | val_accuracy: 0.65128 |  0:00:51s\n",
      "epoch 20 | loss: 0.65735 | val_accuracy: 0.68718 |  0:00:54s\n",
      "epoch 21 | loss: 0.72968 | val_accuracy: 0.70256 |  0:00:57s\n",
      "epoch 22 | loss: 0.74775 | val_accuracy: 0.67692 |  0:00:59s\n",
      "epoch 23 | loss: 0.68566 | val_accuracy: 0.68205 |  0:01:02s\n",
      "epoch 24 | loss: 0.77381 | val_accuracy: 0.70256 |  0:01:05s\n",
      "epoch 25 | loss: 0.71118 | val_accuracy: 0.64615 |  0:01:07s\n",
      "epoch 26 | loss: 0.73596 | val_accuracy: 0.67179 |  0:01:10s\n",
      "epoch 27 | loss: 0.72557 | val_accuracy: 0.62564 |  0:01:13s\n",
      "epoch 28 | loss: 0.65924 | val_accuracy: 0.65128 |  0:01:16s\n",
      "epoch 29 | loss: 0.69867 | val_accuracy: 0.65641 |  0:01:19s\n",
      "epoch 30 | loss: 0.66938 | val_accuracy: 0.67179 |  0:01:21s\n",
      "epoch 31 | loss: 0.6958  | val_accuracy: 0.66667 |  0:01:24s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_accuracy = 0.70256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:11:24,245] Trial 8 finished with value: 0.7025641025641025 and parameters: {'n_d': 46, 'n_a': 52, 'n_steps': 7, 'gamma': 1.504904713892269, 'lambda_sparse': 1.936495763845598e-05, 'learning_rate': 0.0007392877341858976, 'batch_size': 32, 'num_epochs': 78}. Best is trial 5 with value: 0.7333333333333333.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.83577 | val_accuracy: 0.59487 |  0:00:00s\n",
      "epoch 1  | loss: 0.66056 | val_accuracy: 0.64615 |  0:00:00s\n",
      "epoch 2  | loss: 0.63301 | val_accuracy: 0.64615 |  0:00:00s\n",
      "epoch 3  | loss: 0.58083 | val_accuracy: 0.72308 |  0:00:01s\n",
      "epoch 4  | loss: 0.59479 | val_accuracy: 0.73333 |  0:00:01s\n",
      "epoch 5  | loss: 0.54701 | val_accuracy: 0.71282 |  0:00:01s\n",
      "epoch 6  | loss: 0.55524 | val_accuracy: 0.65641 |  0:00:02s\n",
      "epoch 7  | loss: 0.55357 | val_accuracy: 0.65128 |  0:00:02s\n",
      "epoch 8  | loss: 0.54112 | val_accuracy: 0.66667 |  0:00:03s\n",
      "epoch 9  | loss: 0.53087 | val_accuracy: 0.66667 |  0:00:03s\n",
      "epoch 10 | loss: 0.5437  | val_accuracy: 0.69231 |  0:00:03s\n",
      "epoch 11 | loss: 0.54106 | val_accuracy: 0.67179 |  0:00:03s\n",
      "epoch 12 | loss: 0.54541 | val_accuracy: 0.68205 |  0:00:04s\n",
      "epoch 13 | loss: 0.54076 | val_accuracy: 0.68205 |  0:00:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:11:29,296] Trial 9 finished with value: 0.7333333333333333 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.864532166875735, 'lambda_sparse': 1.1227648446765894e-06, 'learning_rate': 0.00822026148716611, 'batch_size': 128, 'num_epochs': 81}. Best is trial 5 with value: 0.7333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.53923 | val_accuracy: 0.69744 |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.86321 | val_accuracy: 0.61538 |  0:00:01s\n",
      "epoch 1  | loss: 1.01606 | val_accuracy: 0.6     |  0:00:03s\n",
      "epoch 2  | loss: 0.85721 | val_accuracy: 0.61538 |  0:00:05s\n",
      "epoch 3  | loss: 0.75975 | val_accuracy: 0.66667 |  0:00:07s\n",
      "epoch 4  | loss: 0.81256 | val_accuracy: 0.62564 |  0:00:08s\n",
      "epoch 5  | loss: 0.62794 | val_accuracy: 0.65128 |  0:00:10s\n",
      "epoch 6  | loss: 0.6372  | val_accuracy: 0.68205 |  0:00:12s\n",
      "epoch 7  | loss: 0.58164 | val_accuracy: 0.63077 |  0:00:13s\n",
      "epoch 8  | loss: 0.59999 | val_accuracy: 0.69744 |  0:00:15s\n",
      "epoch 9  | loss: 0.53638 | val_accuracy: 0.67179 |  0:00:17s\n",
      "epoch 10 | loss: 0.55047 | val_accuracy: 0.65641 |  0:00:19s\n",
      "epoch 11 | loss: 0.53529 | val_accuracy: 0.68205 |  0:00:21s\n",
      "epoch 12 | loss: 0.55651 | val_accuracy: 0.69744 |  0:00:23s\n",
      "epoch 13 | loss: 0.51592 | val_accuracy: 0.70256 |  0:00:24s\n",
      "epoch 14 | loss: 0.54308 | val_accuracy: 0.76923 |  0:00:26s\n",
      "epoch 15 | loss: 0.5295  | val_accuracy: 0.71795 |  0:00:28s\n",
      "epoch 16 | loss: 0.56023 | val_accuracy: 0.69744 |  0:00:30s\n",
      "epoch 17 | loss: 0.52498 | val_accuracy: 0.71282 |  0:00:32s\n",
      "epoch 18 | loss: 0.54916 | val_accuracy: 0.70256 |  0:00:33s\n",
      "epoch 19 | loss: 0.5393  | val_accuracy: 0.70769 |  0:00:35s\n",
      "epoch 20 | loss: 0.51751 | val_accuracy: 0.68718 |  0:00:38s\n",
      "epoch 21 | loss: 0.52087 | val_accuracy: 0.71795 |  0:00:41s\n",
      "epoch 22 | loss: 0.5307  | val_accuracy: 0.70769 |  0:00:45s\n",
      "epoch 23 | loss: 0.52884 | val_accuracy: 0.68718 |  0:00:47s\n",
      "epoch 24 | loss: 0.52959 | val_accuracy: 0.72308 |  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_accuracy = 0.76923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:12:19,808] Trial 10 finished with value: 0.7692307692307693 and parameters: {'n_d': 44, 'n_a': 64, 'n_steps': 8, 'gamma': 1.0536806680260573, 'lambda_sparse': 0.0009651741843731034, 'learning_rate': 0.014909371860316725, 'batch_size': 64, 'num_epochs': 65}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.67551 | val_accuracy: 0.57949 |  0:00:02s\n",
      "epoch 1  | loss: 0.91818 | val_accuracy: 0.59487 |  0:00:04s\n",
      "epoch 2  | loss: 0.76845 | val_accuracy: 0.57436 |  0:00:07s\n",
      "epoch 3  | loss: 0.66997 | val_accuracy: 0.62564 |  0:00:09s\n",
      "epoch 4  | loss: 0.64081 | val_accuracy: 0.62051 |  0:00:13s\n",
      "epoch 5  | loss: 0.57597 | val_accuracy: 0.68205 |  0:00:15s\n",
      "epoch 6  | loss: 0.61934 | val_accuracy: 0.72821 |  0:00:17s\n",
      "epoch 7  | loss: 0.56822 | val_accuracy: 0.75897 |  0:00:19s\n",
      "epoch 8  | loss: 0.58899 | val_accuracy: 0.73333 |  0:00:20s\n",
      "epoch 9  | loss: 0.57647 | val_accuracy: 0.73333 |  0:00:22s\n",
      "epoch 10 | loss: 0.53334 | val_accuracy: 0.69231 |  0:00:24s\n",
      "epoch 11 | loss: 0.58584 | val_accuracy: 0.64103 |  0:00:26s\n",
      "epoch 12 | loss: 0.57667 | val_accuracy: 0.62564 |  0:00:28s\n",
      "epoch 13 | loss: 0.54785 | val_accuracy: 0.67179 |  0:00:30s\n",
      "epoch 14 | loss: 0.53163 | val_accuracy: 0.67692 |  0:00:32s\n",
      "epoch 15 | loss: 0.52438 | val_accuracy: 0.74359 |  0:00:34s\n",
      "epoch 16 | loss: 0.53225 | val_accuracy: 0.72308 |  0:00:36s\n",
      "epoch 17 | loss: 0.55403 | val_accuracy: 0.72308 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.75897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:12:59,229] Trial 11 finished with value: 0.7589743589743589 and parameters: {'n_d': 43, 'n_a': 64, 'n_steps': 8, 'gamma': 1.097541715979277, 'lambda_sparse': 0.0009383845836298559, 'learning_rate': 0.019471490331383366, 'batch_size': 64, 'num_epochs': 67}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.38387 | val_accuracy: 0.62051 |  0:00:02s\n",
      "epoch 1  | loss: 0.99445 | val_accuracy: 0.69744 |  0:00:03s\n",
      "epoch 2  | loss: 0.85932 | val_accuracy: 0.71282 |  0:00:05s\n",
      "epoch 3  | loss: 0.79368 | val_accuracy: 0.71282 |  0:00:07s\n",
      "epoch 4  | loss: 0.67337 | val_accuracy: 0.63077 |  0:00:08s\n",
      "epoch 5  | loss: 0.75427 | val_accuracy: 0.66667 |  0:00:10s\n",
      "epoch 6  | loss: 0.64566 | val_accuracy: 0.66154 |  0:00:12s\n",
      "epoch 7  | loss: 0.5644  | val_accuracy: 0.65128 |  0:00:14s\n",
      "epoch 8  | loss: 0.53521 | val_accuracy: 0.6359  |  0:00:16s\n",
      "epoch 9  | loss: 0.53326 | val_accuracy: 0.67179 |  0:00:18s\n",
      "epoch 10 | loss: 0.53919 | val_accuracy: 0.69231 |  0:00:20s\n",
      "epoch 11 | loss: 0.52142 | val_accuracy: 0.69744 |  0:00:21s\n",
      "epoch 12 | loss: 0.50656 | val_accuracy: 0.71795 |  0:00:23s\n",
      "epoch 13 | loss: 0.49952 | val_accuracy: 0.73846 |  0:00:25s\n",
      "epoch 14 | loss: 0.5179  | val_accuracy: 0.73333 |  0:00:27s\n",
      "epoch 15 | loss: 0.51551 | val_accuracy: 0.72308 |  0:00:29s\n",
      "epoch 16 | loss: 0.50081 | val_accuracy: 0.66154 |  0:00:30s\n",
      "epoch 17 | loss: 0.50824 | val_accuracy: 0.72308 |  0:00:32s\n",
      "epoch 18 | loss: 0.50607 | val_accuracy: 0.74872 |  0:00:34s\n",
      "epoch 19 | loss: 0.52499 | val_accuracy: 0.73333 |  0:00:36s\n",
      "epoch 20 | loss: 0.51442 | val_accuracy: 0.68205 |  0:00:38s\n",
      "epoch 21 | loss: 0.51929 | val_accuracy: 0.71795 |  0:00:40s\n",
      "epoch 22 | loss: 0.4825  | val_accuracy: 0.71795 |  0:00:41s\n",
      "epoch 23 | loss: 0.50102 | val_accuracy: 0.69231 |  0:00:43s\n",
      "epoch 24 | loss: 0.5102  | val_accuracy: 0.69744 |  0:00:45s\n",
      "epoch 25 | loss: 0.4802  | val_accuracy: 0.68205 |  0:00:47s\n",
      "epoch 26 | loss: 0.46959 | val_accuracy: 0.69744 |  0:00:48s\n",
      "epoch 27 | loss: 0.48299 | val_accuracy: 0.68718 |  0:00:50s\n",
      "epoch 28 | loss: 0.47496 | val_accuracy: 0.71282 |  0:00:52s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_accuracy = 0.74872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:13:52,074] Trial 12 finished with value: 0.7487179487179487 and parameters: {'n_d': 38, 'n_a': 64, 'n_steps': 8, 'gamma': 1.0225020269811136, 'lambda_sparse': 0.0009305941441358778, 'learning_rate': 0.010654302434232382, 'batch_size': 64, 'num_epochs': 61}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.52814 | val_accuracy: 0.61026 |  0:00:02s\n",
      "epoch 1  | loss: 1.39212 | val_accuracy: 0.64615 |  0:00:04s\n",
      "epoch 2  | loss: 1.116   | val_accuracy: 0.65128 |  0:00:06s\n",
      "epoch 3  | loss: 0.82908 | val_accuracy: 0.72821 |  0:00:08s\n",
      "epoch 4  | loss: 0.66796 | val_accuracy: 0.67692 |  0:00:10s\n",
      "epoch 5  | loss: 0.63114 | val_accuracy: 0.65641 |  0:00:13s\n",
      "epoch 6  | loss: 0.60503 | val_accuracy: 0.64615 |  0:00:15s\n",
      "epoch 7  | loss: 0.57009 | val_accuracy: 0.70256 |  0:00:17s\n",
      "epoch 8  | loss: 0.56346 | val_accuracy: 0.72308 |  0:00:19s\n",
      "epoch 9  | loss: 0.64884 | val_accuracy: 0.67179 |  0:00:21s\n",
      "epoch 10 | loss: 0.62235 | val_accuracy: 0.70256 |  0:00:23s\n",
      "epoch 11 | loss: 0.57914 | val_accuracy: 0.71795 |  0:00:25s\n",
      "epoch 12 | loss: 0.56637 | val_accuracy: 0.67692 |  0:00:27s\n",
      "epoch 13 | loss: 0.5456  | val_accuracy: 0.68718 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.72821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:14:23,115] Trial 13 finished with value: 0.7282051282051282 and parameters: {'n_d': 42, 'n_a': 63, 'n_steps': 10, 'gamma': 1.0010939230641154, 'lambda_sparse': 0.0009554191077814104, 'learning_rate': 0.019310823579486002, 'batch_size': 64, 'num_epochs': 66}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08702 | val_accuracy: 0.59487 |  0:00:01s\n",
      "epoch 1  | loss: 0.8354  | val_accuracy: 0.69231 |  0:00:03s\n",
      "epoch 2  | loss: 0.76382 | val_accuracy: 0.64103 |  0:00:05s\n",
      "epoch 3  | loss: 0.78146 | val_accuracy: 0.67179 |  0:00:07s\n",
      "epoch 4  | loss: 0.76232 | val_accuracy: 0.67692 |  0:00:09s\n",
      "epoch 5  | loss: 0.77418 | val_accuracy: 0.69231 |  0:00:10s\n",
      "epoch 6  | loss: 0.6962  | val_accuracy: 0.66667 |  0:00:12s\n",
      "epoch 7  | loss: 0.68205 | val_accuracy: 0.71795 |  0:00:14s\n",
      "epoch 8  | loss: 0.65973 | val_accuracy: 0.72308 |  0:00:16s\n",
      "epoch 9  | loss: 0.6593  | val_accuracy: 0.67692 |  0:00:18s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_accuracy = 0.72308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:14:42,856] Trial 14 finished with value: 0.7230769230769231 and parameters: {'n_d': 31, 'n_a': 59, 'n_steps': 8, 'gamma': 1.1794831160910817, 'lambda_sparse': 0.0002578983847684253, 'learning_rate': 0.0036691049977703766, 'batch_size': 64, 'num_epochs': 10}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.33553 | val_accuracy: 0.57949 |  0:00:01s\n",
      "epoch 1  | loss: 0.99244 | val_accuracy: 0.58974 |  0:00:03s\n",
      "epoch 2  | loss: 0.83556 | val_accuracy: 0.59487 |  0:00:04s\n",
      "epoch 3  | loss: 0.76208 | val_accuracy: 0.63077 |  0:00:06s\n",
      "epoch 4  | loss: 0.61018 | val_accuracy: 0.60513 |  0:00:07s\n",
      "epoch 5  | loss: 0.75727 | val_accuracy: 0.61026 |  0:00:09s\n",
      "epoch 6  | loss: 0.64777 | val_accuracy: 0.64615 |  0:00:10s\n",
      "epoch 7  | loss: 0.58232 | val_accuracy: 0.61538 |  0:00:12s\n",
      "epoch 8  | loss: 0.62005 | val_accuracy: 0.66667 |  0:00:14s\n",
      "epoch 9  | loss: 0.56622 | val_accuracy: 0.67692 |  0:00:15s\n",
      "epoch 10 | loss: 0.58128 | val_accuracy: 0.66667 |  0:00:17s\n",
      "epoch 11 | loss: 0.56318 | val_accuracy: 0.69231 |  0:00:19s\n",
      "epoch 12 | loss: 0.57402 | val_accuracy: 0.6359  |  0:00:21s\n",
      "epoch 13 | loss: 0.60607 | val_accuracy: 0.67179 |  0:00:22s\n",
      "epoch 14 | loss: 0.59626 | val_accuracy: 0.70256 |  0:00:24s\n",
      "epoch 15 | loss: 0.55107 | val_accuracy: 0.67179 |  0:00:25s\n",
      "epoch 16 | loss: 0.58588 | val_accuracy: 0.6359  |  0:00:27s\n",
      "epoch 17 | loss: 0.57096 | val_accuracy: 0.68718 |  0:00:29s\n",
      "epoch 18 | loss: 0.54253 | val_accuracy: 0.69231 |  0:00:30s\n",
      "epoch 19 | loss: 0.54819 | val_accuracy: 0.70769 |  0:00:32s\n",
      "epoch 20 | loss: 0.5438  | val_accuracy: 0.70256 |  0:00:33s\n",
      "epoch 21 | loss: 0.53715 | val_accuracy: 0.71795 |  0:00:35s\n",
      "epoch 22 | loss: 0.55852 | val_accuracy: 0.69231 |  0:00:36s\n",
      "epoch 23 | loss: 0.55577 | val_accuracy: 0.72821 |  0:00:38s\n",
      "epoch 24 | loss: 0.53072 | val_accuracy: 0.73333 |  0:00:40s\n",
      "epoch 25 | loss: 0.51976 | val_accuracy: 0.70256 |  0:00:41s\n",
      "epoch 26 | loss: 0.51369 | val_accuracy: 0.71282 |  0:00:43s\n",
      "epoch 27 | loss: 0.53757 | val_accuracy: 0.70769 |  0:00:44s\n",
      "epoch 28 | loss: 0.54778 | val_accuracy: 0.65641 |  0:00:46s\n",
      "epoch 29 | loss: 0.53237 | val_accuracy: 0.71282 |  0:00:47s\n",
      "epoch 30 | loss: 0.52267 | val_accuracy: 0.71282 |  0:00:49s\n",
      "epoch 31 | loss: 0.5425  | val_accuracy: 0.68718 |  0:00:51s\n",
      "epoch 32 | loss: 0.52911 | val_accuracy: 0.71282 |  0:00:52s\n",
      "epoch 33 | loss: 0.51064 | val_accuracy: 0.72821 |  0:00:54s\n",
      "epoch 34 | loss: 0.5182  | val_accuracy: 0.69231 |  0:00:55s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:15:39,943] Trial 15 finished with value: 0.7333333333333333 and parameters: {'n_d': 50, 'n_a': 30, 'n_steps': 8, 'gamma': 1.1356078007331136, 'lambda_sparse': 8.44268409753167e-05, 'learning_rate': 0.021529987841165235, 'batch_size': 64, 'num_epochs': 69}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.25674 | val_accuracy: 0.54872 |  0:00:02s\n",
      "epoch 1  | loss: 0.95389 | val_accuracy: 0.61538 |  0:00:04s\n",
      "epoch 2  | loss: 0.79866 | val_accuracy: 0.67692 |  0:00:06s\n",
      "epoch 3  | loss: 0.7904  | val_accuracy: 0.65128 |  0:00:08s\n",
      "epoch 4  | loss: 0.7908  | val_accuracy: 0.6359  |  0:00:10s\n",
      "epoch 5  | loss: 0.72806 | val_accuracy: 0.66154 |  0:00:12s\n",
      "epoch 6  | loss: 0.73316 | val_accuracy: 0.67179 |  0:00:13s\n",
      "epoch 7  | loss: 0.77101 | val_accuracy: 0.64103 |  0:00:15s\n",
      "epoch 8  | loss: 0.85507 | val_accuracy: 0.64615 |  0:00:17s\n",
      "epoch 9  | loss: 0.68954 | val_accuracy: 0.65641 |  0:00:19s\n",
      "epoch 10 | loss: 0.72392 | val_accuracy: 0.62051 |  0:00:21s\n",
      "epoch 11 | loss: 0.94064 | val_accuracy: 0.62051 |  0:00:23s\n",
      "epoch 12 | loss: 0.90019 | val_accuracy: 0.64615 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.67692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:16:05,849] Trial 16 finished with value: 0.676923076923077 and parameters: {'n_d': 37, 'n_a': 36, 'n_steps': 10, 'gamma': 1.285331090166534, 'lambda_sparse': 0.0004984515747833953, 'learning_rate': 0.004332465188586322, 'batch_size': 64, 'num_epochs': 51}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.38351 | val_accuracy: 0.67179 |  0:00:00s\n",
      "epoch 1  | loss: 0.64832 | val_accuracy: 0.6     |  0:00:02s\n",
      "epoch 2  | loss: 0.6603  | val_accuracy: 0.61538 |  0:00:03s\n",
      "epoch 3  | loss: 0.67697 | val_accuracy: 0.47692 |  0:00:04s\n",
      "epoch 4  | loss: 0.69816 | val_accuracy: 0.64103 |  0:00:05s\n",
      "epoch 5  | loss: 0.58287 | val_accuracy: 0.65641 |  0:00:07s\n",
      "epoch 6  | loss: 0.60594 | val_accuracy: 0.68718 |  0:00:08s\n",
      "epoch 7  | loss: 0.5895  | val_accuracy: 0.64615 |  0:00:09s\n",
      "epoch 8  | loss: 0.58006 | val_accuracy: 0.68718 |  0:00:10s\n",
      "epoch 9  | loss: 0.58752 | val_accuracy: 0.70256 |  0:00:11s\n",
      "epoch 10 | loss: 0.57782 | val_accuracy: 0.67692 |  0:00:13s\n",
      "epoch 11 | loss: 0.59959 | val_accuracy: 0.68205 |  0:00:14s\n",
      "epoch 12 | loss: 0.55179 | val_accuracy: 0.70256 |  0:00:15s\n",
      "epoch 13 | loss: 0.5887  | val_accuracy: 0.69231 |  0:00:16s\n",
      "epoch 14 | loss: 0.55777 | val_accuracy: 0.70256 |  0:00:17s\n",
      "epoch 15 | loss: 0.5708  | val_accuracy: 0.70769 |  0:00:19s\n",
      "epoch 16 | loss: 0.58068 | val_accuracy: 0.67179 |  0:00:20s\n",
      "epoch 17 | loss: 0.55701 | val_accuracy: 0.71795 |  0:00:21s\n",
      "epoch 18 | loss: 0.5611  | val_accuracy: 0.72308 |  0:00:22s\n",
      "epoch 19 | loss: 0.58146 | val_accuracy: 0.66667 |  0:00:24s\n",
      "epoch 20 | loss: 0.56922 | val_accuracy: 0.70769 |  0:00:25s\n",
      "epoch 21 | loss: 0.54822 | val_accuracy: 0.68205 |  0:00:26s\n",
      "epoch 22 | loss: 0.55074 | val_accuracy: 0.73846 |  0:00:27s\n",
      "epoch 23 | loss: 0.53857 | val_accuracy: 0.73846 |  0:00:28s\n",
      "epoch 24 | loss: 0.52817 | val_accuracy: 0.70769 |  0:00:30s\n",
      "epoch 25 | loss: 0.54037 | val_accuracy: 0.71282 |  0:00:31s\n",
      "epoch 26 | loss: 0.52369 | val_accuracy: 0.72821 |  0:00:32s\n",
      "epoch 27 | loss: 0.53411 | val_accuracy: 0.72821 |  0:00:33s\n",
      "epoch 28 | loss: 0.51745 | val_accuracy: 0.72821 |  0:00:34s\n",
      "epoch 29 | loss: 0.51705 | val_accuracy: 0.7641  |  0:00:35s\n",
      "epoch 30 | loss: 0.53076 | val_accuracy: 0.74359 |  0:00:36s\n",
      "epoch 31 | loss: 0.52816 | val_accuracy: 0.71795 |  0:00:37s\n",
      "epoch 32 | loss: 0.52834 | val_accuracy: 0.69231 |  0:00:38s\n",
      "epoch 33 | loss: 0.52662 | val_accuracy: 0.72308 |  0:00:39s\n",
      "epoch 34 | loss: 0.54576 | val_accuracy: 0.73846 |  0:00:40s\n",
      "epoch 35 | loss: 0.50883 | val_accuracy: 0.71795 |  0:00:41s\n",
      "epoch 36 | loss: 0.54237 | val_accuracy: 0.73333 |  0:00:42s\n",
      "epoch 37 | loss: 0.51337 | val_accuracy: 0.72308 |  0:00:42s\n",
      "epoch 38 | loss: 0.51303 | val_accuracy: 0.73846 |  0:00:43s\n",
      "epoch 39 | loss: 0.51863 | val_accuracy: 0.72821 |  0:00:44s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_accuracy = 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:16:51,337] Trial 17 finished with value: 0.764102564102564 and parameters: {'n_d': 24, 'n_a': 8, 'n_steps': 7, 'gamma': 1.104921695089, 'lambda_sparse': 0.00011834813690312385, 'learning_rate': 0.09067809977281778, 'batch_size': 64, 'num_epochs': 72}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.36853 | val_accuracy: 0.63077 |  0:00:01s\n",
      "epoch 1  | loss: 0.66708 | val_accuracy: 0.67179 |  0:00:02s\n",
      "epoch 2  | loss: 0.61813 | val_accuracy: 0.6     |  0:00:04s\n",
      "epoch 3  | loss: 0.59657 | val_accuracy: 0.63077 |  0:00:05s\n",
      "epoch 4  | loss: 0.65278 | val_accuracy: 0.63077 |  0:00:07s\n",
      "epoch 5  | loss: 0.60921 | val_accuracy: 0.64615 |  0:00:08s\n",
      "epoch 6  | loss: 0.65094 | val_accuracy: 0.66667 |  0:00:09s\n",
      "epoch 7  | loss: 0.58791 | val_accuracy: 0.66154 |  0:00:10s\n",
      "epoch 8  | loss: 0.59263 | val_accuracy: 0.66667 |  0:00:11s\n",
      "epoch 9  | loss: 0.61428 | val_accuracy: 0.67179 |  0:00:13s\n",
      "epoch 10 | loss: 0.59989 | val_accuracy: 0.62564 |  0:00:14s\n",
      "epoch 11 | loss: 0.57562 | val_accuracy: 0.67179 |  0:00:15s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.67179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:17:07,980] Trial 18 finished with value: 0.6717948717948717 and parameters: {'n_d': 22, 'n_a': 10, 'n_steps': 7, 'gamma': 1.328126807120161, 'lambda_sparse': 0.00010280566361474043, 'learning_rate': 0.09972131386652579, 'batch_size': 64, 'num_epochs': 44}. Best is trial 10 with value: 0.7692307692307693.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6536\\976418090.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.32713 | val_accuracy: 0.63077 |  0:00:01s\n",
      "epoch 1  | loss: 0.69975 | val_accuracy: 0.61026 |  0:00:02s\n",
      "epoch 2  | loss: 0.63773 | val_accuracy: 0.60513 |  0:00:03s\n",
      "epoch 3  | loss: 0.69081 | val_accuracy: 0.64615 |  0:00:04s\n",
      "epoch 4  | loss: 0.58557 | val_accuracy: 0.67692 |  0:00:05s\n",
      "epoch 5  | loss: 0.60118 | val_accuracy: 0.67179 |  0:00:06s\n",
      "epoch 6  | loss: 0.6362  | val_accuracy: 0.61538 |  0:00:07s\n",
      "epoch 7  | loss: 0.63918 | val_accuracy: 0.70256 |  0:00:08s\n",
      "epoch 8  | loss: 0.57456 | val_accuracy: 0.71282 |  0:00:10s\n",
      "epoch 9  | loss: 0.59137 | val_accuracy: 0.68718 |  0:00:11s\n",
      "epoch 10 | loss: 0.55959 | val_accuracy: 0.72308 |  0:00:12s\n",
      "epoch 11 | loss: 0.55957 | val_accuracy: 0.70256 |  0:00:13s\n",
      "epoch 12 | loss: 0.549   | val_accuracy: 0.66154 |  0:00:15s\n",
      "epoch 13 | loss: 0.58846 | val_accuracy: 0.67179 |  0:00:16s\n",
      "epoch 14 | loss: 0.57942 | val_accuracy: 0.72308 |  0:00:17s\n",
      "epoch 15 | loss: 0.55671 | val_accuracy: 0.68718 |  0:00:18s\n",
      "epoch 16 | loss: 0.57221 | val_accuracy: 0.66154 |  0:00:20s\n",
      "epoch 17 | loss: 0.56292 | val_accuracy: 0.69231 |  0:00:21s\n",
      "epoch 18 | loss: 0.56018 | val_accuracy: 0.70769 |  0:00:22s\n",
      "epoch 19 | loss: 0.53666 | val_accuracy: 0.71282 |  0:00:23s\n",
      "epoch 20 | loss: 0.54188 | val_accuracy: 0.70769 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.72308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 22:17:33,676] Trial 19 finished with value: 0.7230769230769231 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 9, 'gamma': 1.0609717646774035, 'lambda_sparse': 4.068106497222082e-05, 'learning_rate': 0.041600966352928884, 'batch_size': 64, 'num_epochs': 74}. Best is trial 10 with value: 0.7692307692307693.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.86321 | val_accuracy: 0.61538 |  0:00:01s\n",
      "epoch 1  | loss: 1.01606 | val_accuracy: 0.6     |  0:00:03s\n",
      "epoch 2  | loss: 0.85721 | val_accuracy: 0.61538 |  0:00:05s\n",
      "epoch 3  | loss: 0.75975 | val_accuracy: 0.66667 |  0:00:07s\n",
      "epoch 4  | loss: 0.81256 | val_accuracy: 0.62564 |  0:00:09s\n",
      "epoch 5  | loss: 0.62794 | val_accuracy: 0.65128 |  0:00:11s\n",
      "epoch 6  | loss: 0.6372  | val_accuracy: 0.68205 |  0:00:14s\n",
      "epoch 7  | loss: 0.58164 | val_accuracy: 0.63077 |  0:00:17s\n",
      "epoch 8  | loss: 0.59999 | val_accuracy: 0.69744 |  0:00:19s\n",
      "epoch 9  | loss: 0.53638 | val_accuracy: 0.67179 |  0:00:20s\n",
      "epoch 10 | loss: 0.55047 | val_accuracy: 0.65641 |  0:00:22s\n",
      "epoch 11 | loss: 0.53529 | val_accuracy: 0.68205 |  0:00:24s\n",
      "epoch 12 | loss: 0.55651 | val_accuracy: 0.69744 |  0:00:25s\n",
      "epoch 13 | loss: 0.51592 | val_accuracy: 0.70256 |  0:00:27s\n",
      "epoch 14 | loss: 0.54308 | val_accuracy: 0.76923 |  0:00:29s\n",
      "epoch 15 | loss: 0.5295  | val_accuracy: 0.71795 |  0:00:32s\n",
      "epoch 16 | loss: 0.56023 | val_accuracy: 0.69744 |  0:00:34s\n",
      "epoch 17 | loss: 0.52498 | val_accuracy: 0.71282 |  0:00:37s\n",
      "epoch 18 | loss: 0.54916 | val_accuracy: 0.70256 |  0:00:39s\n",
      "epoch 19 | loss: 0.5393  | val_accuracy: 0.70769 |  0:00:41s\n",
      "epoch 20 | loss: 0.51751 | val_accuracy: 0.68718 |  0:00:44s\n",
      "epoch 21 | loss: 0.52087 | val_accuracy: 0.71795 |  0:00:46s\n",
      "epoch 22 | loss: 0.5307  | val_accuracy: 0.70769 |  0:00:49s\n",
      "epoch 23 | loss: 0.52884 | val_accuracy: 0.68718 |  0:00:51s\n",
      "epoch 24 | loss: 0.52959 | val_accuracy: 0.72308 |  0:00:55s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_accuracy = 0.76923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression         0.748718  0.796644                    0.003991   \n",
      "KNN                         0.753846  0.840861                    0.001994   \n",
      "Decision Tree               0.728205  0.749789                    0.004987   \n",
      "Random Forest                    0.8  0.864816                    0.727075   \n",
      "Gradient Boosting                0.8  0.861439                    0.513596   \n",
      "XGBoost                     0.764103  0.853525                      0.1127   \n",
      "LightGBM                    0.789744  0.866083                    0.057846   \n",
      "CatBoost                    0.753846  0.843288                    0.245005   \n",
      "MLP                         0.764103  0.838863                    3.545068   \n",
      "DNN                         0.774359  0.836116                    2.187616   \n",
      "DCN                         0.723077  0.778793                    5.800046   \n",
      "Wide_and_Deep               0.774359  0.835693                    2.907177   \n",
      "XGBoost + NN                0.697436    0.8209                    3.071482   \n",
      "LightGBM + NN               0.753846  0.834954                    5.210468   \n",
      "CatBoost + NN               0.764103   0.84309                    8.960259   \n",
      "AutoInt                     0.738462  0.831255                    9.652604   \n",
      "FT-Transformer              0.789744  0.819738                   13.344369   \n",
      "Neural Architecture Search  0.774359   0.83041                    6.319774   \n",
      "NODE                        0.779487  0.847105                   12.640264   \n",
      "TabNet                      0.779487  0.814349                   56.761422   \n",
      "KAN                         0.789744  0.846154                    2.070088   \n",
      "SAINT                       0.733333  0.807904                   52.492634   \n",
      "VIME                        0.769231  0.816462                   57.333854   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                                 0.0   \n",
      "KNN                                            0.004881   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.053861   \n",
      "Gradient Boosting                              0.001995   \n",
      "XGBoost                                        0.002992   \n",
      "LightGBM                                       0.002992   \n",
      "CatBoost                                       0.000997   \n",
      "MLP                                            0.000997   \n",
      "DNN                                            0.000997   \n",
      "DCN                                            0.000998   \n",
      "Wide_and_Deep                                  0.000998   \n",
      "XGBoost + NN                                   0.000995   \n",
      "LightGBM + NN                                  0.000999   \n",
      "CatBoost + NN                                   0.00099   \n",
      "AutoInt                                        0.001995   \n",
      "FT-Transformer                                 0.000996   \n",
      "Neural Architecture Search                     0.000996   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.080219   \n",
      "KAN                                                 0.0   \n",
      "SAINT                                          0.012965   \n",
      "VIME                                           0.258017   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        4.049861   \n",
      "KNN                                        0.248299   \n",
      "Decision Tree                               0.11569   \n",
      "Random Forest                              9.095487   \n",
      "Gradient Boosting                         12.765274   \n",
      "XGBoost                                    1.586735   \n",
      "LightGBM                                   6.770629   \n",
      "CatBoost                                   10.83189   \n",
      "MLP                                      234.750347   \n",
      "DNN                                       94.017027   \n",
      "DCN                                      127.458129   \n",
      "Wide_and_Deep                             96.221561   \n",
      "XGBoost + NN                             107.399315   \n",
      "LightGBM + NN                            104.179843   \n",
      "CatBoost + NN                            119.045705   \n",
      "AutoInt                                  154.137457   \n",
      "FT-Transformer                           270.296228   \n",
      "Neural Architecture Search               102.635101   \n",
      "NODE                                     296.128333   \n",
      "TabNet                                   607.092467   \n",
      "KAN                                        44.48086   \n",
      "SAINT                                    638.582521   \n",
      "VIME                                     749.467483   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                       {'max_depth': 20, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 5, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "XGBoost                     {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  \n",
      "LightGBM                    {'learning_rate': 0.1, 'n_estimators': 100, 'n...  \n",
      "CatBoost                    {'depth': 6, 'iterations': 100, 'learning_rate...  \n",
      "MLP                         {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  \n",
      "DNN                         {'hidden_dim_0': 193, 'hidden_dim_1': 234, 'hi...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 85, 'hid...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 145, 'hidden_layer_1': 101,...  \n",
      "XGBoost + NN                {'n_estimators': 295, 'max_depth': 9, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 269, 'max_depth': 4, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 296, 'depth': 8, 'catboost_lear...  \n",
      "AutoInt                     {'num_heads': 8, 'embedding_dim': 56, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 6, 'embedding_dim': 48, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 2, 'hidden_layer_0': 61, 'hidde...  \n",
      "NODE                        {'num_layers': 1, 'num_trees': 8, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 33, 'n_a': 8, 'n_steps': 5, 'gamma': 1...  \n",
      "KAN                         {'hidden_dim': 195, 'learning_rate': 0.0030866...  \n",
      "SAINT                       {'heads': 2, 'dim': 168, 'depth': 4, 'mlp_dim'...  \n",
      "VIME                        {'n_d': 44, 'n_a': 64, 'n_steps': 8, 'gamma': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_d: 44\n",
      "n_a: 64\n",
      "n_steps: 8\n",
      "gamma: 1.0536806680260573\n",
      "lambda_sparse: 0.0009651741843731034\n",
      "learning_rate: 0.014909371860316725\n",
      "batch_size: 64\n",
      "num_epochs: 65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for VIME-like model (using TabNet as proxy)\n",
    "    n_d = trial.suggest_int('n_d', 8, 64)\n",
    "    n_a = trial.suggest_int('n_a', 8, 64)\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the TabNet model\n",
    "    model = TabNetClassifier(\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        device_name=device\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        X_train=X_train_scaled, y_train=y_train.values,\n",
    "        eval_set=[(X_test_scaled, y_test.values)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=10,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=batch_size // 2,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final TabNet model with the best hyperparameters\n",
    "best_model = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "    device_name=device\n",
    ")\n",
    "\n",
    "training_start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train=X_train_scaled, y_train=y_train.values,\n",
    "    eval_set=[(X_test_scaled, y_test.values)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=best_params['num_epochs'],\n",
    "    patience=10,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    virtual_batch_size=best_params['batch_size'] // 2,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_start_time = time.time()\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['VIME'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"result_white\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
