{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = rice_cammeo_and_osmancik.data.features \n",
    "y = rice_cammeo_and_osmancik.data.targets\n",
    "   \n",
    "# Combine X and y into a single DataFrame\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rename the target column to 'Y'\n",
    "df = df.rename(columns={df.columns[-1]: 'Y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Y_column = df['Y'].copy()\n",
    "df.drop('Y', axis=1, inplace=True)\n",
    "\n",
    "# Identify categorical data (change this based on your actual data)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Standardize only the continuous (non-categorical) columns\n",
    "continuous_cols = df.columns.difference(categorical_cols)  # Gets the difference, i.e., continuous cols\n",
    "df[continuous_cols] = (df[continuous_cols] - df[continuous_cols].mean()) / df[continuous_cols].std()\n",
    "\n",
    "# Filter out outliers in continuous data (|z-score| > 5)\n",
    "mask = (np.abs(df[continuous_cols]) < 10).all(axis=1)\n",
    "df = df[mask]\n",
    "\n",
    "# Reattach the target variable 'Y' to the DataFrame\n",
    "df['Y'] = Y_column[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for column in categorical_cols:\n",
    "    # Ensure the column is of type object (string) or category\n",
    "    if df[column].dtype == 'object' or df[column].dtype.name == 'category':\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        \n",
    "df['Y'], unique = pd.factorize(df['Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:55:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1768, number of negative: 1280\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.580052 -> initscore=0.322989\n",
      "[LightGBM] [Info] Start training from score 0.322989\n",
      "   Transformation                Model  Accuracy       AUC\n",
      "0        standard  Logistic Regression  0.929134  0.982240\n",
      "1        standard                  KNN  0.905512  0.959702\n",
      "2        standard        Decision Tree  0.868766  0.866602\n",
      "3        standard        Random Forest  0.923885  0.977705\n",
      "4        standard    Gradient Boosting  0.923885  0.979386\n",
      "5        standard              XGBoost  0.918635  0.974355\n",
      "6        standard             LightGBM  0.918635  0.975978\n",
      "7        standard             CatBoost  0.923885  0.979535\n",
      "8          minmax  Logistic Regression  0.933071  0.982247\n",
      "9          minmax                  KNN  0.909449  0.957063\n",
      "10         minmax        Decision Tree  0.880577  0.878384\n",
      "11         minmax        Random Forest  0.916010  0.978051\n",
      "12         minmax    Gradient Boosting  0.925197  0.979421\n",
      "13         minmax              XGBoost  0.918635  0.974355\n",
      "14         minmax             LightGBM  0.919948  0.975846\n",
      "15         minmax             CatBoost  0.923885  0.979528\n",
      "16         robust  Logistic Regression  0.930446  0.982198\n",
      "17         robust                  KNN  0.902887  0.961279\n",
      "18         robust        Decision Tree  0.876640  0.874313\n",
      "19         robust        Random Forest  0.919948  0.977798\n",
      "20         robust    Gradient Boosting  0.925197  0.979435\n",
      "21         robust              XGBoost  0.918635  0.974355\n",
      "22         robust             LightGBM  0.916010  0.976189\n",
      "23         robust             CatBoost  0.923885  0.979528\n",
      "24     yeojohnson  Logistic Regression  0.927822  0.981893\n",
      "25     yeojohnson                  KNN  0.910761  0.957126\n",
      "26     yeojohnson        Decision Tree  0.885827  0.883024\n",
      "27     yeojohnson        Random Forest  0.922572  0.978859\n",
      "28     yeojohnson    Gradient Boosting  0.925197  0.979428\n",
      "29     yeojohnson              XGBoost  0.918635  0.974355\n",
      "30     yeojohnson             LightGBM  0.912073  0.975742\n",
      "31     yeojohnson             CatBoost  0.923885  0.979528\n",
      "32           mice  Logistic Regression  0.929134  0.982247\n",
      "33           mice                  KNN  0.905512  0.958707\n",
      "34           mice        Decision Tree  0.877953  0.875742\n",
      "35           mice        Random Forest  0.922572  0.978571\n",
      "36           mice    Gradient Boosting  0.925197  0.979386\n",
      "37           mice              XGBoost  0.918635  0.974355\n",
      "38           mice             LightGBM  0.917323  0.976789\n",
      "39           mice             CatBoost  0.923885  0.979528\n",
      "40     kernel_pca  Logistic Regression  0.933071  0.977080\n",
      "41     kernel_pca                  KNN  0.918635  0.957764\n",
      "42     kernel_pca        Decision Tree  0.885827  0.883883\n",
      "43     kernel_pca        Random Forest  0.916010  0.974754\n",
      "44     kernel_pca    Gradient Boosting  0.922572  0.976904\n",
      "45     kernel_pca              XGBoost  0.897638  0.967906\n",
      "46     kernel_pca             LightGBM  0.906824  0.970520\n",
      "47     kernel_pca             CatBoost  0.912073  0.976019\n",
      "48     no_scaling  Logistic Regression  0.929134  0.982247\n",
      "49     no_scaling                  KNN  0.905512  0.958707\n",
      "50     no_scaling        Decision Tree  0.875328  0.873100\n",
      "51     no_scaling        Random Forest  0.921260  0.978336\n",
      "52     no_scaling    Gradient Boosting  0.925197  0.979442\n",
      "53     no_scaling              XGBoost  0.918635  0.974355\n",
      "54     no_scaling             LightGBM  0.917323  0.976789\n",
      "55     no_scaling             CatBoost  0.923885  0.979528\n",
      "\n",
      "Best Accuracy:\n",
      "Transformation                 minmax\n",
      "Model             Logistic Regression\n",
      "Accuracy                     0.933071\n",
      "AUC                          0.982247\n",
      "Name: 8, dtype: object\n",
      "\n",
      "Best AUC:\n",
      "Transformation                 minmax\n",
      "Model             Logistic Regression\n",
      "Accuracy                     0.933071\n",
      "AUC                          0.982247\n",
      "Name: 8, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, PowerTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer for Box-Cox (handles non-positive data)\n",
    "class SafeBoxCoxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lambdas = None\n",
    "        self.min_values = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.min_values = X.min(axis=0)\n",
    "        X_shifted = X - self.min_values + 1  # Shift data to be strictly positive\n",
    "        _, self.lambdas = stats.boxcox(X_shifted)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_shifted = X - self.min_values + 1\n",
    "        return np.column_stack([stats.boxcox(X_shifted[:, i], self.lambdas[i]) for i in range(X.shape[1])])\n",
    "\n",
    "def run_analysis(X, y, is_binary):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Define transformations\n",
    "    transformations = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'robust': RobustScaler(),\n",
    "        'yeojohnson': PowerTransformer(method='yeo-johnson'),\n",
    "        'mice': IterativeImputer(),\n",
    "        'kernel_pca': KernelPCA(n_components=min(5, len(numeric_features)), kernel='rbf'),\n",
    "        'no_scaling': None\n",
    "    }\n",
    "\n",
    "    # Define models with configurations for both binary and multi-class\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(multi_class='ovr' if not is_binary else 'auto', max_iter=1000),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss' if is_binary else 'mlogloss'),\n",
    "        'LightGBM': LGBMClassifier(objective='binary' if is_binary else 'multiclass'),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for trans_name, transformer in transformations.items():\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Create pipeline\n",
    "                if transformer:\n",
    "                    numeric_transformer = Pipeline(steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', transformer)\n",
    "                    ])\n",
    "                    \n",
    "                    categorical_transformer = Pipeline(steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "                    \n",
    "                    preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                            ('num', numeric_transformer, numeric_features),\n",
    "                            ('cat', categorical_transformer, categorical_features)\n",
    "                        ])\n",
    "                    \n",
    "                    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                               ('classifier', model)])\n",
    "                else:\n",
    "                    pipeline = model\n",
    "                \n",
    "                # Fit and predict\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                y_pred = pipeline.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # For AUC, we need probability predictions\n",
    "                if hasattr(pipeline, \"predict_proba\"):\n",
    "                    y_pred_proba = pipeline.predict_proba(X_test)\n",
    "                    if is_binary:\n",
    "                        # Binary classification\n",
    "                        auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "                    else:\n",
    "                        # Multi-class classification\n",
    "                        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "                else:\n",
    "                    auc = np.nan\n",
    "                \n",
    "                results.append({\n",
    "                    'Transformation': trans_name,\n",
    "                    'Model': model_name,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {trans_name} transformation and {model_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Assume df is your DataFrame and 'Y' is your target column\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "# Get number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "is_binary = n_classes == 2\n",
    "\n",
    "# Run analysis\n",
    "results_df = run_analysis(X, y, is_binary)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n",
    "# Find best performing combination\n",
    "best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "best_auc = results_df.loc[results_df['AUC'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Accuracy:\")\n",
    "print(best_accuracy)\n",
    "print(\"\\nBest AUC:\")\n",
    "print(best_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average improvement by transformation:\n",
      "                Accuracy_Diff  AUC_Diff\n",
      "Transformation                         \n",
      "minmax               0.001312  0.000299\n",
      "yeojohnson           0.001312  0.000931\n",
      "mice                 0.000492  0.000353\n",
      "robust              -0.000328  0.000324\n",
      "standard            -0.000492 -0.000875\n",
      "kernel_pca          -0.002953 -0.002209\n",
      "\n",
      "Transformations that increase performance on average:\n",
      "                Accuracy_Diff  AUC_Diff\n",
      "Transformation                         \n",
      "minmax               0.001312  0.000299\n",
      "yeojohnson           0.001312  0.000931\n",
      "mice                 0.000492  0.000353\n",
      "robust              -0.000328  0.000324\n",
      "\n",
      "Best transformation for each model:\n",
      "                 Model Transformation  Accuracy_Diff  AUC_Diff\n",
      "0             CatBoost           mice       0.000000  0.000000\n",
      "1        Decision Tree     kernel_pca       0.010499  0.010784\n",
      "2    Gradient Boosting           mice       0.000000 -0.000055\n",
      "3                  KNN     kernel_pca       0.013123 -0.000943\n",
      "4             LightGBM         minmax       0.002625 -0.000943\n",
      "5  Logistic Regression     kernel_pca       0.003937 -0.005166\n",
      "6        Random Forest       standard       0.002625 -0.000631\n",
      "7              XGBoost           mice       0.000000  0.000000\n",
      "\n",
      "Overall best performing model and transformation:\n",
      "Transformation             kernel_pca\n",
      "Model             Logistic Regression\n",
      "Accuracy                     0.933071\n",
      "AUC                           0.97708\n",
      "Accuracy_Diff                0.003937\n",
      "AUC_Diff                    -0.005166\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming results_df is your DataFrame with the results\n",
    "\n",
    "# Function to compare with no_scaling\n",
    "def compare_with_no_scaling(df):\n",
    "    no_scaling = df[df['Transformation'] == 'no_scaling'].set_index('Model')\n",
    "    \n",
    "    def diff(group):\n",
    "        transformation = group['Transformation'].iloc[0]\n",
    "        if transformation == 'no_scaling':\n",
    "            group['Accuracy_Diff'] = 0\n",
    "            group['AUC_Diff'] = 0\n",
    "        else:\n",
    "            model = group['Model'].iloc[0]\n",
    "            group['Accuracy_Diff'] = group['Accuracy'] - no_scaling.loc[model, 'Accuracy']\n",
    "            group['AUC_Diff'] = group['AUC'] - no_scaling.loc[model, 'AUC']\n",
    "        return group\n",
    "    \n",
    "    return df.groupby(['Transformation', 'Model']).apply(diff).reset_index(drop=True)\n",
    "\n",
    "# Compare each method with no_scaling\n",
    "compared_results = compare_with_no_scaling(results_df)\n",
    "\n",
    "# Calculate average improvement for each transformation\n",
    "avg_improvement = compared_results[compared_results['Transformation'] != 'no_scaling'].groupby('Transformation').agg({\n",
    "    'Accuracy_Diff': 'mean',\n",
    "    'AUC_Diff': 'mean'\n",
    "}).sort_values('Accuracy_Diff', ascending=False)\n",
    "\n",
    "print(\"Average improvement by transformation:\")\n",
    "print(avg_improvement)\n",
    "\n",
    "# Find transformations that increase performance on average\n",
    "improved_transformations = avg_improvement[(avg_improvement['Accuracy_Diff'] > 0) | (avg_improvement['AUC_Diff'] > 0)]\n",
    "\n",
    "print(\"\\nTransformations that increase performance on average:\")\n",
    "print(improved_transformations)\n",
    "\n",
    "# Find the best transformation for each model\n",
    "best_transformations = compared_results[compared_results['Transformation'] != 'no_scaling'].groupby('Model').apply(\n",
    "    lambda x: x.loc[x['Accuracy_Diff'].idxmax()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBest transformation for each model:\")\n",
    "print(best_transformations[['Model', 'Transformation', 'Accuracy_Diff', 'AUC_Diff']])\n",
    "\n",
    "# Overall best performing model and transformation\n",
    "best_overall = compared_results.loc[compared_results['Accuracy'].idxmax()]\n",
    "print(\"\\nOverall best performing model and transformation:\")\n",
    "print(best_overall[['Transformation', 'Model', 'Accuracy', 'AUC', 'Accuracy_Diff', 'AUC_Diff']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
