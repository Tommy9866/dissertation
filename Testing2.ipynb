{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "chess_king_rook_vs_king_pawn = fetch_ucirepo(id=22) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = chess_king_rook_vs_king_pawn.data.features \n",
    "y = chess_king_rook_vs_king_pawn.data.targets   \n",
    "  \n",
    "# Combine X and y into a single DataFrame\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rename the target column to 'Y'\n",
    "df = df.rename(columns={df.columns[-1]: 'Y'})\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -25.224351\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -25.224351\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.136773  0.999999      0.001988            0.0   \n",
      "Ridge Regression     0.836741  0.999961      0.000997       0.000998   \n",
      "Lasso Regression     0.207878  0.999998      0.000997            0.0   \n",
      "ElasticNet           4.180667  0.999015      0.002025            0.0   \n",
      "Decision Tree       74.185527  0.689995      0.000998            0.0   \n",
      "Random Forest       72.050707   0.70758       0.27595        0.01502   \n",
      "Gradient Boosting   65.093976  0.761322       0.16496            0.0   \n",
      "XGBoost             56.539934   0.81993      0.065024       0.001979   \n",
      "LightGBM             143.6955 -0.163101      0.006974       0.000997   \n",
      "CatBoost           108.472498  0.337219      0.209005       0.000998   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         4.806936   \n",
      "Ridge Regression           0.05398   \n",
      "Lasso Regression          0.050011   \n",
      "ElasticNet                0.086986   \n",
      "Decision Tree             0.124971   \n",
      "Random Forest             4.793996   \n",
      "Gradient Boosting              2.1   \n",
      "XGBoost                    0.77401   \n",
      "LightGBM                  4.576046   \n",
      "CatBoost                  5.975939   \n",
      "\n",
      "                                               Best Parameters  \n",
      "Linear Regression                                           {}  \n",
      "Ridge Regression                                {'alpha': 0.1}  \n",
      "Lasso Regression                                {'alpha': 0.1}  \n",
      "ElasticNet                     {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                 {'max_depth': 7}  \n",
      "Random Forest            {'max_depth': 7, 'n_estimators': 100}  \n",
      "Gradient Boosting  {'learning_rate': 0.3, 'n_estimators': 200}  \n",
      "XGBoost             {'learning_rate': 0.1, 'n_estimators': 50}  \n",
      "LightGBM           {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost             {'iterations': 200, 'learning_rate': 0.3}  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary of regression models with their parameter grids\n",
    "models = {\n",
    "    'Linear Regression': (LinearRegression(), {}),\n",
    "    'Ridge Regression': (Ridge(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "    'Lasso Regression': (Lasso(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "    'ElasticNet': (ElasticNet(), {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8]}),\n",
    "    'Decision Tree': (DecisionTreeRegressor(), {'max_depth': [3, 5, 7]}),\n",
    "    'Random Forest': (RandomForestRegressor(), {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}),\n",
    "    'Gradient Boosting': (GradientBoostingRegressor(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.3]}),\n",
    "    'XGBoost': (XGBRegressor(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.3]}),\n",
    "    'LightGBM': (LGBMRegressor(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.3]}),\n",
    "    'CatBoost': (CatBoostRegressor(verbose=0), {'iterations': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.3]})\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Training time for best parameters\n",
    "    training_start = time.time()\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - training_start\n",
    "    \n",
    "    # Inference time for best parameters\n",
    "    inference_start = time.time()\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Computation time (total run time)\n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    results[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'R-squared': r2,\n",
    "        'Training Time': training_time,\n",
    "        'Inference Time': inference_time,\n",
    "        'Computation Time': computation_time,\n",
    "        'Best Parameters': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "result = pd.DataFrame(results).T  # Transpose to have models as rows\n",
    "\n",
    "# Display results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "\n",
      "Best Parameters:\n",
      "{'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the MLP model and parameter grid\n",
    "mlp = MLPRegressor(random_state=42, max_iter=1000)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Training time for best parameters\n",
    "training_start = time.time()\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "# Inference time for best parameters\n",
    "inference_start = time.time()\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_time = time.time() - inference_start\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "result.loc['MLP'] = [rmse, r2, training_time, inference_time, computation_time, grid_search.best_params_]\n",
    "\n",
    "# Display results\n",
    "print(result)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "\n",
      "Best Parameters:\n",
      "{'batch_size': 32, 'hidden_dims': [50], 'learning_rate': 0.0001, 'num_epochs': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "72 fits failed out of a total of 216.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "72 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3484552703.py\", line 64, in fit\n",
      "ValueError: could not determine the shape of object type 'Series'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(DNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DNNRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dims, learning_rate=0.001, batch_size=32, num_epochs=100):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = DNN(input_dim, hidden_dims).to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(X).to(device), torch.FloatTensor(y).unsqueeze(1).to(device))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(device)\n",
    "            predictions = self.model(X_tensor)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_dims': [[50], [100], [50, 50], [100, 50]],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'num_epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Create the DNNRegressor instance\n",
    "dnn_regressor = DNNRegressor(input_dim=X_train.shape[1], hidden_dims=[50])\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(dnn_regressor, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Training time for best parameters\n",
    "training_start = time.time()\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "# Inference time for best parameters\n",
    "inference_start = time.time()\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_time = time.time() - inference_start\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Assuming result is your existing DataFrame\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DNN'] = [rmse, r2, training_time, inference_time, computation_time, grid_search.best_params_]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:08:56,710] A new study created in memory with name: no-name-182fff70-c98f-4851-b413-5a94b6c913be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:57,040] Trial 0 finished with value: 9.475296020507812 and parameters: {'cross_layers': 4, 'deep_layer_0': 225, 'deep_layer_1': 76, 'deep_layer_2': 127, 'learning_rate': 0.03614504961249343, 'batch_size': 64, 'num_epochs': 42}. Best is trial 0 with value: 9.475296020507812.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:57,304] Trial 1 finished with value: 94.39893341064453 and parameters: {'cross_layers': 5, 'deep_layer_0': 36, 'deep_layer_1': 124, 'deep_layer_2': 55, 'learning_rate': 0.002292692342985842, 'batch_size': 256, 'num_epochs': 38}. Best is trial 0 with value: 9.475296020507812.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:57,832] Trial 2 finished with value: 67.55377197265625 and parameters: {'cross_layers': 5, 'deep_layer_0': 66, 'deep_layer_1': 210, 'deep_layer_2': 212, 'learning_rate': 0.0005842933229423102, 'batch_size': 32, 'num_epochs': 71}. Best is trial 0 with value: 9.475296020507812.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:58,113] Trial 3 finished with value: 101.90653991699219 and parameters: {'cross_layers': 1, 'deep_layer_0': 184, 'deep_layer_1': 184, 'deep_layer_2': 54, 'learning_rate': 0.0005356635239981444, 'batch_size': 64, 'num_epochs': 53}. Best is trial 0 with value: 9.475296020507812.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:58,326] Trial 4 finished with value: 6.90287971496582 and parameters: {'cross_layers': 2, 'deep_layer_0': 255, 'deep_layer_1': 226, 'deep_layer_2': 95, 'learning_rate': 0.017068470204677322, 'batch_size': 256, 'num_epochs': 34}. Best is trial 4 with value: 6.90287971496582.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:58,420] Trial 5 finished with value: 21.472036361694336 and parameters: {'cross_layers': 1, 'deep_layer_0': 160, 'deep_layer_1': 143, 'deep_layer_2': 195, 'learning_rate': 0.011791211689316187, 'batch_size': 32, 'num_epochs': 15}. Best is trial 4 with value: 6.90287971496582.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:58,683] Trial 6 finished with value: 110.31954956054688 and parameters: {'cross_layers': 2, 'deep_layer_0': 105, 'deep_layer_1': 133, 'deep_layer_2': 71, 'learning_rate': 0.00016801932960482495, 'batch_size': 64, 'num_epochs': 49}. Best is trial 4 with value: 6.90287971496582.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:58,826] Trial 7 finished with value: 24.468673706054688 and parameters: {'cross_layers': 4, 'deep_layer_0': 160, 'deep_layer_1': 167, 'deep_layer_2': 159, 'learning_rate': 0.08116234229420964, 'batch_size': 64, 'num_epochs': 24}. Best is trial 4 with value: 6.90287971496582.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:59,322] Trial 8 finished with value: 98.53482055664062 and parameters: {'cross_layers': 5, 'deep_layer_0': 200, 'deep_layer_1': 100, 'deep_layer_2': 133, 'learning_rate': 0.00041399001224610234, 'batch_size': 128, 'num_epochs': 68}. Best is trial 4 with value: 6.90287971496582.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:08:59,860] Trial 9 finished with value: 4.86602783203125 and parameters: {'cross_layers': 3, 'deep_layer_0': 201, 'deep_layer_1': 139, 'deep_layer_2': 64, 'learning_rate': 0.001926334056728195, 'batch_size': 32, 'num_epochs': 91}. Best is trial 9 with value: 4.86602783203125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:00,507] Trial 10 finished with value: 5.174714088439941 and parameters: {'cross_layers': 3, 'deep_layer_0': 114, 'deep_layer_1': 60, 'deep_layer_2': 247, 'learning_rate': 0.0032590780117479246, 'batch_size': 32, 'num_epochs': 100}. Best is trial 9 with value: 4.86602783203125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:01,085] Trial 11 finished with value: 7.626889705657959 and parameters: {'cross_layers': 3, 'deep_layer_0': 111, 'deep_layer_1': 46, 'deep_layer_2': 243, 'learning_rate': 0.002827965871082962, 'batch_size': 32, 'num_epochs': 99}. Best is trial 9 with value: 4.86602783203125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:01,647] Trial 12 finished with value: 9.5353422164917 and parameters: {'cross_layers': 3, 'deep_layer_0': 110, 'deep_layer_1': 44, 'deep_layer_2': 256, 'learning_rate': 0.005190528038398635, 'batch_size': 32, 'num_epochs': 96}. Best is trial 9 with value: 4.86602783203125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:02,089] Trial 13 finished with value: 8.24738883972168 and parameters: {'cross_layers': 2, 'deep_layer_0': 130, 'deep_layer_1': 91, 'deep_layer_2': 180, 'learning_rate': 0.0014311673442859156, 'batch_size': 32, 'num_epochs': 84}. Best is trial 9 with value: 4.86602783203125.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:02,719] Trial 14 finished with value: 2.59476900100708 and parameters: {'cross_layers': 4, 'deep_layer_0': 204, 'deep_layer_1': 178, 'deep_layer_2': 100, 'learning_rate': 0.006320725321987375, 'batch_size': 128, 'num_epochs': 83}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:03,298] Trial 15 finished with value: 6.350270748138428 and parameters: {'cross_layers': 4, 'deep_layer_0': 211, 'deep_layer_1': 177, 'deep_layer_2': 93, 'learning_rate': 0.00753795360602924, 'batch_size': 128, 'num_epochs': 80}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:03,921] Trial 16 finished with value: 22.10601806640625 and parameters: {'cross_layers': 4, 'deep_layer_0': 250, 'deep_layer_1': 202, 'deep_layer_2': 32, 'learning_rate': 0.0013462040714262648, 'batch_size': 128, 'num_epochs': 85}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:04,441] Trial 17 finished with value: 5.789754867553711 and parameters: {'cross_layers': 4, 'deep_layer_0': 184, 'deep_layer_1': 239, 'deep_layer_2': 105, 'learning_rate': 0.022681736512811707, 'batch_size': 128, 'num_epochs': 63}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:05,005] Trial 18 finished with value: 107.49608612060547 and parameters: {'cross_layers': 3, 'deep_layer_0': 227, 'deep_layer_1': 159, 'deep_layer_2': 112, 'learning_rate': 0.0001344508935390691, 'batch_size': 128, 'num_epochs': 77}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:05,560] Trial 19 finished with value: 3.8135340213775635 and parameters: {'cross_layers': 2, 'deep_layer_0': 168, 'deep_layer_1': 118, 'deep_layer_2': 76, 'learning_rate': 0.0074619627393432844, 'batch_size': 256, 'num_epochs': 89}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:05,899] Trial 20 finished with value: 8.705162048339844 and parameters: {'cross_layers': 2, 'deep_layer_0': 145, 'deep_layer_1': 111, 'deep_layer_2': 80, 'learning_rate': 0.04729897863619996, 'batch_size': 256, 'num_epochs': 60}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:06,457] Trial 21 finished with value: 3.5062198638916016 and parameters: {'cross_layers': 2, 'deep_layer_0': 182, 'deep_layer_1': 149, 'deep_layer_2': 37, 'learning_rate': 0.006808784009543505, 'batch_size': 256, 'num_epochs': 93}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:06,951] Trial 22 finished with value: 6.56850004196167 and parameters: {'cross_layers': 1, 'deep_layer_0': 176, 'deep_layer_1': 155, 'deep_layer_2': 32, 'learning_rate': 0.008579021763401388, 'batch_size': 256, 'num_epochs': 90}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:07,562] Trial 23 finished with value: 5.917472839355469 and parameters: {'cross_layers': 2, 'deep_layer_0': 164, 'deep_layer_1': 193, 'deep_layer_2': 48, 'learning_rate': 0.00499003744548531, 'batch_size': 256, 'num_epochs': 74}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:08,137] Trial 24 finished with value: 3.860020875930786 and parameters: {'cross_layers': 2, 'deep_layer_0': 139, 'deep_layer_1': 114, 'deep_layer_2': 85, 'learning_rate': 0.014125591033811148, 'batch_size': 256, 'num_epochs': 90}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:08,594] Trial 25 finished with value: 5.145963668823242 and parameters: {'cross_layers': 1, 'deep_layer_0': 226, 'deep_layer_1': 82, 'deep_layer_2': 117, 'learning_rate': 0.005207830615602138, 'batch_size': 256, 'num_epochs': 84}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:09,330] Trial 26 finished with value: 6.3127641677856445 and parameters: {'cross_layers': 3, 'deep_layer_0': 197, 'deep_layer_1': 171, 'deep_layer_2': 160, 'learning_rate': 0.02568736929987525, 'batch_size': 256, 'num_epochs': 92}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:09,767] Trial 27 finished with value: 7.200280666351318 and parameters: {'cross_layers': 2, 'deep_layer_0': 175, 'deep_layer_1': 219, 'deep_layer_2': 75, 'learning_rate': 0.008871981056326032, 'batch_size': 128, 'num_epochs': 64}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:10,204] Trial 28 finished with value: 37.618927001953125 and parameters: {'cross_layers': 1, 'deep_layer_0': 240, 'deep_layer_1': 125, 'deep_layer_2': 45, 'learning_rate': 0.000931385130091629, 'batch_size': 256, 'num_epochs': 78}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:10,834] Trial 29 finished with value: 5.789892196655273 and parameters: {'cross_layers': 3, 'deep_layer_0': 213, 'deep_layer_1': 152, 'deep_layer_2': 131, 'learning_rate': 0.043450033418998515, 'batch_size': 256, 'num_epochs': 83}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:11,514] Trial 30 finished with value: 5.635142803192139 and parameters: {'cross_layers': 4, 'deep_layer_0': 82, 'deep_layer_1': 72, 'deep_layer_2': 99, 'learning_rate': 0.004113558472132238, 'batch_size': 128, 'num_epochs': 95}. Best is trial 14 with value: 2.59476900100708.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:12,030] Trial 31 finished with value: 1.3206959962844849 and parameters: {'cross_layers': 2, 'deep_layer_0': 148, 'deep_layer_1': 108, 'deep_layer_2': 86, 'learning_rate': 0.014445126277881372, 'batch_size': 256, 'num_epochs': 89}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:12,578] Trial 32 finished with value: 1.9661765098571777 and parameters: {'cross_layers': 2, 'deep_layer_0': 146, 'deep_layer_1': 109, 'deep_layer_2': 65, 'learning_rate': 0.029177258862302945, 'batch_size': 256, 'num_epochs': 88}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:13,120] Trial 33 finished with value: 8.864716529846191 and parameters: {'cross_layers': 5, 'deep_layer_0': 147, 'deep_layer_1': 102, 'deep_layer_2': 62, 'learning_rate': 0.026555539914012594, 'batch_size': 256, 'num_epochs': 69}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:13,603] Trial 34 finished with value: 6.0106353759765625 and parameters: {'cross_layers': 2, 'deep_layer_0': 133, 'deep_layer_1': 137, 'deep_layer_2': 50, 'learning_rate': 0.07130720087070777, 'batch_size': 256, 'num_epochs': 74}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:13,933] Trial 35 finished with value: 15.104378700256348 and parameters: {'cross_layers': 1, 'deep_layer_0': 188, 'deep_layer_1': 188, 'deep_layer_2': 122, 'learning_rate': 0.02000483267561061, 'batch_size': 64, 'num_epochs': 46}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:14,459] Trial 36 finished with value: 5.965147972106934 and parameters: {'cross_layers': 2, 'deep_layer_0': 33, 'deep_layer_1': 125, 'deep_layer_2': 61, 'learning_rate': 0.011843980071533604, 'batch_size': 256, 'num_epochs': 95}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:14,953] Trial 37 finished with value: 12.158634185791016 and parameters: {'cross_layers': 2, 'deep_layer_0': 152, 'deep_layer_1': 71, 'deep_layer_2': 86, 'learning_rate': 0.056469651889200646, 'batch_size': 256, 'num_epochs': 87}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:15,252] Trial 38 finished with value: 10.532183647155762 and parameters: {'cross_layers': 5, 'deep_layer_0': 127, 'deep_layer_1': 101, 'deep_layer_2': 39, 'learning_rate': 0.028907342277058167, 'batch_size': 64, 'num_epochs': 31}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:15,611] Trial 39 finished with value: 4.481373310089111 and parameters: {'cross_layers': 1, 'deep_layer_0': 154, 'deep_layer_1': 256, 'deep_layer_2': 106, 'learning_rate': 0.017699256596458303, 'batch_size': 256, 'num_epochs': 57}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:16,981] Trial 40 finished with value: 3.490036725997925 and parameters: {'cross_layers': 3, 'deep_layer_0': 214, 'deep_layer_1': 152, 'deep_layer_2': 58, 'learning_rate': 0.011600609720542644, 'batch_size': 128, 'num_epochs': 80}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:17,638] Trial 41 finished with value: 1.695832371711731 and parameters: {'cross_layers': 3, 'deep_layer_0': 211, 'deep_layer_1': 147, 'deep_layer_2': 67, 'learning_rate': 0.01129083983638429, 'batch_size': 128, 'num_epochs': 80}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:18,176] Trial 42 finished with value: 3.3344991207122803 and parameters: {'cross_layers': 3, 'deep_layer_0': 50, 'deep_layer_1': 164, 'deep_layer_2': 66, 'learning_rate': 0.01220181718317882, 'batch_size': 128, 'num_epochs': 73}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:18,758] Trial 43 finished with value: 4.499431133270264 and parameters: {'cross_layers': 4, 'deep_layer_0': 67, 'deep_layer_1': 166, 'deep_layer_2': 94, 'learning_rate': 0.035434583463094825, 'batch_size': 128, 'num_epochs': 74}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:19,205] Trial 44 finished with value: 4.122987747192383 and parameters: {'cross_layers': 3, 'deep_layer_0': 49, 'deep_layer_1': 131, 'deep_layer_2': 65, 'learning_rate': 0.012753591963668641, 'batch_size': 128, 'num_epochs': 67}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:19,797] Trial 45 finished with value: 2.9054906368255615 and parameters: {'cross_layers': 3, 'deep_layer_0': 87, 'deep_layer_1': 202, 'deep_layer_2': 70, 'learning_rate': 0.016529009021147432, 'batch_size': 128, 'num_epochs': 81}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:20,455] Trial 46 finished with value: 7.382887363433838 and parameters: {'cross_layers': 3, 'deep_layer_0': 95, 'deep_layer_1': 179, 'deep_layer_2': 86, 'learning_rate': 0.033849283629384185, 'batch_size': 128, 'num_epochs': 99}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:21,030] Trial 47 finished with value: 7.317471027374268 and parameters: {'cross_layers': 4, 'deep_layer_0': 92, 'deep_layer_1': 210, 'deep_layer_2': 74, 'learning_rate': 0.0179579403431984, 'batch_size': 128, 'num_epochs': 81}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:21,210] Trial 48 finished with value: 63.46274185180664 and parameters: {'cross_layers': 5, 'deep_layer_0': 239, 'deep_layer_1': 202, 'deep_layer_2': 153, 'learning_rate': 0.08913398395646346, 'batch_size': 128, 'num_epochs': 11}. Best is trial 31 with value: 1.3206959962844849.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\95374287.py:79: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:09:21,793] Trial 49 finished with value: 9.322080612182617 and parameters: {'cross_layers': 3, 'deep_layer_0': 123, 'deep_layer_1': 87, 'deep_layer_2': 54, 'learning_rate': 0.002676218129750107, 'batch_size': 64, 'num_epochs': 87}. Best is trial 31 with value: 1.3206959962844849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "cross_layers: 2\n",
      "deep_layer_0: 148\n",
      "deep_layer_1: 108\n",
      "deep_layer_2: 86\n",
      "learning_rate: 0.014445126277881372\n",
      "batch_size: 256\n",
      "num_epochs: 89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x0, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x0 = x0.unsqueeze(2)\n",
    "        interaction = torch.matmul(x0, torch.matmul(x.transpose(1, 2), self.weight))\n",
    "        return x0.squeeze(2) + interaction.squeeze(2) + self.bias.T\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, input_dim, cross_layers, deep_layers):\n",
    "        super(DCN, self).__init__()\n",
    "        self.cross_layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(cross_layers)])\n",
    "        \n",
    "        deep_dims = [input_dim] + deep_layers\n",
    "        self.deep_layers = nn.ModuleList([nn.Linear(deep_dims[i], deep_dims[i+1]) for i in range(len(deep_layers))])\n",
    "        self.deep_activation = nn.ReLU()\n",
    "        \n",
    "        self.final_layer = nn.Linear(deep_layers[-1] + input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        cross_out = x\n",
    "        for layer in self.cross_layers:\n",
    "            cross_out = layer(x0, cross_out)\n",
    "        \n",
    "        deep_out = x\n",
    "        for layer in self.deep_layers:\n",
    "            deep_out = self.deep_activation(layer(deep_out))\n",
    "        \n",
    "        combined = torch.cat([cross_out, deep_out], dim=1)\n",
    "        return self.final_layer(combined)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    cross_layers = trial.suggest_int('cross_layers', 1, 5)\n",
    "    deep_layers = [trial.suggest_int(f'deep_layer_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the DCN model\n",
    "    model = DCN(X_train.shape[1], cross_layers, deep_layers).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final DCN model with the best hyperparameters\n",
    "best_model = DCN(X_train.shape[1], \n",
    "                 best_params['cross_layers'], \n",
    "                 [best_params[f'deep_layer_{i}'] for i in range(3)]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DCN'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:10:15,159] A new study created in memory with name: no-name-c86b482c-8e21-4561-bf41-f15418b31c04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:15,432] Trial 0 finished with value: 6.078895092010498 and parameters: {'wide_dim': 92, 'n_deep_layers': 3, 'deep_dim_0': 142, 'deep_dim_1': 37, 'deep_dim_2': 207, 'learning_rate': 0.0791549434665011, 'batch_size': 64, 'num_epochs': 53}. Best is trial 0 with value: 6.078895092010498.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:15,678] Trial 1 finished with value: 2.9665329456329346 and parameters: {'wide_dim': 114, 'n_deep_layers': 1, 'deep_dim_0': 205, 'learning_rate': 0.027147206811385417, 'batch_size': 128, 'num_epochs': 92}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:15,818] Trial 2 finished with value: 81.11412811279297 and parameters: {'wide_dim': 89, 'n_deep_layers': 1, 'deep_dim_0': 177, 'learning_rate': 0.005431418025440753, 'batch_size': 256, 'num_epochs': 68}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:16,436] Trial 3 finished with value: 5.459197521209717 and parameters: {'wide_dim': 32, 'n_deep_layers': 4, 'deep_dim_0': 111, 'deep_dim_1': 174, 'deep_dim_2': 166, 'deep_dim_3': 87, 'learning_rate': 0.02989890484324296, 'batch_size': 64, 'num_epochs': 91}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:16,603] Trial 4 finished with value: 109.96965026855469 and parameters: {'wide_dim': 103, 'n_deep_layers': 5, 'deep_dim_0': 69, 'deep_dim_1': 59, 'deep_dim_2': 232, 'deep_dim_3': 225, 'deep_dim_4': 197, 'learning_rate': 0.0007978726079364803, 'batch_size': 128, 'num_epochs': 15}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:16,739] Trial 5 finished with value: 110.32549285888672 and parameters: {'wide_dim': 126, 'n_deep_layers': 2, 'deep_dim_0': 158, 'deep_dim_1': 53, 'learning_rate': 0.00031960696429798093, 'batch_size': 32, 'num_epochs': 22}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:17,105] Trial 6 finished with value: 38.2297477722168 and parameters: {'wide_dim': 11, 'n_deep_layers': 4, 'deep_dim_0': 199, 'deep_dim_1': 250, 'deep_dim_2': 214, 'deep_dim_3': 108, 'learning_rate': 0.05379560701289437, 'batch_size': 256, 'num_epochs': 53}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:17,529] Trial 7 finished with value: 6.88093376159668 and parameters: {'wide_dim': 109, 'n_deep_layers': 5, 'deep_dim_0': 99, 'deep_dim_1': 88, 'deep_dim_2': 67, 'deep_dim_3': 201, 'deep_dim_4': 143, 'learning_rate': 0.0036290845160881087, 'batch_size': 256, 'num_epochs': 63}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:18,211] Trial 8 finished with value: 7.941153526306152 and parameters: {'wide_dim': 77, 'n_deep_layers': 5, 'deep_dim_0': 235, 'deep_dim_1': 240, 'deep_dim_2': 154, 'deep_dim_3': 72, 'deep_dim_4': 133, 'learning_rate': 0.0004733915081074997, 'batch_size': 256, 'num_epochs': 86}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:18,493] Trial 9 finished with value: 110.62481689453125 and parameters: {'wide_dim': 37, 'n_deep_layers': 5, 'deep_dim_0': 198, 'deep_dim_1': 93, 'deep_dim_2': 146, 'deep_dim_3': 99, 'deep_dim_4': 94, 'learning_rate': 0.00015588276704114274, 'batch_size': 128, 'num_epochs': 41}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:18,861] Trial 10 finished with value: 6.453968048095703 and parameters: {'wide_dim': 56, 'n_deep_layers': 1, 'deep_dim_0': 256, 'learning_rate': 0.014086010642836778, 'batch_size': 128, 'num_epochs': 100}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:19,367] Trial 11 finished with value: 7.300862789154053 and parameters: {'wide_dim': 48, 'n_deep_layers': 3, 'deep_dim_0': 111, 'deep_dim_1': 169, 'deep_dim_2': 67, 'learning_rate': 0.021508696375513368, 'batch_size': 64, 'num_epochs': 82}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:20,675] Trial 12 finished with value: 3.5522210597991943 and parameters: {'wide_dim': 20, 'n_deep_layers': 2, 'deep_dim_0': 105, 'deep_dim_1': 181, 'learning_rate': 0.020667261546092067, 'batch_size': 64, 'num_epochs': 100}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:21,067] Trial 13 finished with value: 5.563155651092529 and parameters: {'wide_dim': 14, 'n_deep_layers': 2, 'deep_dim_0': 39, 'deep_dim_1': 206, 'learning_rate': 0.008710102694770398, 'batch_size': 32, 'num_epochs': 97}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:21,406] Trial 14 finished with value: 74.98186492919922 and parameters: {'wide_dim': 69, 'n_deep_layers': 2, 'deep_dim_0': 142, 'deep_dim_1': 124, 'learning_rate': 0.0015614045221450548, 'batch_size': 64, 'num_epochs': 75}. Best is trial 1 with value: 2.9665329456329346.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:21,725] Trial 15 finished with value: 1.456606149673462 and parameters: {'wide_dim': 123, 'n_deep_layers': 1, 'deep_dim_0': 222, 'learning_rate': 0.04630318046083572, 'batch_size': 128, 'num_epochs': 81}. Best is trial 15 with value: 1.456606149673462.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:22,068] Trial 16 finished with value: 2.447002649307251 and parameters: {'wide_dim': 125, 'n_deep_layers': 1, 'deep_dim_0': 218, 'learning_rate': 0.08577629074883589, 'batch_size': 128, 'num_epochs': 79}. Best is trial 15 with value: 1.456606149673462.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:22,251] Trial 17 finished with value: 2.285641670227051 and parameters: {'wide_dim': 128, 'n_deep_layers': 1, 'deep_dim_0': 233, 'learning_rate': 0.09318139531392179, 'batch_size': 128, 'num_epochs': 74}. Best is trial 15 with value: 1.456606149673462.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:22,350] Trial 18 finished with value: 13.263693809509277 and parameters: {'wide_dim': 94, 'n_deep_layers': 1, 'deep_dim_0': 246, 'learning_rate': 0.04783447460601553, 'batch_size': 128, 'num_epochs': 39}. Best is trial 15 with value: 1.456606149673462.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:22,649] Trial 19 finished with value: 5.239173889160156 and parameters: {'wide_dim': 124, 'n_deep_layers': 2, 'deep_dim_0': 224, 'deep_dim_1': 116, 'learning_rate': 0.009471734669004814, 'batch_size': 128, 'num_epochs': 68}. Best is trial 15 with value: 1.456606149673462.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:22,978] Trial 20 finished with value: 0.6555135250091553 and parameters: {'wide_dim': 113, 'n_deep_layers': 3, 'deep_dim_0': 175, 'deep_dim_1': 223, 'deep_dim_2': 43, 'learning_rate': 0.09837591075095582, 'batch_size': 128, 'num_epochs': 59}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:23,320] Trial 21 finished with value: 2.9348556995391846 and parameters: {'wide_dim': 114, 'n_deep_layers': 3, 'deep_dim_0': 177, 'deep_dim_1': 223, 'deep_dim_2': 35, 'learning_rate': 0.09943560491066074, 'batch_size': 128, 'num_epochs': 60}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:23,786] Trial 22 finished with value: 5.626152038574219 and parameters: {'wide_dim': 104, 'n_deep_layers': 4, 'deep_dim_0': 178, 'deep_dim_1': 207, 'deep_dim_2': 94, 'deep_dim_3': 166, 'learning_rate': 0.0387141577472659, 'batch_size': 128, 'num_epochs': 74}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:23,929] Trial 23 finished with value: 3.752359628677368 and parameters: {'wide_dim': 128, 'n_deep_layers': 1, 'deep_dim_0': 221, 'learning_rate': 0.053130236517146205, 'batch_size': 128, 'num_epochs': 44}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:24,221] Trial 24 finished with value: 2.7801830768585205 and parameters: {'wide_dim': 117, 'n_deep_layers': 2, 'deep_dim_0': 240, 'deep_dim_1': 152, 'learning_rate': 0.09680301275040154, 'batch_size': 32, 'num_epochs': 68}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:24,548] Trial 25 finished with value: 4.225579738616943 and parameters: {'wide_dim': 100, 'n_deep_layers': 3, 'deep_dim_0': 163, 'deep_dim_1': 205, 'deep_dim_2': 103, 'learning_rate': 0.017464110657930354, 'batch_size': 128, 'num_epochs': 58}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:24,853] Trial 26 finished with value: 16.8971004486084 and parameters: {'wide_dim': 81, 'n_deep_layers': 4, 'deep_dim_0': 195, 'deep_dim_1': 145, 'deep_dim_2': 33, 'deep_dim_3': 39, 'learning_rate': 0.0019623471110084313, 'batch_size': 128, 'num_epochs': 50}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:24,984] Trial 27 finished with value: 18.389583587646484 and parameters: {'wide_dim': 118, 'n_deep_layers': 1, 'deep_dim_0': 212, 'learning_rate': 0.053034951836643085, 'batch_size': 128, 'num_epochs': 33}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:25,460] Trial 28 finished with value: 4.710514545440674 and parameters: {'wide_dim': 111, 'n_deep_layers': 2, 'deep_dim_0': 256, 'deep_dim_1': 256, 'learning_rate': 0.011534308229625043, 'batch_size': 128, 'num_epochs': 84}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:25,782] Trial 29 finished with value: 6.039952278137207 and parameters: {'wide_dim': 88, 'n_deep_layers': 3, 'deep_dim_0': 131, 'deep_dim_1': 221, 'deep_dim_2': 113, 'learning_rate': 0.05998178439146216, 'batch_size': 32, 'num_epochs': 48}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:26,021] Trial 30 finished with value: 1.5578688383102417 and parameters: {'wide_dim': 120, 'n_deep_layers': 1, 'deep_dim_0': 186, 'learning_rate': 0.03109169646597973, 'batch_size': 128, 'num_epochs': 74}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:26,222] Trial 31 finished with value: 2.6307148933410645 and parameters: {'wide_dim': 120, 'n_deep_layers': 1, 'deep_dim_0': 185, 'learning_rate': 0.03672635487858524, 'batch_size': 128, 'num_epochs': 75}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:26,380] Trial 32 finished with value: 2.091747999191284 and parameters: {'wide_dim': 108, 'n_deep_layers': 1, 'deep_dim_0': 156, 'learning_rate': 0.0735724975785897, 'batch_size': 128, 'num_epochs': 64}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:26,558] Trial 33 finished with value: 2.997659683227539 and parameters: {'wide_dim': 98, 'n_deep_layers': 1, 'deep_dim_0': 158, 'learning_rate': 0.028168364513797877, 'batch_size': 128, 'num_epochs': 66}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:26,708] Trial 34 finished with value: 87.5800552368164 and parameters: {'wide_dim': 109, 'n_deep_layers': 1, 'deep_dim_0': 166, 'learning_rate': 0.0054243430054488175, 'batch_size': 128, 'num_epochs': 56}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:27,126] Trial 35 finished with value: 8.400964736938477 and parameters: {'wide_dim': 105, 'n_deep_layers': 2, 'deep_dim_0': 134, 'deep_dim_1': 232, 'learning_rate': 0.03037710465563824, 'batch_size': 128, 'num_epochs': 88}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:27,316] Trial 36 finished with value: 1.769368290901184 and parameters: {'wide_dim': 87, 'n_deep_layers': 1, 'deep_dim_0': 190, 'learning_rate': 0.07020912257365441, 'batch_size': 256, 'num_epochs': 62}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:27,650] Trial 37 finished with value: 7.117736339569092 and parameters: {'wide_dim': 85, 'n_deep_layers': 2, 'deep_dim_0': 189, 'deep_dim_1': 194, 'learning_rate': 0.040588997858285646, 'batch_size': 256, 'num_epochs': 70}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:28,336] Trial 38 finished with value: 58.08317565917969 and parameters: {'wide_dim': 94, 'n_deep_layers': 4, 'deep_dim_0': 213, 'deep_dim_1': 120, 'deep_dim_2': 179, 'deep_dim_3': 245, 'learning_rate': 0.0671946668384175, 'batch_size': 256, 'num_epochs': 79}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:28,673] Trial 39 finished with value: 3.522061347961426 and parameters: {'wide_dim': 74, 'n_deep_layers': 1, 'deep_dim_0': 200, 'learning_rate': 0.02495233806558463, 'batch_size': 256, 'num_epochs': 92}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:28,970] Trial 40 finished with value: 11.044641494750977 and parameters: {'wide_dim': 55, 'n_deep_layers': 2, 'deep_dim_0': 184, 'deep_dim_1': 90, 'learning_rate': 0.0053552924813877354, 'batch_size': 256, 'num_epochs': 62}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:29,171] Trial 41 finished with value: 2.498439311981201 and parameters: {'wide_dim': 120, 'n_deep_layers': 1, 'deep_dim_0': 153, 'learning_rate': 0.060601288228067006, 'batch_size': 256, 'num_epochs': 64}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:29,324] Trial 42 finished with value: 5.377795696258545 and parameters: {'wide_dim': 110, 'n_deep_layers': 1, 'deep_dim_0': 169, 'learning_rate': 0.07113324894605849, 'batch_size': 128, 'num_epochs': 56}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:29,461] Trial 43 finished with value: 11.427762031555176 and parameters: {'wide_dim': 104, 'n_deep_layers': 1, 'deep_dim_0': 149, 'learning_rate': 0.03585889198807891, 'batch_size': 64, 'num_epochs': 51}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:29,641] Trial 44 finished with value: 5.23545503616333 and parameters: {'wide_dim': 114, 'n_deep_layers': 1, 'deep_dim_0': 173, 'learning_rate': 0.017850037727427322, 'batch_size': 256, 'num_epochs': 80}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:29,785] Trial 45 finished with value: 3.20971941947937 and parameters: {'wide_dim': 99, 'n_deep_layers': 1, 'deep_dim_0': 121, 'learning_rate': 0.07511968925587835, 'batch_size': 32, 'num_epochs': 60}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:30,152] Trial 46 finished with value: 5.1539530754089355 and parameters: {'wide_dim': 122, 'n_deep_layers': 2, 'deep_dim_0': 208, 'deep_dim_1': 190, 'learning_rate': 0.044356207460517126, 'batch_size': 128, 'num_epochs': 71}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:30,345] Trial 47 finished with value: 92.54086303710938 and parameters: {'wide_dim': 115, 'n_deep_layers': 5, 'deep_dim_0': 93, 'deep_dim_1': 166, 'deep_dim_2': 124, 'deep_dim_3': 164, 'deep_dim_4': 246, 'learning_rate': 0.02818265892119783, 'batch_size': 64, 'num_epochs': 14}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:30,551] Trial 48 finished with value: 7.8687825202941895 and parameters: {'wide_dim': 91, 'n_deep_layers': 1, 'deep_dim_0': 193, 'learning_rate': 0.014492681243498761, 'batch_size': 128, 'num_epochs': 64}. Best is trial 20 with value: 0.6555135250091553.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\3817342284.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:10:30,931] Trial 49 finished with value: 106.41236114501953 and parameters: {'wide_dim': 66, 'n_deep_layers': 2, 'deep_dim_0': 148, 'deep_dim_1': 129, 'learning_rate': 0.0003627844189873134, 'batch_size': 256, 'num_epochs': 88}. Best is trial 20 with value: 0.6555135250091553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "wide_dim: 113\n",
      "n_deep_layers: 3\n",
      "deep_dim_0: 175\n",
      "deep_dim_1: 223\n",
      "deep_dim_2: 43\n",
      "learning_rate: 0.09837591075095582\n",
      "batch_size: 128\n",
      "num_epochs: 59\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class WideAndDeepNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, wide_dim, deep_dims):\n",
    "        super(WideAndDeepNetwork, self).__init__()\n",
    "        self.wide = nn.Linear(input_dim, wide_dim)\n",
    "        \n",
    "        deep_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in deep_dims:\n",
    "            deep_layers.append(nn.Linear(prev_dim, dim))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        self.deep = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        self.final = nn.Linear(wide_dim + deep_dims[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        wide_out = self.wide(x)\n",
    "        deep_out = self.deep(x)\n",
    "        combined = torch.cat([wide_out, deep_out], dim=1)\n",
    "        return self.final(combined)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    wide_dim = trial.suggest_int('wide_dim', 8, 128)\n",
    "    n_deep_layers = trial.suggest_int('n_deep_layers', 1, 5)\n",
    "    deep_dims = [trial.suggest_int(f'deep_dim_{i}', 32, 256) for i in range(n_deep_layers)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Wide & Deep model\n",
    "    model = WideAndDeepNetwork(X_train.shape[1], wide_dim, deep_dims).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final Wide & Deep model with the best hyperparameters\n",
    "best_model = WideAndDeepNetwork(X_train.shape[1], \n",
    "                                best_params['wide_dim'], \n",
    "                                [best_params[f'deep_dim_{i}'] for i in range(best_params['n_deep_layers'])]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['Wide & Deep'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:11:32,383] A new study created in memory with name: no-name-02df36d3-dd1e-4742-a6eb-fd488a87cb4a\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:32,519] Trial 0 finished with value: 100.7371826171875 and parameters: {'xgb_n_estimators': 65, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.00012616736217413617, 'xgb_subsample': 0.6620903178422187, 'xgb_colsample_bytree': 0.6655663978982062, 'nn_n_layers': 3, 'nn_hidden_dim_0': 104, 'nn_hidden_dim_1': 191, 'nn_hidden_dim_2': 237, 'nn_learning_rate': 0.04983956435788168, 'nn_batch_size': 32, 'nn_num_epochs': 16}. Best is trial 0 with value: 100.7371826171875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:32,937] Trial 1 finished with value: 57.06631088256836 and parameters: {'xgb_n_estimators': 81, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.031570465181887185, 'xgb_subsample': 0.6432306473108582, 'xgb_colsample_bytree': 0.6656993623987824, 'nn_n_layers': 3, 'nn_hidden_dim_0': 252, 'nn_hidden_dim_1': 103, 'nn_hidden_dim_2': 253, 'nn_learning_rate': 0.012292455085503839, 'nn_batch_size': 64, 'nn_num_epochs': 66}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:33,379] Trial 2 finished with value: 87.11090850830078 and parameters: {'xgb_n_estimators': 186, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.0014172687711339236, 'xgb_subsample': 0.5143699991637937, 'xgb_colsample_bytree': 0.6621413500617791, 'nn_n_layers': 3, 'nn_hidden_dim_0': 219, 'nn_hidden_dim_1': 67, 'nn_hidden_dim_2': 157, 'nn_learning_rate': 0.004258391093813496, 'nn_batch_size': 64, 'nn_num_epochs': 75}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:33,823] Trial 3 finished with value: 66.4078140258789 and parameters: {'xgb_n_estimators': 113, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.010161164822042962, 'xgb_subsample': 0.6547875902636062, 'xgb_colsample_bytree': 0.999619545728551, 'nn_n_layers': 3, 'nn_hidden_dim_0': 40, 'nn_hidden_dim_1': 69, 'nn_hidden_dim_2': 77, 'nn_learning_rate': 0.0010487019690318528, 'nn_batch_size': 32, 'nn_num_epochs': 98}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:34,165] Trial 4 finished with value: 94.75475311279297 and parameters: {'xgb_n_estimators': 139, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.02313345528753526, 'xgb_subsample': 0.5404939574922678, 'xgb_colsample_bytree': 0.8895067330976528, 'nn_n_layers': 4, 'nn_hidden_dim_0': 190, 'nn_hidden_dim_1': 83, 'nn_hidden_dim_2': 248, 'nn_hidden_dim_3': 164, 'nn_learning_rate': 0.021412482595798046, 'nn_batch_size': 32, 'nn_num_epochs': 28}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:34,332] Trial 5 finished with value: 101.83836364746094 and parameters: {'xgb_n_estimators': 113, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.0004014062052101528, 'xgb_subsample': 0.9888785148488026, 'xgb_colsample_bytree': 0.5147766497320063, 'nn_n_layers': 1, 'nn_hidden_dim_0': 75, 'nn_learning_rate': 0.00015641628015044938, 'nn_batch_size': 64, 'nn_num_epochs': 40}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:34,464] Trial 6 finished with value: 106.75548553466797 and parameters: {'xgb_n_estimators': 148, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.00016157971392983525, 'xgb_subsample': 0.766399653566672, 'xgb_colsample_bytree': 0.6177645315260454, 'nn_n_layers': 2, 'nn_hidden_dim_0': 165, 'nn_hidden_dim_1': 64, 'nn_learning_rate': 0.00023278628966272985, 'nn_batch_size': 32, 'nn_num_epochs': 14}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:34,925] Trial 7 finished with value: 82.94449615478516 and parameters: {'xgb_n_estimators': 171, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.0013108820720590583, 'xgb_subsample': 0.7782033924880173, 'xgb_colsample_bytree': 0.5752561736880931, 'nn_n_layers': 3, 'nn_hidden_dim_0': 172, 'nn_hidden_dim_1': 127, 'nn_hidden_dim_2': 232, 'nn_learning_rate': 0.00016619351285615723, 'nn_batch_size': 64, 'nn_num_epochs': 71}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:35,000] Trial 8 finished with value: 113.22511291503906 and parameters: {'xgb_n_estimators': 104, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.03019202367966943, 'xgb_subsample': 0.9985634363825622, 'xgb_colsample_bytree': 0.9149378813078041, 'nn_n_layers': 1, 'nn_hidden_dim_0': 116, 'nn_learning_rate': 0.01183521307426122, 'nn_batch_size': 256, 'nn_num_epochs': 11}. Best is trial 1 with value: 57.06631088256836.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:35,480] Trial 9 finished with value: 37.27361297607422 and parameters: {'xgb_n_estimators': 63, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.04112584119387677, 'xgb_subsample': 0.7780051313993079, 'xgb_colsample_bytree': 0.9760427653067447, 'nn_n_layers': 3, 'nn_hidden_dim_0': 236, 'nn_hidden_dim_1': 60, 'nn_hidden_dim_2': 102, 'nn_learning_rate': 0.025372765424773677, 'nn_batch_size': 256, 'nn_num_epochs': 97}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:36,250] Trial 10 finished with value: 105.1098403930664 and parameters: {'xgb_n_estimators': 57, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.09898904573794744, 'xgb_subsample': 0.8554031887377537, 'xgb_colsample_bytree': 0.8061419712782406, 'nn_n_layers': 5, 'nn_hidden_dim_0': 241, 'nn_hidden_dim_1': 256, 'nn_hidden_dim_2': 36, 'nn_hidden_dim_3': 35, 'nn_hidden_dim_4': 238, 'nn_learning_rate': 0.08677697274663196, 'nn_batch_size': 256, 'nn_num_epochs': 95}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:36,766] Trial 11 finished with value: 65.95701599121094 and parameters: {'xgb_n_estimators': 82, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.06565883851412599, 'xgb_subsample': 0.6514474048092106, 'xgb_colsample_bytree': 0.7288212549256928, 'nn_n_layers': 4, 'nn_hidden_dim_0': 241, 'nn_hidden_dim_1': 124, 'nn_hidden_dim_2': 140, 'nn_hidden_dim_3': 256, 'nn_learning_rate': 0.007144139043883062, 'nn_batch_size': 128, 'nn_num_epochs': 61}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:37,269] Trial 12 finished with value: 137.20062255859375 and parameters: {'xgb_n_estimators': 83, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.0071498165796341166, 'xgb_subsample': 0.865744217630209, 'xgb_colsample_bytree': 0.7758969686916486, 'nn_n_layers': 2, 'nn_hidden_dim_0': 256, 'nn_hidden_dim_1': 113, 'nn_learning_rate': 0.0014639414365296966, 'nn_batch_size': 256, 'nn_num_epochs': 82}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:37,614] Trial 13 finished with value: 63.21449661254883 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.0192576665707695, 'xgb_subsample': 0.5965701851402689, 'xgb_colsample_bytree': 0.868456385501087, 'nn_n_layers': 4, 'nn_hidden_dim_0': 213, 'nn_hidden_dim_1': 40, 'nn_hidden_dim_2': 132, 'nn_hidden_dim_3': 53, 'nn_learning_rate': 0.02031342010201241, 'nn_batch_size': 128, 'nn_num_epochs': 51}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:38,119] Trial 14 finished with value: 55.96055603027344 and parameters: {'xgb_n_estimators': 85, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.0029764130043014566, 'xgb_subsample': 0.7187880999721358, 'xgb_colsample_bytree': 0.9901744569765845, 'nn_n_layers': 2, 'nn_hidden_dim_0': 210, 'nn_hidden_dim_1': 178, 'nn_learning_rate': 0.038433481238797335, 'nn_batch_size': 64, 'nn_num_epochs': 84}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:39,461] Trial 15 finished with value: 105.21226501464844 and parameters: {'xgb_n_estimators': 92, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.003532266305892281, 'xgb_subsample': 0.8386943453522665, 'xgb_colsample_bytree': 0.9974989874079224, 'nn_n_layers': 2, 'nn_hidden_dim_0': 206, 'nn_hidden_dim_1': 173, 'nn_learning_rate': 0.04650080160314081, 'nn_batch_size': 256, 'nn_num_epochs': 87}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:39,938] Trial 16 finished with value: 105.09561920166016 and parameters: {'xgb_n_estimators': 68, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.0019280071599981892, 'xgb_subsample': 0.721130149738691, 'xgb_colsample_bytree': 0.9438639009177078, 'nn_n_layers': 2, 'nn_hidden_dim_0': 145, 'nn_hidden_dim_1': 169, 'nn_learning_rate': 0.09809948149396397, 'nn_batch_size': 256, 'nn_num_epochs': 87}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:40,425] Trial 17 finished with value: 60.23439025878906 and parameters: {'xgb_n_estimators': 133, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.0005342015812251694, 'xgb_subsample': 0.7225800557813088, 'xgb_colsample_bytree': 0.8285417476999268, 'nn_n_layers': 1, 'nn_hidden_dim_0': 180, 'nn_learning_rate': 0.001961879619906722, 'nn_batch_size': 64, 'nn_num_epochs': 98}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:41,128] Trial 18 finished with value: 59.17097473144531 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.005419003690382548, 'xgb_subsample': 0.8151894144635006, 'xgb_colsample_bytree': 0.9457228889512148, 'nn_n_layers': 5, 'nn_hidden_dim_0': 222, 'nn_hidden_dim_1': 218, 'nn_hidden_dim_2': 92, 'nn_hidden_dim_3': 152, 'nn_hidden_dim_4': 43, 'nn_learning_rate': 0.028917941842290774, 'nn_batch_size': 128, 'nn_num_epochs': 80}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:41,467] Trial 19 finished with value: 88.68536376953125 and parameters: {'xgb_n_estimators': 72, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.0006502061454573633, 'xgb_subsample': 0.914940673430696, 'xgb_colsample_bytree': 0.8462131925993067, 'nn_n_layers': 2, 'nn_hidden_dim_0': 148, 'nn_hidden_dim_1': 154, 'nn_learning_rate': 0.0004917614660375662, 'nn_batch_size': 256, 'nn_num_epochs': 49}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:42,264] Trial 20 finished with value: 70.36013793945312 and parameters: {'xgb_n_estimators': 154, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.0117265808685172, 'xgb_subsample': 0.9132627117227736, 'xgb_colsample_bytree': 0.9544518701913521, 'nn_n_layers': 4, 'nn_hidden_dim_0': 191, 'nn_hidden_dim_1': 219, 'nn_hidden_dim_2': 191, 'nn_hidden_dim_3': 252, 'nn_learning_rate': 0.004565662739743702, 'nn_batch_size': 64, 'nn_num_epochs': 91}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:42,738] Trial 21 finished with value: 61.382080078125 and parameters: {'xgb_n_estimators': 81, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.04828946641309299, 'xgb_subsample': 0.6938696813259185, 'xgb_colsample_bytree': 0.7378725008112632, 'nn_n_layers': 3, 'nn_hidden_dim_0': 254, 'nn_hidden_dim_1': 99, 'nn_hidden_dim_2': 184, 'nn_learning_rate': 0.011415124600071524, 'nn_batch_size': 64, 'nn_num_epochs': 68}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:43,112] Trial 22 finished with value: 56.718448638916016 and parameters: {'xgb_n_estimators': 92, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.040721183760772654, 'xgb_subsample': 0.5998220187950122, 'xgb_colsample_bytree': 0.6947738432715986, 'nn_n_layers': 3, 'nn_hidden_dim_0': 230, 'nn_hidden_dim_1': 41, 'nn_hidden_dim_2': 103, 'nn_learning_rate': 0.00995295509210571, 'nn_batch_size': 64, 'nn_num_epochs': 60}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:43,399] Trial 23 finished with value: 71.86012268066406 and parameters: {'xgb_n_estimators': 120, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.01215255387130962, 'xgb_subsample': 0.5609809415382745, 'xgb_colsample_bytree': 0.7090729318057152, 'nn_n_layers': 2, 'nn_hidden_dim_0': 228, 'nn_hidden_dim_1': 33, 'nn_learning_rate': 0.033275206928494186, 'nn_batch_size': 64, 'nn_num_epochs': 42}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:43,783] Trial 24 finished with value: 77.43582916259766 and parameters: {'xgb_n_estimators': 92, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.0033651241882081094, 'xgb_subsample': 0.6184462492037679, 'xgb_colsample_bytree': 0.7769102772149992, 'nn_n_layers': 3, 'nn_hidden_dim_0': 203, 'nn_hidden_dim_1': 56, 'nn_hidden_dim_2': 102, 'nn_learning_rate': 0.006883590929222057, 'nn_batch_size': 64, 'nn_num_epochs': 58}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:44,306] Trial 25 finished with value: 84.9731674194336 and parameters: {'xgb_n_estimators': 61, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.04063686355023759, 'xgb_subsample': 0.7925190132705255, 'xgb_colsample_bytree': 0.9012423557427146, 'nn_n_layers': 4, 'nn_hidden_dim_0': 230, 'nn_hidden_dim_1': 137, 'nn_hidden_dim_2': 47, 'nn_hidden_dim_3': 101, 'nn_learning_rate': 0.04808320130379181, 'nn_batch_size': 64, 'nn_num_epochs': 76}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:44,700] Trial 26 finished with value: 42.94084548950195 and parameters: {'xgb_n_estimators': 98, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.08939332562800668, 'xgb_subsample': 0.7296475832899348, 'xgb_colsample_bytree': 0.9618368472891081, 'nn_n_layers': 2, 'nn_hidden_dim_0': 162, 'nn_hidden_dim_1': 91, 'nn_learning_rate': 0.01682828130368302, 'nn_batch_size': 256, 'nn_num_epochs': 84}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:45,080] Trial 27 finished with value: 42.204689025878906 and parameters: {'xgb_n_estimators': 104, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.09634593348833533, 'xgb_subsample': 0.7230324476741149, 'xgb_colsample_bytree': 0.9698707639905306, 'nn_n_layers': 1, 'nn_hidden_dim_0': 112, 'nn_learning_rate': 0.020870742657694535, 'nn_batch_size': 256, 'nn_num_epochs': 86}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:45,557] Trial 28 finished with value: 44.3751335144043 and parameters: {'xgb_n_estimators': 127, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.08688519218155878, 'xgb_subsample': 0.7491742805097441, 'xgb_colsample_bytree': 0.9604735584370707, 'nn_n_layers': 1, 'nn_hidden_dim_0': 108, 'nn_learning_rate': 0.01887483596851533, 'nn_batch_size': 256, 'nn_num_epochs': 91}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:45,988] Trial 29 finished with value: 43.83095169067383 and parameters: {'xgb_n_estimators': 108, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.06762892667909441, 'xgb_subsample': 0.6963057545800078, 'xgb_colsample_bytree': 0.9225377809466082, 'nn_n_layers': 1, 'nn_hidden_dim_0': 86, 'nn_learning_rate': 0.07146709297009818, 'nn_batch_size': 256, 'nn_num_epochs': 100}. Best is trial 9 with value: 37.27361297607422.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:46,403] Trial 30 finished with value: 33.64548110961914 and parameters: {'xgb_n_estimators': 71, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.019383301040344723, 'xgb_subsample': 0.815353708811642, 'xgb_colsample_bytree': 0.8602001196702431, 'nn_n_layers': 1, 'nn_hidden_dim_0': 131, 'nn_learning_rate': 0.01807156783594681, 'nn_batch_size': 256, 'nn_num_epochs': 92}. Best is trial 30 with value: 33.64548110961914.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:46,813] Trial 31 finished with value: 51.7286376953125 and parameters: {'xgb_n_estimators': 73, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.058639513873110755, 'xgb_subsample': 0.8162102516875431, 'xgb_colsample_bytree': 0.8776300955699046, 'nn_n_layers': 1, 'nn_hidden_dim_0': 121, 'nn_learning_rate': 0.018632198095854053, 'nn_batch_size': 256, 'nn_num_epochs': 92}. Best is trial 30 with value: 33.64548110961914.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:47,569] Trial 32 finished with value: 31.19928741455078 and parameters: {'xgb_n_estimators': 53, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.020226794546843598, 'xgb_subsample': 0.7463129575989632, 'xgb_colsample_bytree': 0.967415690790772, 'nn_n_layers': 1, 'nn_hidden_dim_0': 133, 'nn_learning_rate': 0.006850670963313171, 'nn_batch_size': 256, 'nn_num_epochs': 77}. Best is trial 32 with value: 31.19928741455078.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:47,916] Trial 33 finished with value: 62.61332702636719 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.02102384737845634, 'xgb_subsample': 0.679887645386662, 'xgb_colsample_bytree': 0.9266716335806897, 'nn_n_layers': 1, 'nn_hidden_dim_0': 131, 'nn_learning_rate': 0.006914404356243329, 'nn_batch_size': 256, 'nn_num_epochs': 75}. Best is trial 32 with value: 31.19928741455078.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:48,262] Trial 34 finished with value: 75.4917221069336 and parameters: {'xgb_n_estimators': 62, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.029500492666329516, 'xgb_subsample': 0.8803517437542205, 'xgb_colsample_bytree': 0.9745167094370295, 'nn_n_layers': 1, 'nn_hidden_dim_0': 92, 'nn_learning_rate': 0.003333416536025988, 'nn_batch_size': 256, 'nn_num_epochs': 78}. Best is trial 32 with value: 31.19928741455078.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:48,640] Trial 35 finished with value: 43.79644012451172 and parameters: {'xgb_n_estimators': 72, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.017454747464777758, 'xgb_subsample': 0.7993407096991274, 'xgb_colsample_bytree': 0.8363499973571016, 'nn_n_layers': 1, 'nn_hidden_dim_0': 65, 'nn_learning_rate': 0.02952623525916246, 'nn_batch_size': 256, 'nn_num_epochs': 94}. Best is trial 32 with value: 31.19928741455078.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:48,957] Trial 36 finished with value: 27.998464584350586 and parameters: {'xgb_n_estimators': 55, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.007179500757584209, 'xgb_subsample': 0.7563770059361411, 'xgb_colsample_bytree': 0.860817886458613, 'nn_n_layers': 1, 'nn_hidden_dim_0': 130, 'nn_learning_rate': 0.005347184654740098, 'nn_batch_size': 256, 'nn_num_epochs': 71}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:49,238] Trial 37 finished with value: 44.48603820800781 and parameters: {'xgb_n_estimators': 57, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.013534926320477196, 'xgb_subsample': 0.7563025378806425, 'xgb_colsample_bytree': 0.8676821094280991, 'nn_n_layers': 1, 'nn_hidden_dim_0': 128, 'nn_learning_rate': 0.0019367587856224933, 'nn_batch_size': 32, 'nn_num_epochs': 65}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:49,725] Trial 38 finished with value: 89.64818572998047 and parameters: {'xgb_n_estimators': 197, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.006768202244267573, 'xgb_subsample': 0.8268361014300557, 'xgb_colsample_bytree': 0.7960702677291841, 'nn_n_layers': 2, 'nn_hidden_dim_0': 136, 'nn_hidden_dim_1': 202, 'nn_learning_rate': 0.004844106636887993, 'nn_batch_size': 256, 'nn_num_epochs': 71}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:49,979] Trial 39 finished with value: 48.19560623168945 and parameters: {'xgb_n_estimators': 67, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.007242554281810038, 'xgb_subsample': 0.7793805554301221, 'xgb_colsample_bytree': 0.8975593265836099, 'nn_n_layers': 1, 'nn_hidden_dim_0': 152, 'nn_learning_rate': 0.008612000636444402, 'nn_batch_size': 32, 'nn_num_epochs': 27}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:50,608] Trial 40 finished with value: 91.61748504638672 and parameters: {'xgb_n_estimators': 55, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.027849416779488517, 'xgb_subsample': 0.9020069321001661, 'xgb_colsample_bytree': 0.8466204727112191, 'nn_n_layers': 5, 'nn_hidden_dim_0': 96, 'nn_hidden_dim_1': 254, 'nn_hidden_dim_2': 66, 'nn_hidden_dim_3': 192, 'nn_hidden_dim_4': 117, 'nn_learning_rate': 0.014641019421280379, 'nn_batch_size': 256, 'nn_num_epochs': 70}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:50,966] Trial 41 finished with value: 37.86721420288086 and parameters: {'xgb_n_estimators': 76, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.039993472651001595, 'xgb_subsample': 0.7556825105508599, 'xgb_colsample_bytree': 0.9331056643143697, 'nn_n_layers': 1, 'nn_hidden_dim_0': 109, 'nn_learning_rate': 0.00272009320787227, 'nn_batch_size': 256, 'nn_num_epochs': 87}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:51,764] Trial 42 finished with value: 41.82988357543945 and parameters: {'xgb_n_estimators': 76, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.01623808493321497, 'xgb_subsample': 0.7564299261094434, 'xgb_colsample_bytree': 0.931044568123531, 'nn_n_layers': 1, 'nn_hidden_dim_0': 65, 'nn_learning_rate': 0.0025100366820807727, 'nn_batch_size': 256, 'nn_num_epochs': 89}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:52,187] Trial 43 finished with value: 55.469417572021484 and parameters: {'xgb_n_estimators': 66, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.009636088142601588, 'xgb_subsample': 0.7852886019912609, 'xgb_colsample_bytree': 0.9070391055731011, 'nn_n_layers': 1, 'nn_hidden_dim_0': 159, 'nn_learning_rate': 0.001055114846247853, 'nn_batch_size': 256, 'nn_num_epochs': 96}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:52,572] Trial 44 finished with value: 30.76969337463379 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.027428708775115888, 'xgb_subsample': 0.7435832890657283, 'xgb_colsample_bytree': 0.882462490604668, 'nn_n_layers': 1, 'nn_hidden_dim_0': 139, 'nn_learning_rate': 0.0033756731857614223, 'nn_batch_size': 256, 'nn_num_epochs': 73}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:52,994] Trial 45 finished with value: 47.6573486328125 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.02405721903608709, 'xgb_subsample': 0.6646662917716614, 'xgb_colsample_bytree': 0.8064411080892409, 'nn_n_layers': 2, 'nn_hidden_dim_0': 138, 'nn_hidden_dim_1': 233, 'nn_learning_rate': 0.005289220930629526, 'nn_batch_size': 128, 'nn_num_epochs': 64}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:53,358] Trial 46 finished with value: 51.48577880859375 and parameters: {'xgb_n_estimators': 59, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.00010204088771189232, 'xgb_subsample': 0.8412731275359653, 'xgb_colsample_bytree': 0.8745598046569785, 'nn_n_layers': 1, 'nn_hidden_dim_0': 175, 'nn_learning_rate': 0.0036888636942611986, 'nn_batch_size': 256, 'nn_num_epochs': 74}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:53,815] Trial 47 finished with value: 69.17108917236328 and parameters: {'xgb_n_estimators': 55, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.009616715866360228, 'xgb_subsample': 0.9576827980325511, 'xgb_colsample_bytree': 0.5639711815401962, 'nn_n_layers': 2, 'nn_hidden_dim_0': 123, 'nn_hidden_dim_1': 85, 'nn_learning_rate': 0.0007359549820981225, 'nn_batch_size': 256, 'nn_num_epochs': 83}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:54,120] Trial 48 finished with value: 46.016319274902344 and parameters: {'xgb_n_estimators': 64, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.004274960108857591, 'xgb_subsample': 0.7391002009653781, 'xgb_colsample_bytree': 0.9835250886611681, 'nn_n_layers': 1, 'nn_hidden_dim_0': 137, 'nn_learning_rate': 0.006100053623170774, 'nn_batch_size': 32, 'nn_num_epochs': 53}. Best is trial 36 with value: 27.998464584350586.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2800434033.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:11:54,820] Trial 49 finished with value: 74.74002838134766 and parameters: {'xgb_n_estimators': 53, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.03383754209007601, 'xgb_subsample': 0.6960062421432174, 'xgb_colsample_bytree': 0.7717195503520679, 'nn_n_layers': 4, 'nn_hidden_dim_0': 155, 'nn_hidden_dim_1': 152, 'nn_hidden_dim_2': 169, 'nn_hidden_dim_3': 109, 'nn_learning_rate': 0.013550848669833682, 'nn_batch_size': 256, 'nn_num_epochs': 81}. Best is trial 36 with value: 27.998464584350586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "xgb_n_estimators: 55\n",
      "xgb_max_depth: 9\n",
      "xgb_learning_rate: 0.007179500757584209\n",
      "xgb_subsample: 0.7563770059361411\n",
      "xgb_colsample_bytree: 0.860817886458613\n",
      "nn_n_layers: 1\n",
      "nn_hidden_dim_0: 130\n",
      "nn_learning_rate: 0.005347184654740098\n",
      "nn_batch_size: 256\n",
      "nn_num_epochs: 71\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for XGBoost\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
    "        'subsample': trial.suggest_uniform('xgb_subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train XGBoost model\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the data using the XGBoost model\n",
    "    X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "    X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "\n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    n_layers = trial.suggest_int('nn_n_layers', 1, 5)\n",
    "    hidden_dims = [trial.suggest_int(f'nn_hidden_dim_{i}', 32, 256) for i in range(n_layers)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    nn_batch_size = trial.suggest_categorical('nn_batch_size', [32, 64, 128, 256])\n",
    "    nn_num_epochs = trial.suggest_int('nn_num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    nn_model = NeuralNetwork(X_train_transformed.shape[1], hidden_dims).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(nn_num_epochs):\n",
    "        nn_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final XGBoost model with the best hyperparameters\n",
    "xgb_params = {\n",
    "    'n_estimators': best_params['xgb_n_estimators'],\n",
    "    'max_depth': best_params['xgb_max_depth'],\n",
    "    'learning_rate': best_params['xgb_learning_rate'],\n",
    "    'subsample': best_params['xgb_subsample'],\n",
    "    'colsample_bytree': best_params['xgb_colsample_bytree']\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the data using the XGBoost model\n",
    "X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_model = NeuralNetwork(X_train_transformed.shape[1], \n",
    "                         [best_params[f'nn_hidden_dim_{i}'] for i in range(best_params['nn_n_layers'])]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=best_params['nn_learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['nn_batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['nn_num_epochs']):\n",
    "    nn_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['XGBoost + NN'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:22,655] A new study created in memory with name: no-name-c8fd92fb-64cd-4842-99f7-ebcb28058e2d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:22,993] Trial 0 finished with value: 104.6399154663086 and parameters: {'lgb_n_estimators': 54, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.00042179376500963453, 'lgb_subsample': 0.5163096251303236, 'lgb_colsample_bytree': 0.8761492359442916, 'nn_n_layers': 3, 'nn_hidden_dim_0': 208, 'nn_hidden_dim_1': 67, 'nn_hidden_dim_2': 236, 'nn_learning_rate': 0.00024512865277224473, 'nn_batch_size': 32, 'nn_num_epochs': 57}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:23,118] Trial 1 finished with value: 105.28367614746094 and parameters: {'lgb_n_estimators': 122, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.014628995857113634, 'lgb_subsample': 0.6690192914137392, 'lgb_colsample_bytree': 0.5730365536971014, 'nn_n_layers': 3, 'nn_hidden_dim_0': 159, 'nn_hidden_dim_1': 34, 'nn_hidden_dim_2': 80, 'nn_learning_rate': 0.004077044458617467, 'nn_batch_size': 256, 'nn_num_epochs': 26}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:23,304] Trial 2 finished with value: 106.50305938720703 and parameters: {'lgb_n_estimators': 158, 'lgb_max_depth': 4, 'lgb_learning_rate': 0.038195058985550756, 'lgb_subsample': 0.646086635863193, 'lgb_colsample_bytree': 0.9555746204783562, 'nn_n_layers': 5, 'nn_hidden_dim_0': 218, 'nn_hidden_dim_1': 51, 'nn_hidden_dim_2': 65, 'nn_hidden_dim_3': 98, 'nn_hidden_dim_4': 209, 'nn_learning_rate': 0.017250623908633172, 'nn_batch_size': 128, 'nn_num_epochs': 27}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:23,830] Trial 3 finished with value: 105.12828826904297 and parameters: {'lgb_n_estimators': 86, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.013106012580848403, 'lgb_subsample': 0.6567430671894787, 'lgb_colsample_bytree': 0.9371286341986755, 'nn_n_layers': 3, 'nn_hidden_dim_0': 215, 'nn_hidden_dim_1': 187, 'nn_hidden_dim_2': 238, 'nn_learning_rate': 0.0002732289643463954, 'nn_batch_size': 256, 'nn_num_epochs': 80}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:24,024] Trial 4 finished with value: 105.2466049194336 and parameters: {'lgb_n_estimators': 108, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.004661992428285996, 'lgb_subsample': 0.950058910141466, 'lgb_colsample_bytree': 0.5068292940534218, 'nn_n_layers': 2, 'nn_hidden_dim_0': 85, 'nn_hidden_dim_1': 78, 'nn_learning_rate': 0.0009803001402914808, 'nn_batch_size': 128, 'nn_num_epochs': 52}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:24,700] Trial 5 finished with value: 105.04753112792969 and parameters: {'lgb_n_estimators': 77, 'lgb_max_depth': 4, 'lgb_learning_rate': 0.009509274033423549, 'lgb_subsample': 0.9132806481233388, 'lgb_colsample_bytree': 0.6762631949675548, 'nn_n_layers': 4, 'nn_hidden_dim_0': 150, 'nn_hidden_dim_1': 159, 'nn_hidden_dim_2': 108, 'nn_hidden_dim_3': 186, 'nn_learning_rate': 0.0004305671017741831, 'nn_batch_size': 64, 'nn_num_epochs': 89}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:24,950] Trial 6 finished with value: 106.21504974365234 and parameters: {'lgb_n_estimators': 102, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00563586056575545, 'lgb_subsample': 0.8219929662113465, 'lgb_colsample_bytree': 0.7174286525623176, 'nn_n_layers': 2, 'nn_hidden_dim_0': 181, 'nn_hidden_dim_1': 185, 'nn_learning_rate': 0.00015208131236318554, 'nn_batch_size': 64, 'nn_num_epochs': 49}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:25,098] Trial 7 finished with value: 110.00047302246094 and parameters: {'lgb_n_estimators': 143, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.08875038268334944, 'lgb_subsample': 0.7356743450106915, 'lgb_colsample_bytree': 0.7287093802130847, 'nn_n_layers': 5, 'nn_hidden_dim_0': 109, 'nn_hidden_dim_1': 44, 'nn_hidden_dim_2': 129, 'nn_hidden_dim_3': 174, 'nn_hidden_dim_4': 186, 'nn_learning_rate': 0.00015387230121980086, 'nn_batch_size': 64, 'nn_num_epochs': 15}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:25,340] Trial 8 finished with value: 105.14608001708984 and parameters: {'lgb_n_estimators': 84, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.011743006273115628, 'lgb_subsample': 0.5794933754157657, 'lgb_colsample_bytree': 0.5690603464598271, 'nn_n_layers': 2, 'nn_hidden_dim_0': 246, 'nn_hidden_dim_1': 151, 'nn_learning_rate': 0.0005565788062898846, 'nn_batch_size': 256, 'nn_num_epochs': 49}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:25,445] Trial 9 finished with value: 108.37620544433594 and parameters: {'lgb_n_estimators': 54, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.00016299732737135215, 'lgb_subsample': 0.5093314129933125, 'lgb_colsample_bytree': 0.5664266288712854, 'nn_n_layers': 4, 'nn_hidden_dim_0': 112, 'nn_hidden_dim_1': 82, 'nn_hidden_dim_2': 174, 'nn_hidden_dim_3': 202, 'nn_learning_rate': 0.020110688744282575, 'nn_batch_size': 32, 'nn_num_epochs': 12}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:25,732] Trial 10 finished with value: 105.21692657470703 and parameters: {'lgb_n_estimators': 183, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.000588145388421464, 'lgb_subsample': 0.5145257119691895, 'lgb_colsample_bytree': 0.8453881529267584, 'nn_n_layers': 1, 'nn_hidden_dim_0': 191, 'nn_learning_rate': 0.0021956309545749903, 'nn_batch_size': 32, 'nn_num_epochs': 72}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:26,810] Trial 11 finished with value: 105.10245513916016 and parameters: {'lgb_n_estimators': 65, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0010459560092104098, 'lgb_subsample': 0.9902553105645449, 'lgb_colsample_bytree': 0.8332652358640823, 'nn_n_layers': 4, 'nn_hidden_dim_0': 37, 'nn_hidden_dim_1': 253, 'nn_hidden_dim_2': 254, 'nn_hidden_dim_3': 246, 'nn_learning_rate': 0.0007772421313441734, 'nn_batch_size': 64, 'nn_num_epochs': 99}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:27,365] Trial 12 finished with value: 106.71044158935547 and parameters: {'lgb_n_estimators': 52, 'lgb_max_depth': 5, 'lgb_learning_rate': 0.0011095277948729976, 'lgb_subsample': 0.8585157894818644, 'lgb_colsample_bytree': 0.8165537403467128, 'nn_n_layers': 4, 'nn_hidden_dim_0': 138, 'nn_hidden_dim_1': 114, 'nn_hidden_dim_2': 147, 'nn_hidden_dim_3': 38, 'nn_learning_rate': 0.00010207698377775588, 'nn_batch_size': 32, 'nn_num_epochs': 98}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:27,944] Trial 13 finished with value: 105.12277221679688 and parameters: {'lgb_n_estimators': 79, 'lgb_max_depth': 6, 'lgb_learning_rate': 0.00017701922956591832, 'lgb_subsample': 0.8806899517616948, 'lgb_colsample_bytree': 0.6523306985305392, 'nn_n_layers': 4, 'nn_hidden_dim_0': 255, 'nn_hidden_dim_1': 137, 'nn_hidden_dim_2': 187, 'nn_hidden_dim_3': 127, 'nn_learning_rate': 0.09362031497363987, 'nn_batch_size': 32, 'nn_num_epochs': 73}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:29,291] Trial 14 finished with value: 105.13893127441406 and parameters: {'lgb_n_estimators': 70, 'lgb_max_depth': 5, 'lgb_learning_rate': 0.001983509484350235, 'lgb_subsample': 0.7720644045106894, 'lgb_colsample_bytree': 0.8964295639465052, 'nn_n_layers': 3, 'nn_hidden_dim_0': 181, 'nn_hidden_dim_1': 183, 'nn_hidden_dim_2': 112, 'nn_learning_rate': 0.0003747110722546206, 'nn_batch_size': 64, 'nn_num_epochs': 86}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:29,716] Trial 15 finished with value: 105.62723541259766 and parameters: {'lgb_n_estimators': 104, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.0003838774127723813, 'lgb_subsample': 0.9229009979675693, 'lgb_colsample_bytree': 0.6791212431723318, 'nn_n_layers': 5, 'nn_hidden_dim_0': 159, 'nn_hidden_dim_1': 230, 'nn_hidden_dim_2': 35, 'nn_hidden_dim_3': 252, 'nn_hidden_dim_4': 56, 'nn_learning_rate': 0.0020845990411855085, 'nn_batch_size': 32, 'nn_num_epochs': 38}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:29,942] Trial 16 finished with value: 105.21481323242188 and parameters: {'lgb_n_estimators': 54, 'lgb_max_depth': 6, 'lgb_learning_rate': 0.0024737794851845563, 'lgb_subsample': 0.7256087388510831, 'lgb_colsample_bytree': 0.7865036938032675, 'nn_n_layers': 1, 'nn_hidden_dim_0': 216, 'nn_learning_rate': 0.0060718676048460785, 'nn_batch_size': 64, 'nn_num_epochs': 64}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:30,636] Trial 17 finished with value: 105.12393951416016 and parameters: {'lgb_n_estimators': 127, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.00010187367730751595, 'lgb_subsample': 0.805789287203229, 'lgb_colsample_bytree': 0.6474286357732697, 'nn_n_layers': 4, 'nn_hidden_dim_0': 55, 'nn_hidden_dim_1': 106, 'nn_hidden_dim_2': 219, 'nn_hidden_dim_3': 189, 'nn_learning_rate': 0.0010260604262142354, 'nn_batch_size': 128, 'nn_num_epochs': 65}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:31,263] Trial 18 finished with value: 105.19861602783203 and parameters: {'lgb_n_estimators': 193, 'lgb_max_depth': 5, 'lgb_learning_rate': 0.02950064677236357, 'lgb_subsample': 0.5892538440795134, 'lgb_colsample_bytree': 0.9938046166870105, 'nn_n_layers': 3, 'nn_hidden_dim_0': 139, 'nn_hidden_dim_1': 150, 'nn_hidden_dim_2': 100, 'nn_learning_rate': 0.0003105457886348364, 'nn_batch_size': 64, 'nn_num_epochs': 89}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:31,552] Trial 19 finished with value: 104.715576171875 and parameters: {'lgb_n_estimators': 93, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00033586287424902005, 'lgb_subsample': 0.579356215026103, 'lgb_colsample_bytree': 0.886211703520994, 'nn_n_layers': 2, 'nn_hidden_dim_0': 108, 'nn_hidden_dim_1': 207, 'nn_learning_rate': 0.0015550629932259642, 'nn_batch_size': 32, 'nn_num_epochs': 38}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:31,844] Trial 20 finished with value: 104.82938385009766 and parameters: {'lgb_n_estimators': 95, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00035842515421034474, 'lgb_subsample': 0.5653014733744197, 'lgb_colsample_bytree': 0.8967352991630179, 'nn_n_layers': 2, 'nn_hidden_dim_0': 82, 'nn_hidden_dim_1': 207, 'nn_learning_rate': 0.010625811633286245, 'nn_batch_size': 32, 'nn_num_epochs': 39}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:32,181] Trial 21 finished with value: 104.8520278930664 and parameters: {'lgb_n_estimators': 91, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00036807342162744526, 'lgb_subsample': 0.5883233944252878, 'lgb_colsample_bytree': 0.8695428646638181, 'nn_n_layers': 2, 'nn_hidden_dim_0': 80, 'nn_hidden_dim_1': 227, 'nn_learning_rate': 0.009730423176239449, 'nn_batch_size': 32, 'nn_num_epochs': 40}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:32,482] Trial 22 finished with value: 105.05113983154297 and parameters: {'lgb_n_estimators': 120, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00031053057674986726, 'lgb_subsample': 0.5518060640135853, 'lgb_colsample_bytree': 0.9127553501274046, 'nn_n_layers': 2, 'nn_hidden_dim_0': 108, 'nn_hidden_dim_1': 213, 'nn_learning_rate': 0.043299244091483106, 'nn_batch_size': 32, 'nn_num_epochs': 40}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:32,665] Trial 23 finished with value: 105.84293365478516 and parameters: {'lgb_n_estimators': 97, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0008251884685813018, 'lgb_subsample': 0.6216379851692604, 'lgb_colsample_bytree': 0.779693155817506, 'nn_n_layers': 1, 'nn_hidden_dim_0': 80, 'nn_learning_rate': 0.008412215205799836, 'nn_batch_size': 32, 'nn_num_epochs': 29}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:33,131] Trial 24 finished with value: 105.17169952392578 and parameters: {'lgb_n_estimators': 69, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.0002143282844388477, 'lgb_subsample': 0.5401500567451343, 'lgb_colsample_bytree': 0.9849840715230123, 'nn_n_layers': 3, 'nn_hidden_dim_0': 122, 'nn_hidden_dim_1': 204, 'nn_hidden_dim_2': 196, 'nn_learning_rate': 0.0016163637485276314, 'nn_batch_size': 32, 'nn_num_epochs': 58}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:33,385] Trial 25 finished with value: 105.14616394042969 and parameters: {'lgb_n_estimators': 143, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.000622657559377899, 'lgb_subsample': 0.6978897177013096, 'lgb_colsample_bytree': 0.8776927300164727, 'nn_n_layers': 2, 'nn_hidden_dim_0': 61, 'nn_hidden_dim_1': 250, 'nn_learning_rate': 0.0033914260578183377, 'nn_batch_size': 32, 'nn_num_epochs': 35}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:33,633] Trial 26 finished with value: 105.39818572998047 and parameters: {'lgb_n_estimators': 111, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0014708902138905384, 'lgb_subsample': 0.5012606053692553, 'lgb_colsample_bytree': 0.9297397211270766, 'nn_n_layers': 1, 'nn_hidden_dim_0': 87, 'nn_learning_rate': 0.022932779563372932, 'nn_batch_size': 32, 'nn_num_epochs': 43}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:33,851] Trial 27 finished with value: 105.83656311035156 and parameters: {'lgb_n_estimators': 135, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00010353888546192184, 'lgb_subsample': 0.6203610203721386, 'lgb_colsample_bytree': 0.7990879676935866, 'nn_n_layers': 3, 'nn_hidden_dim_0': 97, 'nn_hidden_dim_1': 167, 'nn_hidden_dim_2': 154, 'nn_learning_rate': 0.005310597878511717, 'nn_batch_size': 32, 'nn_num_epochs': 20}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:35,047] Trial 28 finished with value: 105.11751556396484 and parameters: {'lgb_n_estimators': 163, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00042753121777514523, 'lgb_subsample': 0.5595733654037651, 'lgb_colsample_bytree': 0.8589458273030148, 'nn_n_layers': 2, 'nn_hidden_dim_0': 51, 'nn_hidden_dim_1': 126, 'nn_learning_rate': 0.01310236927213674, 'nn_batch_size': 32, 'nn_num_epochs': 57}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:35,327] Trial 29 finished with value: 105.71556854248047 and parameters: {'lgb_n_estimators': 114, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.00023800937302895507, 'lgb_subsample': 0.6146100915018151, 'lgb_colsample_bytree': 0.9531206864059752, 'nn_n_layers': 2, 'nn_hidden_dim_0': 130, 'nn_hidden_dim_1': 203, 'nn_learning_rate': 0.035532979080029066, 'nn_batch_size': 256, 'nn_num_epochs': 33}. Best is trial 0 with value: 104.6399154663086.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:35,555] Trial 30 finished with value: 104.99534606933594 and parameters: {'lgb_n_estimators': 60, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.000660327142659241, 'lgb_subsample': 0.7040221734701735, 'lgb_colsample_bytree': 0.7514546374534615, 'nn_n_layers': 1, 'nn_hidden_dim_0': 69, 'nn_learning_rate': 0.00390132057185263, 'nn_batch_size': 32, 'nn_num_epochs': 45}. Best is trial 0 with value: 104.6399154663086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:35,716] Trial 31 finished with value: 104.5672607421875 and parameters: {'lgb_n_estimators': 90, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.000380258536824582, 'lgb_subsample': 0.594032821135376, 'lgb_colsample_bytree': 0.8997484019295007, 'nn_n_layers': 2, 'nn_hidden_dim_0': 33, 'nn_hidden_dim_1': 226, 'nn_learning_rate': 0.006743188645866281, 'nn_batch_size': 32, 'nn_num_epochs': 21}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:36,264] Trial 32 finished with value: 105.16194915771484 and parameters: {'lgb_n_estimators': 92, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.0005203837173892043, 'lgb_subsample': 0.5284351717927248, 'lgb_colsample_bytree': 0.8894114521178983, 'nn_n_layers': 3, 'nn_hidden_dim_0': 40, 'nn_hidden_dim_1': 232, 'nn_hidden_dim_2': 223, 'nn_learning_rate': 0.006935316879649457, 'nn_batch_size': 32, 'nn_num_epochs': 16}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:36,493] Trial 33 finished with value: 106.59446716308594 and parameters: {'lgb_n_estimators': 76, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.00022846886852720478, 'lgb_subsample': 0.6448071735217865, 'lgb_colsample_bytree': 0.9220800675742858, 'nn_n_layers': 2, 'nn_hidden_dim_0': 234, 'nn_hidden_dim_1': 216, 'nn_learning_rate': 0.012354786931723429, 'nn_batch_size': 128, 'nn_num_epochs': 23}. Best is trial 31 with value: 104.5672607421875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:36,809] Trial 34 finished with value: 105.74711608886719 and parameters: {'lgb_n_estimators': 95, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00014011719148488084, 'lgb_subsample': 0.5660912588931056, 'lgb_colsample_bytree': 0.9694800860514312, 'nn_n_layers': 3, 'nn_hidden_dim_0': 70, 'nn_hidden_dim_1': 173, 'nn_hidden_dim_2': 210, 'nn_learning_rate': 0.0013793096719111155, 'nn_batch_size': 256, 'nn_num_epochs': 28}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:37,156] Trial 35 finished with value: 105.09170532226562 and parameters: {'lgb_n_estimators': 85, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.003719832745464487, 'lgb_subsample': 0.6848121298252398, 'lgb_colsample_bytree': 0.9553914207254042, 'nn_n_layers': 2, 'nn_hidden_dim_0': 94, 'nn_hidden_dim_1': 245, 'nn_learning_rate': 0.0026950869807247976, 'nn_batch_size': 32, 'nn_num_epochs': 61}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:37,517] Trial 36 finished with value: 105.19314575195312 and parameters: {'lgb_n_estimators': 124, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.000310304788250096, 'lgb_subsample': 0.6008229591745248, 'lgb_colsample_bytree': 0.8987182740738502, 'nn_n_layers': 3, 'nn_hidden_dim_0': 33, 'nn_hidden_dim_1': 196, 'nn_hidden_dim_2': 256, 'nn_learning_rate': 0.03001748430612727, 'nn_batch_size': 128, 'nn_num_epochs': 52}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:37,812] Trial 37 finished with value: 106.72945404052734 and parameters: {'lgb_n_estimators': 101, 'lgb_max_depth': 9, 'lgb_learning_rate': 0.0014993135187394897, 'lgb_subsample': 0.6522735546972487, 'lgb_colsample_bytree': 0.8360076767810515, 'nn_n_layers': 2, 'nn_hidden_dim_0': 192, 'nn_hidden_dim_1': 63, 'nn_learning_rate': 0.00019826356634515497, 'nn_batch_size': 32, 'nn_num_epochs': 34}. Best is trial 31 with value: 104.5672607421875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:38,028] Trial 38 finished with value: 105.24493408203125 and parameters: {'lgb_n_estimators': 75, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0008299034749961804, 'lgb_subsample': 0.5359718088471037, 'lgb_colsample_bytree': 0.9411754094332717, 'nn_n_layers': 1, 'nn_hidden_dim_0': 170, 'nn_learning_rate': 0.004601582644459463, 'nn_batch_size': 256, 'nn_num_epochs': 46}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:38,195] Trial 39 finished with value: 107.5615005493164 and parameters: {'lgb_n_estimators': 113, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.007823924063453218, 'lgb_subsample': 0.6386356782357735, 'lgb_colsample_bytree': 0.8614585057075129, 'nn_n_layers': 2, 'nn_hidden_dim_0': 122, 'nn_hidden_dim_1': 33, 'nn_learning_rate': 0.0004812114916245511, 'nn_batch_size': 32, 'nn_num_epochs': 22}. Best is trial 31 with value: 104.5672607421875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:38,379] Trial 40 finished with value: 103.45968627929688 and parameters: {'lgb_n_estimators': 84, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00027848775760416306, 'lgb_subsample': 0.6744402577794993, 'lgb_colsample_bytree': 0.7622452542593923, 'nn_n_layers': 2, 'nn_hidden_dim_0': 204, 'nn_hidden_dim_1': 215, 'nn_learning_rate': 0.05799908673567021, 'nn_batch_size': 32, 'nn_num_epochs': 10}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:38,559] Trial 41 finished with value: 107.35567474365234 and parameters: {'lgb_n_estimators': 85, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.000317458405185175, 'lgb_subsample': 0.6701275320308063, 'lgb_colsample_bytree': 0.8165762718623268, 'nn_n_layers': 2, 'nn_hidden_dim_0': 203, 'nn_hidden_dim_1': 218, 'nn_learning_rate': 0.06370047207290923, 'nn_batch_size': 32, 'nn_num_epochs': 10}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:38,749] Trial 42 finished with value: 104.81035614013672 and parameters: {'lgb_n_estimators': 62, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00013779746459518765, 'lgb_subsample': 0.5750806056710004, 'lgb_colsample_bytree': 0.9120795264699705, 'nn_n_layers': 2, 'nn_hidden_dim_0': 228, 'nn_hidden_dim_1': 234, 'nn_learning_rate': 0.013322216353318438, 'nn_batch_size': 32, 'nn_num_epochs': 17}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:39,806] Trial 43 finished with value: 106.35819244384766 and parameters: {'lgb_n_estimators': 61, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00013865197575545172, 'lgb_subsample': 0.6058086407488158, 'lgb_colsample_bytree': 0.7539000253869703, 'nn_n_layers': 2, 'nn_hidden_dim_0': 231, 'nn_hidden_dim_1': 242, 'nn_learning_rate': 0.016660063263505068, 'nn_batch_size': 32, 'nn_num_epochs': 17}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:39,979] Trial 44 finished with value: 104.08866119384766 and parameters: {'lgb_n_estimators': 63, 'lgb_max_depth': 8, 'lgb_learning_rate': 0.00023153781984944878, 'lgb_subsample': 0.5298309501427008, 'lgb_colsample_bytree': 0.9127632689402722, 'nn_n_layers': 1, 'nn_hidden_dim_0': 205, 'nn_learning_rate': 0.05187491687325274, 'nn_batch_size': 32, 'nn_num_epochs': 14}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:40,135] Trial 45 finished with value: 104.52873992919922 and parameters: {'lgb_n_estimators': 50, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0002462895811311937, 'lgb_subsample': 0.5218013897838257, 'lgb_colsample_bytree': 0.842212370767243, 'nn_n_layers': 1, 'nn_hidden_dim_0': 202, 'nn_learning_rate': 0.039624238933723646, 'nn_batch_size': 32, 'nn_num_epochs': 26}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:40,255] Trial 46 finished with value: 104.9756851196289 and parameters: {'lgb_n_estimators': 50, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0002183632727940508, 'lgb_subsample': 0.5187690167825391, 'lgb_colsample_bytree': 0.7109449739173778, 'nn_n_layers': 1, 'nn_hidden_dim_0': 202, 'nn_learning_rate': 0.098258192020486, 'nn_batch_size': 128, 'nn_num_epochs': 12}. Best is trial 40 with value: 103.45968627929688.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:40,626] Trial 47 finished with value: 106.18160247802734 and parameters: {'lgb_n_estimators': 70, 'lgb_max_depth': 7, 'lgb_learning_rate': 0.0004827780794728424, 'lgb_subsample': 0.5020010487241862, 'lgb_colsample_bytree': 0.8083391200005406, 'nn_n_layers': 1, 'nn_hidden_dim_0': 202, 'nn_learning_rate': 0.05752637116376999, 'nn_batch_size': 32, 'nn_num_epochs': 23}. Best is trial 40 with value: 103.45968627929688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:40,843] Trial 48 finished with value: 101.91624450683594 and parameters: {'lgb_n_estimators': 57, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.0009649370536817564, 'lgb_subsample': 0.5330759057920634, 'lgb_colsample_bytree': 0.8463168041687263, 'nn_n_layers': 1, 'nn_hidden_dim_0': 172, 'nn_learning_rate': 0.060956933751015685, 'nn_batch_size': 256, 'nn_num_epochs': 10}. Best is trial 48 with value: 101.91624450683594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:59: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:60: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1234451331.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:40,958] Trial 49 finished with value: 103.74559020996094 and parameters: {'lgb_n_estimators': 56, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.000703869871932133, 'lgb_subsample': 0.5442127536467646, 'lgb_colsample_bytree': 0.7667767786096507, 'nn_n_layers': 1, 'nn_hidden_dim_0': 181, 'nn_learning_rate': 0.07111929774337371, 'nn_batch_size': 256, 'nn_num_epochs': 10}. Best is trial 48 with value: 101.91624450683594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 24, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 7.791087\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "lgb_n_estimators: 57\n",
      "lgb_max_depth: 10\n",
      "lgb_learning_rate: 0.0009649370536817564\n",
      "lgb_subsample: 0.5330759057920634\n",
      "lgb_colsample_bytree: 0.8463168041687263\n",
      "nn_n_layers: 1\n",
      "nn_hidden_dim_0: 172\n",
      "nn_learning_rate: 0.060956933751015685\n",
      "nn_batch_size: 256\n",
      "nn_num_epochs: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for LightGBM\n",
    "    lgb_params = {\n",
    "        'n_estimators': trial.suggest_int('lgb_n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('lgb_max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
    "        'subsample': trial.suggest_uniform('lgb_subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the data using the LightGBM model (use raw predictions as features)\n",
    "    X_train_transformed = lgb_model.predict(X_train_scaled, raw_score=True).reshape(-1, 1)\n",
    "    X_test_transformed = lgb_model.predict(X_test_scaled, raw_score=True).reshape(-1, 1)\n",
    "\n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    n_layers = trial.suggest_int('nn_n_layers', 1, 5)\n",
    "    hidden_dims = [trial.suggest_int(f'nn_hidden_dim_{i}', 32, 256) for i in range(n_layers)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    nn_batch_size = trial.suggest_categorical('nn_batch_size', [32, 64, 128, 256])\n",
    "    nn_num_epochs = trial.suggest_int('nn_num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    nn_model = NeuralNetwork(X_train_transformed.shape[1], hidden_dims).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(nn_num_epochs):\n",
    "        nn_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final LightGBM model with the best hyperparameters\n",
    "lgb_params = {\n",
    "    'n_estimators': best_params['lgb_n_estimators'],\n",
    "    'max_depth': best_params['lgb_max_depth'],\n",
    "    'learning_rate': best_params['lgb_learning_rate'],\n",
    "    'subsample': best_params['lgb_subsample'],\n",
    "    'colsample_bytree': best_params['lgb_colsample_bytree']\n",
    "}\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the data using the LightGBM model (use raw predictions as features)\n",
    "X_train_transformed = lgb_model.predict(X_train_scaled, raw_score=True).reshape(-1, 1)\n",
    "X_test_transformed = lgb_model.predict(X_test_scaled, raw_score=True).reshape(-1, 1)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_model = NeuralNetwork(X_train_transformed.shape[1], \n",
    "                         [best_params[f'nn_hidden_dim_{i}'] for i in range(best_params['nn_n_layers'])]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=best_params['nn_learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['nn_batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['nn_num_epochs']):\n",
    "    nn_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['LightGBM + NN'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:16:57,374] A new study created in memory with name: no-name-add0a9c7-a0a3-48c5-81ab-2b05f7fdf1b2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:58,723] Trial 0 finished with value: 68.5884017944336 and parameters: {'cat_iterations': 808, 'cat_depth': 9, 'cat_learning_rate': 0.006298568706135879, 'cat_l2_leaf_reg': 0.06032636615429546, 'cat_border_count': 55, 'cat_bagging_temperature': 0.24724308918778018, 'nn_n_layers': 2, 'nn_hidden_dim_0': 56, 'nn_hidden_dim_1': 96, 'nn_learning_rate': 0.000115418945612421, 'nn_batch_size': 32, 'nn_num_epochs': 49}. Best is trial 0 with value: 68.5884017944336.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:59,021] Trial 1 finished with value: 48.71460723876953 and parameters: {'cat_iterations': 151, 'cat_depth': 6, 'cat_learning_rate': 0.046889185252571186, 'cat_l2_leaf_reg': 1.9570008213821897e-06, 'cat_border_count': 62, 'cat_bagging_temperature': 0.05947760837286274, 'nn_n_layers': 2, 'nn_hidden_dim_0': 72, 'nn_hidden_dim_1': 148, 'nn_learning_rate': 0.003913902038825667, 'nn_batch_size': 32, 'nn_num_epochs': 38}. Best is trial 1 with value: 48.71460723876953.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:16:59,730] Trial 2 finished with value: 52.61526870727539 and parameters: {'cat_iterations': 532, 'cat_depth': 6, 'cat_learning_rate': 0.0014572761314073865, 'cat_l2_leaf_reg': 4.364341289132982e-07, 'cat_border_count': 143, 'cat_bagging_temperature': 0.020460398377236812, 'nn_n_layers': 4, 'nn_hidden_dim_0': 119, 'nn_hidden_dim_1': 255, 'nn_hidden_dim_2': 106, 'nn_hidden_dim_3': 76, 'nn_learning_rate': 0.016224957628155, 'nn_batch_size': 64, 'nn_num_epochs': 54}. Best is trial 1 with value: 48.71460723876953.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:00,115] Trial 3 finished with value: 109.24057006835938 and parameters: {'cat_iterations': 327, 'cat_depth': 6, 'cat_learning_rate': 0.24236983316952332, 'cat_l2_leaf_reg': 1.3604805522478546e-08, 'cat_border_count': 178, 'cat_bagging_temperature': 0.05105960371840687, 'nn_n_layers': 4, 'nn_hidden_dim_0': 213, 'nn_hidden_dim_1': 156, 'nn_hidden_dim_2': 48, 'nn_hidden_dim_3': 227, 'nn_learning_rate': 0.04429777898480766, 'nn_batch_size': 64, 'nn_num_epochs': 21}. Best is trial 1 with value: 48.71460723876953.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:00,587] Trial 4 finished with value: 43.09843063354492 and parameters: {'cat_iterations': 248, 'cat_depth': 9, 'cat_learning_rate': 0.007272407572770239, 'cat_l2_leaf_reg': 2.6644008095958682, 'cat_border_count': 237, 'cat_bagging_temperature': 21.037486667640085, 'nn_n_layers': 1, 'nn_hidden_dim_0': 176, 'nn_learning_rate': 0.015395871827658955, 'nn_batch_size': 128, 'nn_num_epochs': 57}. Best is trial 4 with value: 43.09843063354492.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:01,387] Trial 5 finished with value: 64.0231704711914 and parameters: {'cat_iterations': 972, 'cat_depth': 4, 'cat_learning_rate': 0.20224601966064917, 'cat_l2_leaf_reg': 5.388001718210881e-08, 'cat_border_count': 45, 'cat_bagging_temperature': 26.592260852218732, 'nn_n_layers': 1, 'nn_hidden_dim_0': 94, 'nn_learning_rate': 0.00016845728910142593, 'nn_batch_size': 64, 'nn_num_epochs': 97}. Best is trial 4 with value: 43.09843063354492.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:03,581] Trial 6 finished with value: 82.69135284423828 and parameters: {'cat_iterations': 874, 'cat_depth': 9, 'cat_learning_rate': 0.24110713279058943, 'cat_l2_leaf_reg': 0.0024941450885316917, 'cat_border_count': 140, 'cat_bagging_temperature': 0.016431735444193626, 'nn_n_layers': 3, 'nn_hidden_dim_0': 240, 'nn_hidden_dim_1': 241, 'nn_hidden_dim_2': 163, 'nn_learning_rate': 0.00044139585200720245, 'nn_batch_size': 32, 'nn_num_epochs': 76}. Best is trial 4 with value: 43.09843063354492.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:05,066] Trial 7 finished with value: 42.648189544677734 and parameters: {'cat_iterations': 803, 'cat_depth': 10, 'cat_learning_rate': 0.008202933354242402, 'cat_l2_leaf_reg': 3.3448740297516283e-06, 'cat_border_count': 89, 'cat_bagging_temperature': 0.026959625555077932, 'nn_n_layers': 3, 'nn_hidden_dim_0': 155, 'nn_hidden_dim_1': 198, 'nn_hidden_dim_2': 185, 'nn_learning_rate': 0.0003446259901243493, 'nn_batch_size': 64, 'nn_num_epochs': 47}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:05,779] Trial 8 finished with value: 56.85830307006836 and parameters: {'cat_iterations': 296, 'cat_depth': 5, 'cat_learning_rate': 0.0699391802156096, 'cat_l2_leaf_reg': 8.395763075857322e-05, 'cat_border_count': 65, 'cat_bagging_temperature': 84.44398843507574, 'nn_n_layers': 4, 'nn_hidden_dim_0': 185, 'nn_hidden_dim_1': 184, 'nn_hidden_dim_2': 234, 'nn_hidden_dim_3': 133, 'nn_learning_rate': 0.00010220399066611457, 'nn_batch_size': 256, 'nn_num_epochs': 74}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:06,478] Trial 9 finished with value: 100.0682373046875 and parameters: {'cat_iterations': 966, 'cat_depth': 5, 'cat_learning_rate': 0.5205504928090458, 'cat_l2_leaf_reg': 1.1787212745434612, 'cat_border_count': 35, 'cat_bagging_temperature': 91.53386648628957, 'nn_n_layers': 2, 'nn_hidden_dim_0': 110, 'nn_hidden_dim_1': 243, 'nn_learning_rate': 0.00034277410184960217, 'nn_batch_size': 32, 'nn_num_epochs': 30}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:07,780] Trial 10 finished with value: 56.370052337646484 and parameters: {'cat_iterations': 629, 'cat_depth': 10, 'cat_learning_rate': 0.010521262982200262, 'cat_l2_leaf_reg': 3.733292943163914e-05, 'cat_border_count': 102, 'cat_bagging_temperature': 2.2669827649954386, 'nn_n_layers': 5, 'nn_hidden_dim_0': 151, 'nn_hidden_dim_1': 43, 'nn_hidden_dim_2': 218, 'nn_hidden_dim_3': 244, 'nn_hidden_dim_4': 241, 'nn_learning_rate': 0.0012252401518188904, 'nn_batch_size': 128, 'nn_num_epochs': 10}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:08,755] Trial 11 finished with value: 110.50239562988281 and parameters: {'cat_iterations': 645, 'cat_depth': 8, 'cat_learning_rate': 0.0077012773156583635, 'cat_l2_leaf_reg': 2.1235262002347377, 'cat_border_count': 244, 'cat_bagging_temperature': 3.281336493501899, 'nn_n_layers': 1, 'nn_hidden_dim_0': 170, 'nn_learning_rate': 0.009072583903929149, 'nn_batch_size': 128, 'nn_num_epochs': 68}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:09,643] Trial 12 finished with value: 104.8542251586914 and parameters: {'cat_iterations': 444, 'cat_depth': 10, 'cat_learning_rate': 0.0015961790216235552, 'cat_l2_leaf_reg': 0.00832506449590672, 'cat_border_count': 228, 'cat_bagging_temperature': 8.88477323442211, 'nn_n_layers': 3, 'nn_hidden_dim_0': 199, 'nn_hidden_dim_1': 190, 'nn_hidden_dim_2': 162, 'nn_learning_rate': 0.07860621913965433, 'nn_batch_size': 128, 'nn_num_epochs': 46}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:10,098] Trial 13 finished with value: 54.32376480102539 and parameters: {'cat_iterations': 110, 'cat_depth': 8, 'cat_learning_rate': 0.01790237959702382, 'cat_l2_leaf_reg': 1.770208255499678e-05, 'cat_border_count': 198, 'cat_bagging_temperature': 0.48747101009893573, 'nn_n_layers': 1, 'nn_hidden_dim_0': 149, 'nn_learning_rate': 0.0021097392661440584, 'nn_batch_size': 256, 'nn_num_epochs': 63}. Best is trial 7 with value: 42.648189544677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:11,616] Trial 14 finished with value: 24.963293075561523 and parameters: {'cat_iterations': 753, 'cat_depth': 9, 'cat_learning_rate': 0.002862495864612102, 'cat_l2_leaf_reg': 0.0006769559669751035, 'cat_border_count': 122, 'cat_bagging_temperature': 0.16189809481169865, 'nn_n_layers': 5, 'nn_hidden_dim_0': 256, 'nn_hidden_dim_1': 91, 'nn_hidden_dim_2': 110, 'nn_hidden_dim_3': 35, 'nn_hidden_dim_4': 33, 'nn_learning_rate': 0.017700837963017992, 'nn_batch_size': 64, 'nn_num_epochs': 84}. Best is trial 14 with value: 24.963293075561523.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:13,394] Trial 15 finished with value: 25.454036712646484 and parameters: {'cat_iterations': 754, 'cat_depth': 10, 'cat_learning_rate': 0.0028265794286415334, 'cat_l2_leaf_reg': 0.0005986746421674544, 'cat_border_count': 102, 'cat_bagging_temperature': 0.12528379012619936, 'nn_n_layers': 5, 'nn_hidden_dim_0': 252, 'nn_hidden_dim_1': 96, 'nn_hidden_dim_2': 112, 'nn_hidden_dim_3': 36, 'nn_hidden_dim_4': 32, 'nn_learning_rate': 0.00460151981296073, 'nn_batch_size': 64, 'nn_num_epochs': 94}. Best is trial 14 with value: 24.963293075561523.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:15,863] Trial 16 finished with value: 23.41659927368164 and parameters: {'cat_iterations': 714, 'cat_depth': 8, 'cat_learning_rate': 0.002724372853603572, 'cat_l2_leaf_reg': 0.0008480307587860994, 'cat_border_count': 114, 'cat_bagging_temperature': 0.22102074486473253, 'nn_n_layers': 5, 'nn_hidden_dim_0': 256, 'nn_hidden_dim_1': 90, 'nn_hidden_dim_2': 90, 'nn_hidden_dim_3': 39, 'nn_hidden_dim_4': 32, 'nn_learning_rate': 0.00556813076690588, 'nn_batch_size': 64, 'nn_num_epochs': 96}. Best is trial 16 with value: 23.41659927368164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:17,188] Trial 17 finished with value: 64.88153839111328 and parameters: {'cat_iterations': 693, 'cat_depth': 8, 'cat_learning_rate': 0.0027268363398023267, 'cat_l2_leaf_reg': 0.041186377165838554, 'cat_border_count': 126, 'cat_bagging_temperature': 0.8537877740307968, 'nn_n_layers': 5, 'nn_hidden_dim_0': 221, 'nn_hidden_dim_1': 81, 'nn_hidden_dim_2': 68, 'nn_hidden_dim_3': 43, 'nn_hidden_dim_4': 38, 'nn_learning_rate': 0.025942798807513278, 'nn_batch_size': 64, 'nn_num_epochs': 88}. Best is trial 16 with value: 23.41659927368164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:18,285] Trial 18 finished with value: 22.49256134033203 and parameters: {'cat_iterations': 521, 'cat_depth': 7, 'cat_learning_rate': 0.02398885310250484, 'cat_l2_leaf_reg': 0.00025593283431546074, 'cat_border_count': 177, 'cat_bagging_temperature': 0.19065651889607088, 'nn_n_layers': 5, 'nn_hidden_dim_0': 255, 'nn_hidden_dim_1': 54, 'nn_hidden_dim_2': 105, 'nn_hidden_dim_3': 106, 'nn_hidden_dim_4': 94, 'nn_learning_rate': 0.009227648463818084, 'nn_batch_size': 64, 'nn_num_epochs': 82}. Best is trial 18 with value: 22.49256134033203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:19,896] Trial 19 finished with value: 20.309091567993164 and parameters: {'cat_iterations': 511, 'cat_depth': 7, 'cat_learning_rate': 0.027418225642950624, 'cat_l2_leaf_reg': 0.09542092371043792, 'cat_border_count': 180, 'cat_bagging_temperature': 0.522598172290182, 'nn_n_layers': 4, 'nn_hidden_dim_0': 227, 'nn_hidden_dim_1': 42, 'nn_hidden_dim_2': 84, 'nn_hidden_dim_3': 119, 'nn_learning_rate': 0.0064802081202078865, 'nn_batch_size': 64, 'nn_num_epochs': 86}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:21,177] Trial 20 finished with value: 41.35484313964844 and parameters: {'cat_iterations': 463, 'cat_depth': 7, 'cat_learning_rate': 0.09265517580598555, 'cat_l2_leaf_reg': 0.18767911643246749, 'cat_border_count': 179, 'cat_bagging_temperature': 2.036794571023788, 'nn_n_layers': 4, 'nn_hidden_dim_0': 220, 'nn_hidden_dim_1': 37, 'nn_hidden_dim_2': 32, 'nn_hidden_dim_3': 122, 'nn_learning_rate': 0.0008314417120736396, 'nn_batch_size': 256, 'nn_num_epochs': 81}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:22,449] Trial 21 finished with value: 29.636262893676758 and parameters: {'cat_iterations': 504, 'cat_depth': 7, 'cat_learning_rate': 0.030105659727194926, 'cat_l2_leaf_reg': 0.0064391854282880015, 'cat_border_count': 172, 'cat_bagging_temperature': 0.4258533940881927, 'nn_n_layers': 5, 'nn_hidden_dim_0': 233, 'nn_hidden_dim_1': 62, 'nn_hidden_dim_2': 83, 'nn_hidden_dim_3': 100, 'nn_hidden_dim_4': 122, 'nn_learning_rate': 0.007500293782840755, 'nn_batch_size': 64, 'nn_num_epochs': 94}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:24,401] Trial 22 finished with value: 29.79831314086914 and parameters: {'cat_iterations': 591, 'cat_depth': 7, 'cat_learning_rate': 0.019729327114456286, 'cat_l2_leaf_reg': 0.0004060804454228586, 'cat_border_count': 203, 'cat_bagging_temperature': 0.0947068166178174, 'nn_n_layers': 4, 'nn_hidden_dim_0': 256, 'nn_hidden_dim_1': 64, 'nn_hidden_dim_2': 86, 'nn_hidden_dim_3': 181, 'nn_learning_rate': 0.0020653427998661017, 'nn_batch_size': 64, 'nn_num_epochs': 100}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:25,776] Trial 23 finished with value: 30.276771545410156 and parameters: {'cat_iterations': 406, 'cat_depth': 8, 'cat_learning_rate': 0.017933831116212478, 'cat_l2_leaf_reg': 0.3369816570878044, 'cat_border_count': 163, 'cat_bagging_temperature': 0.8142725235675462, 'nn_n_layers': 5, 'nn_hidden_dim_0': 201, 'nn_hidden_dim_1': 120, 'nn_hidden_dim_2': 133, 'nn_hidden_dim_3': 167, 'nn_hidden_dim_4': 117, 'nn_learning_rate': 0.006479875043389053, 'nn_batch_size': 64, 'nn_num_epochs': 88}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:26,882] Trial 24 finished with value: 47.26432418823242 and parameters: {'cat_iterations': 386, 'cat_depth': 7, 'cat_learning_rate': 0.12678523582603418, 'cat_l2_leaf_reg': 9.372901705344647, 'cat_border_count': 204, 'cat_bagging_temperature': 0.3068638746835999, 'nn_n_layers': 4, 'nn_hidden_dim_0': 239, 'nn_hidden_dim_1': 60, 'nn_hidden_dim_2': 75, 'nn_hidden_dim_3': 84, 'nn_learning_rate': 0.00251876612701121, 'nn_batch_size': 64, 'nn_num_epochs': 69}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:28,487] Trial 25 finished with value: 28.60442543029785 and parameters: {'cat_iterations': 557, 'cat_depth': 6, 'cat_learning_rate': 0.036757901010255375, 'cat_l2_leaf_reg': 0.011937976158708657, 'cat_border_count': 155, 'cat_bagging_temperature': 1.2612658802221282, 'nn_n_layers': 5, 'nn_hidden_dim_0': 232, 'nn_hidden_dim_1': 35, 'nn_hidden_dim_2': 126, 'nn_hidden_dim_3': 106, 'nn_hidden_dim_4': 94, 'nn_learning_rate': 0.010008118764412828, 'nn_batch_size': 64, 'nn_num_epochs': 79}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:29,809] Trial 26 finished with value: 105.01898193359375 and parameters: {'cat_iterations': 676, 'cat_depth': 8, 'cat_learning_rate': 0.7199996004947458, 'cat_l2_leaf_reg': 8.789626862182154e-05, 'cat_border_count': 224, 'cat_bagging_temperature': 0.06544832177082438, 'nn_n_layers': 4, 'nn_hidden_dim_0': 198, 'nn_hidden_dim_1': 121, 'nn_hidden_dim_2': 58, 'nn_hidden_dim_3': 168, 'nn_learning_rate': 0.03907084788583112, 'nn_batch_size': 64, 'nn_num_epochs': 88}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:31,575] Trial 27 finished with value: 38.92044448852539 and parameters: {'cat_iterations': 589, 'cat_depth': 7, 'cat_learning_rate': 0.05344440190628794, 'cat_l2_leaf_reg': 0.0017407927603900592, 'cat_border_count': 188, 'cat_bagging_temperature': 0.2019526225298585, 'nn_n_layers': 5, 'nn_hidden_dim_0': 33, 'nn_hidden_dim_1': 68, 'nn_hidden_dim_2': 101, 'nn_hidden_dim_3': 64, 'nn_hidden_dim_4': 176, 'nn_learning_rate': 0.004449686198630058, 'nn_batch_size': 64, 'nn_num_epochs': 100}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:33,039] Trial 28 finished with value: 51.418025970458984 and parameters: {'cat_iterations': 507, 'cat_depth': 5, 'cat_learning_rate': 0.004664961231668779, 'cat_l2_leaf_reg': 1.0090280957243209e-05, 'cat_border_count': 126, 'cat_bagging_temperature': 5.440477872312415, 'nn_n_layers': 3, 'nn_hidden_dim_0': 215, 'nn_hidden_dim_1': 115, 'nn_hidden_dim_2': 153, 'nn_learning_rate': 0.0011021199795788247, 'nn_batch_size': 256, 'nn_num_epochs': 91}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:36,363] Trial 29 finished with value: 52.15182876586914 and parameters: {'cat_iterations': 892, 'cat_depth': 8, 'cat_learning_rate': 0.013164487665609981, 'cat_l2_leaf_reg': 0.040682578052427434, 'cat_border_count': 155, 'cat_bagging_temperature': 0.5246372818201105, 'nn_n_layers': 4, 'nn_hidden_dim_0': 244, 'nn_hidden_dim_1': 51, 'nn_hidden_dim_2': 92, 'nn_hidden_dim_3': 142, 'nn_learning_rate': 0.032415479356249534, 'nn_batch_size': 32, 'nn_num_epochs': 71}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:38,319] Trial 30 finished with value: 29.55491828918457 and parameters: {'cat_iterations': 725, 'cat_depth': 6, 'cat_learning_rate': 0.004338509190760786, 'cat_l2_leaf_reg': 0.00013840714898837332, 'cat_border_count': 217, 'cat_bagging_temperature': 0.3059784461653553, 'nn_n_layers': 5, 'nn_hidden_dim_0': 226, 'nn_hidden_dim_1': 81, 'nn_hidden_dim_2': 130, 'nn_hidden_dim_3': 106, 'nn_hidden_dim_4': 82, 'nn_learning_rate': 0.011586192293932413, 'nn_batch_size': 64, 'nn_num_epochs': 62}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:40,175] Trial 31 finished with value: 43.5709342956543 and parameters: {'cat_iterations': 804, 'cat_depth': 9, 'cat_learning_rate': 0.002299919368493591, 'cat_l2_leaf_reg': 0.0008832402269553281, 'cat_border_count': 125, 'cat_bagging_temperature': 0.1675511052941085, 'nn_n_layers': 5, 'nn_hidden_dim_0': 255, 'nn_hidden_dim_1': 98, 'nn_hidden_dim_2': 110, 'nn_hidden_dim_3': 48, 'nn_hidden_dim_4': 61, 'nn_learning_rate': 0.019375303005538285, 'nn_batch_size': 64, 'nn_num_epochs': 81}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:43,599] Trial 32 finished with value: 32.207088470458984 and parameters: {'cat_iterations': 730, 'cat_depth': 9, 'cat_learning_rate': 0.004952385835913591, 'cat_l2_leaf_reg': 0.00019028887482308742, 'cat_border_count': 85, 'cat_bagging_temperature': 0.038547696175505734, 'nn_n_layers': 5, 'nn_hidden_dim_0': 244, 'nn_hidden_dim_1': 74, 'nn_hidden_dim_2': 119, 'nn_hidden_dim_3': 61, 'nn_hidden_dim_4': 64, 'nn_learning_rate': 0.005556490060140146, 'nn_batch_size': 64, 'nn_num_epochs': 83}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:45,781] Trial 33 finished with value: 32.136688232421875 and parameters: {'cat_iterations': 615, 'cat_depth': 9, 'cat_learning_rate': 0.0012187283895804615, 'cat_l2_leaf_reg': 1.2347923391009463e-06, 'cat_border_count': 112, 'cat_bagging_temperature': 0.12381368068429727, 'nn_n_layers': 5, 'nn_hidden_dim_0': 255, 'nn_hidden_dim_1': 49, 'nn_hidden_dim_2': 94, 'nn_hidden_dim_3': 34, 'nn_hidden_dim_4': 158, 'nn_learning_rate': 0.0031901204719837675, 'nn_batch_size': 64, 'nn_num_epochs': 85}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:48,077] Trial 34 finished with value: 37.10438919067383 and parameters: {'cat_iterations': 895, 'cat_depth': 7, 'cat_learning_rate': 0.027386848994451325, 'cat_l2_leaf_reg': 0.002575077693380081, 'cat_border_count': 136, 'cat_bagging_temperature': 0.22326088440430847, 'nn_n_layers': 4, 'nn_hidden_dim_0': 203, 'nn_hidden_dim_1': 98, 'nn_hidden_dim_2': 60, 'nn_hidden_dim_3': 89, 'nn_learning_rate': 0.013434515085735014, 'nn_batch_size': 64, 'nn_num_epochs': 91}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:49,358] Trial 35 finished with value: 110.59685516357422 and parameters: {'cat_iterations': 540, 'cat_depth': 8, 'cat_learning_rate': 0.002159167118312536, 'cat_l2_leaf_reg': 0.011034165050272441, 'cat_border_count': 155, 'cat_bagging_temperature': 0.07191419984508564, 'nn_n_layers': 5, 'nn_hidden_dim_0': 129, 'nn_hidden_dim_1': 87, 'nn_hidden_dim_2': 142, 'nn_hidden_dim_3': 59, 'nn_hidden_dim_4': 38, 'nn_learning_rate': 0.06462210894096143, 'nn_batch_size': 64, 'nn_num_epochs': 77}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:52,492] Trial 36 finished with value: 34.46031188964844 and parameters: {'cat_iterations': 338, 'cat_depth': 6, 'cat_learning_rate': 0.0011305736069199704, 'cat_l2_leaf_reg': 0.15148187708616523, 'cat_border_count': 72, 'cat_bagging_temperature': 0.03775199432779406, 'nn_n_layers': 4, 'nn_hidden_dim_0': 231, 'nn_hidden_dim_1': 32, 'nn_hidden_dim_2': 77, 'nn_hidden_dim_3': 127, 'nn_learning_rate': 0.021003696225264788, 'nn_batch_size': 32, 'nn_num_epochs': 95}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:53,685] Trial 37 finished with value: 32.56877517700195 and parameters: {'cat_iterations': 240, 'cat_depth': 9, 'cat_learning_rate': 0.0036603370055138837, 'cat_l2_leaf_reg': 2.296130542706894e-07, 'cat_border_count': 111, 'cat_bagging_temperature': 0.013245889185073067, 'nn_n_layers': 4, 'nn_hidden_dim_0': 67, 'nn_hidden_dim_1': 137, 'nn_hidden_dim_2': 98, 'nn_hidden_dim_3': 156, 'nn_learning_rate': 0.0034456610981849546, 'nn_batch_size': 64, 'nn_num_epochs': 53}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:55,229] Trial 38 finished with value: 46.04986572265625 and parameters: {'cat_iterations': 771, 'cat_depth': 8, 'cat_learning_rate': 0.012116484395332815, 'cat_l2_leaf_reg': 6.407564880035898e-06, 'cat_border_count': 189, 'cat_bagging_temperature': 1.283448931692552, 'nn_n_layers': 5, 'nn_hidden_dim_0': 210, 'nn_hidden_dim_1': 52, 'nn_hidden_dim_2': 107, 'nn_hidden_dim_3': 76, 'nn_hidden_dim_4': 72, 'nn_learning_rate': 0.01483318623280657, 'nn_batch_size': 64, 'nn_num_epochs': 62}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:57,258] Trial 39 finished with value: 178.81141662597656 and parameters: {'cat_iterations': 667, 'cat_depth': 9, 'cat_learning_rate': 0.05200455596321849, 'cat_l2_leaf_reg': 0.0011942440401233759, 'cat_border_count': 170, 'cat_bagging_temperature': 0.6332415913589208, 'nn_n_layers': 2, 'nn_hidden_dim_0': 181, 'nn_hidden_dim_1': 111, 'nn_learning_rate': 0.04883725495700957, 'nn_batch_size': 128, 'nn_num_epochs': 74}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:17:59,926] Trial 40 finished with value: 43.59765625 and parameters: {'cat_iterations': 855, 'cat_depth': 6, 'cat_learning_rate': 0.1345425983188248, 'cat_l2_leaf_reg': 0.02683951811313384, 'cat_border_count': 145, 'cat_bagging_temperature': 0.33082703437652855, 'nn_n_layers': 3, 'nn_hidden_dim_0': 243, 'nn_hidden_dim_1': 159, 'nn_hidden_dim_2': 40, 'nn_learning_rate': 0.007051579203897532, 'nn_batch_size': 64, 'nn_num_epochs': 85}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:02,003] Trial 41 finished with value: 31.285429000854492 and parameters: {'cat_iterations': 757, 'cat_depth': 10, 'cat_learning_rate': 0.003341817097345764, 'cat_l2_leaf_reg': 0.00044300662106400276, 'cat_border_count': 85, 'cat_bagging_temperature': 0.1284098674096758, 'nn_n_layers': 5, 'nn_hidden_dim_0': 249, 'nn_hidden_dim_1': 93, 'nn_hidden_dim_2': 117, 'nn_hidden_dim_3': 32, 'nn_hidden_dim_4': 34, 'nn_learning_rate': 0.0045226068048682235, 'nn_batch_size': 64, 'nn_num_epochs': 95}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:04,109] Trial 42 finished with value: 28.716140747070312 and parameters: {'cat_iterations': 825, 'cat_depth': 10, 'cat_learning_rate': 0.0018150909365519572, 'cat_l2_leaf_reg': 3.14582677581684e-05, 'cat_border_count': 99, 'cat_bagging_temperature': 0.15685445463059947, 'nn_n_layers': 5, 'nn_hidden_dim_0': 235, 'nn_hidden_dim_1': 134, 'nn_hidden_dim_2': 109, 'nn_hidden_dim_3': 48, 'nn_hidden_dim_4': 33, 'nn_learning_rate': 0.005193200123427891, 'nn_batch_size': 64, 'nn_num_epochs': 97}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:06,225] Trial 43 finished with value: 31.013545989990234 and parameters: {'cat_iterations': 713, 'cat_depth': 10, 'cat_learning_rate': 0.00643216767157898, 'cat_l2_leaf_reg': 0.000481362077158138, 'cat_border_count': 114, 'cat_bagging_temperature': 0.07957240241844456, 'nn_n_layers': 5, 'nn_hidden_dim_0': 247, 'nn_hidden_dim_1': 108, 'nn_hidden_dim_2': 145, 'nn_hidden_dim_3': 197, 'nn_hidden_dim_4': 65, 'nn_learning_rate': 0.009142575439701569, 'nn_batch_size': 64, 'nn_num_epochs': 92}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:07,264] Trial 44 finished with value: 42.31686019897461 and parameters: {'cat_iterations': 457, 'cat_depth': 7, 'cat_learning_rate': 0.0032309774607539774, 'cat_l2_leaf_reg': 0.004001715873248488, 'cat_border_count': 98, 'cat_bagging_temperature': 0.04554053900116203, 'nn_n_layers': 4, 'nn_hidden_dim_0': 221, 'nn_hidden_dim_1': 76, 'nn_hidden_dim_2': 53, 'nn_hidden_dim_3': 68, 'nn_learning_rate': 0.0015504468751679032, 'nn_batch_size': 32, 'nn_num_epochs': 86}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:10,060] Trial 45 finished with value: 98.01847076416016 and parameters: {'cat_iterations': 941, 'cat_depth': 10, 'cat_learning_rate': 0.009802620134351275, 'cat_l2_leaf_reg': 5.0510465562805114e-05, 'cat_border_count': 143, 'cat_bagging_temperature': 0.02604868094839366, 'nn_n_layers': 5, 'nn_hidden_dim_0': 163, 'nn_hidden_dim_1': 100, 'nn_hidden_dim_2': 72, 'nn_hidden_dim_3': 49, 'nn_hidden_dim_4': 95, 'nn_learning_rate': 0.024662299545598024, 'nn_batch_size': 64, 'nn_num_epochs': 100}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:14,634] Trial 46 finished with value: 103.65399169921875 and parameters: {'cat_iterations': 761, 'cat_depth': 9, 'cat_learning_rate': 0.3683528609776272, 'cat_l2_leaf_reg': 0.00018167580703972892, 'cat_border_count': 54, 'cat_bagging_temperature': 0.0980636857779455, 'nn_n_layers': 5, 'nn_hidden_dim_0': 256, 'nn_hidden_dim_1': 53, 'nn_hidden_dim_2': 193, 'nn_hidden_dim_3': 117, 'nn_hidden_dim_4': 56, 'nn_learning_rate': 0.016130546439125, 'nn_batch_size': 256, 'nn_num_epochs': 74}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:15,882] Trial 47 finished with value: 22.77091407775879 and parameters: {'cat_iterations': 568, 'cat_depth': 8, 'cat_learning_rate': 0.006179037510979338, 'cat_l2_leaf_reg': 0.0009182791502565052, 'cat_border_count': 76, 'cat_bagging_temperature': 0.34617964386630096, 'nn_n_layers': 4, 'nn_hidden_dim_0': 188, 'nn_hidden_dim_1': 168, 'nn_hidden_dim_2': 121, 'nn_hidden_dim_3': 32, 'nn_learning_rate': 0.0027396657404384736, 'nn_batch_size': 128, 'nn_num_epochs': 90}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:16,818] Trial 48 finished with value: 35.95827102661133 and parameters: {'cat_iterations': 572, 'cat_depth': 8, 'cat_learning_rate': 0.006645730331809005, 'cat_l2_leaf_reg': 0.916420351541079, 'cat_border_count': 79, 'cat_bagging_temperature': 0.22363340588177394, 'nn_n_layers': 3, 'nn_hidden_dim_0': 208, 'nn_hidden_dim_1': 217, 'nn_hidden_dim_2': 250, 'nn_learning_rate': 0.002289833892803149, 'nn_batch_size': 128, 'nn_num_epochs': 40}. Best is trial 19 with value: 20.309091567993164.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2959969898.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:18:18,133] Trial 49 finished with value: 39.078609466552734 and parameters: {'cat_iterations': 512, 'cat_depth': 8, 'cat_learning_rate': 0.014462492370445832, 'cat_l2_leaf_reg': 0.0014366937001826434, 'cat_border_count': 61, 'cat_bagging_temperature': 0.4024017812328633, 'nn_n_layers': 4, 'nn_hidden_dim_0': 194, 'nn_hidden_dim_1': 167, 'nn_hidden_dim_2': 89, 'nn_hidden_dim_3': 89, 'nn_learning_rate': 0.0006301806513540334, 'nn_batch_size': 128, 'nn_num_epochs': 66}. Best is trial 19 with value: 20.309091567993164.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "cat_iterations: 511\n",
      "cat_depth: 7\n",
      "cat_learning_rate: 0.027418225642950624\n",
      "cat_l2_leaf_reg: 0.09542092371043792\n",
      "cat_border_count: 180\n",
      "cat_bagging_temperature: 0.522598172290182\n",
      "nn_n_layers: 4\n",
      "nn_hidden_dim_0: 227\n",
      "nn_hidden_dim_1: 42\n",
      "nn_hidden_dim_2: 84\n",
      "nn_hidden_dim_3: 119\n",
      "nn_learning_rate: 0.0064802081202078865\n",
      "nn_batch_size: 64\n",
      "nn_num_epochs: 86\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for CatBoost\n",
    "    cat_params = {\n",
    "        'iterations': trial.suggest_int('cat_iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('cat_depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('cat_learning_rate', 1e-3, 1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('cat_l2_leaf_reg', 1e-8, 10),\n",
    "        'border_count': trial.suggest_int('cat_border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_loguniform('cat_bagging_temperature', 0.01, 100.0)\n",
    "    }\n",
    "\n",
    "    # Train CatBoost model\n",
    "    cat_model = CatBoostRegressor(**cat_params, verbose=False)\n",
    "    cat_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the data using the CatBoost model\n",
    "    X_train_transformed = cat_model.calc_leaf_indexes(X_train_scaled)\n",
    "    X_test_transformed = cat_model.calc_leaf_indexes(X_test_scaled)\n",
    "\n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    n_layers = trial.suggest_int('nn_n_layers', 1, 5)\n",
    "    hidden_dims = [trial.suggest_int(f'nn_hidden_dim_{i}', 32, 256) for i in range(n_layers)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    nn_batch_size = trial.suggest_categorical('nn_batch_size', [32, 64, 128, 256])\n",
    "    nn_num_epochs = trial.suggest_int('nn_num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    nn_model = NeuralNetwork(X_train_transformed.shape[1], hidden_dims).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=nn_batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(nn_num_epochs):\n",
    "        nn_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final CatBoost model with the best hyperparameters\n",
    "cat_params = {\n",
    "    'iterations': best_params['cat_iterations'],\n",
    "    'depth': best_params['cat_depth'],\n",
    "    'learning_rate': best_params['cat_learning_rate'],\n",
    "    'l2_leaf_reg': best_params['cat_l2_leaf_reg'],\n",
    "    'border_count': best_params['cat_border_count'],\n",
    "    'bagging_temperature': best_params['cat_bagging_temperature']\n",
    "}\n",
    "cat_model = CatBoostRegressor(**cat_params, verbose=False)\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the data using the CatBoost model\n",
    "X_train_transformed = cat_model.calc_leaf_indexes(X_train_scaled)\n",
    "X_test_transformed = cat_model.calc_leaf_indexes(X_test_scaled)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_model = NeuralNetwork(X_train_transformed.shape[1], \n",
    "                         [best_params[f'nn_hidden_dim_{i}'] for i in range(best_params['nn_n_layers'])]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=best_params['nn_learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_transformed).to(device), y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['nn_batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['nn_num_epochs']):\n",
    "    nn_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = nn_model(torch.FloatTensor(X_test_transformed).to(device))\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['CatBoost + NN'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:23:37,592] A new study created in memory with name: no-name-72a72849-e5aa-409c-9409-fb80cc019d2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:39,866] Trial 0 finished with value: 53.65874481201172 and parameters: {'num_heads': 8, 'embedding_dim': 216, 'num_layers': 5, 'dropout': 0.33285250940195116, 'learning_rate': 0.00016484901891047237, 'batch_size': 128, 'num_epochs': 83}. Best is trial 0 with value: 53.65874481201172.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:40,821] Trial 1 finished with value: 29091127296.0 and parameters: {'num_heads': 7, 'embedding_dim': 217, 'num_layers': 5, 'dropout': 0.4136746604752212, 'learning_rate': 0.08854690893073805, 'batch_size': 128, 'num_epochs': 35}. Best is trial 0 with value: 53.65874481201172.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:41,323] Trial 2 finished with value: 74.18865966796875 and parameters: {'num_heads': 3, 'embedding_dim': 138, 'num_layers': 1, 'dropout': 0.46379974863399825, 'learning_rate': 0.00033691204787698946, 'batch_size': 256, 'num_epochs': 89}. Best is trial 0 with value: 53.65874481201172.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:42,456] Trial 3 finished with value: 64.91082763671875 and parameters: {'num_heads': 1, 'embedding_dim': 236, 'num_layers': 2, 'dropout': 0.36344317543858323, 'learning_rate': 0.008180700260364705, 'batch_size': 64, 'num_epochs': 78}. Best is trial 0 with value: 53.65874481201172.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:42,850] Trial 4 finished with value: 50.03213882446289 and parameters: {'num_heads': 3, 'embedding_dim': 246, 'num_layers': 3, 'dropout': 0.27545871970605873, 'learning_rate': 0.0017818502812301923, 'batch_size': 256, 'num_epochs': 17}. Best is trial 4 with value: 50.03213882446289.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:43,421] Trial 5 finished with value: 157.81375122070312 and parameters: {'num_heads': 6, 'embedding_dim': 222, 'num_layers': 2, 'dropout': 0.19980519295974325, 'learning_rate': 0.02608256485241417, 'batch_size': 128, 'num_epochs': 48}. Best is trial 4 with value: 50.03213882446289.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:44,399] Trial 6 finished with value: 77.82630157470703 and parameters: {'num_heads': 3, 'embedding_dim': 54, 'num_layers': 4, 'dropout': 0.36124138579709897, 'learning_rate': 0.030571992470796908, 'batch_size': 64, 'num_epochs': 92}. Best is trial 4 with value: 50.03213882446289.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:44,615] Trial 7 finished with value: 6.277908802032471 and parameters: {'num_heads': 8, 'embedding_dim': 128, 'num_layers': 1, 'dropout': 0.38534842586009344, 'learning_rate': 0.014493440058621784, 'batch_size': 256, 'num_epochs': 37}. Best is trial 7 with value: 6.277908802032471.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:44,913] Trial 8 finished with value: 33.407588958740234 and parameters: {'num_heads': 7, 'embedding_dim': 77, 'num_layers': 2, 'dropout': 0.24224040006498737, 'learning_rate': 0.02491664652253574, 'batch_size': 32, 'num_epochs': 33}. Best is trial 7 with value: 6.277908802032471.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:45,404] Trial 9 finished with value: 68.89391326904297 and parameters: {'num_heads': 1, 'embedding_dim': 245, 'num_layers': 2, 'dropout': 0.3241965519154544, 'learning_rate': 0.00344771179442097, 'batch_size': 64, 'num_epochs': 34}. Best is trial 7 with value: 6.277908802032471.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:45,805] Trial 10 finished with value: 5.281002044677734 and parameters: {'num_heads': 5, 'embedding_dim': 145, 'num_layers': 1, 'dropout': 0.03520169357535663, 'learning_rate': 0.0009986119457968065, 'batch_size': 256, 'num_epochs': 63}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:46,230] Trial 11 finished with value: 41.956607818603516 and parameters: {'num_heads': 5, 'embedding_dim': 140, 'num_layers': 1, 'dropout': 0.04176655367617603, 'learning_rate': 0.0008228959502276398, 'batch_size': 256, 'num_epochs': 58}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:46,644] Trial 12 finished with value: 6.161325454711914 and parameters: {'num_heads': 5, 'embedding_dim': 95, 'num_layers': 1, 'dropout': 0.07002493469512272, 'learning_rate': 0.006109258899456271, 'batch_size': 256, 'num_epochs': 65}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:47,122] Trial 13 finished with value: 28.805635452270508 and parameters: {'num_heads': 5, 'embedding_dim': 10, 'num_layers': 3, 'dropout': 0.001699457111924818, 'learning_rate': 0.0034965355042527774, 'batch_size': 256, 'num_epochs': 71}. Best is trial 10 with value: 5.281002044677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:47,484] Trial 14 finished with value: 60.3777961730957 and parameters: {'num_heads': 4, 'embedding_dim': 100, 'num_layers': 1, 'dropout': 0.11211295194094209, 'learning_rate': 0.0008023510494956127, 'batch_size': 32, 'num_epochs': 64}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:47,857] Trial 15 finished with value: 6.182893753051758 and parameters: {'num_heads': 6, 'embedding_dim': 174, 'num_layers': 1, 'dropout': 0.11963757401032202, 'learning_rate': 0.001137384668814419, 'batch_size': 256, 'num_epochs': 48}. Best is trial 10 with value: 5.281002044677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:49,151] Trial 16 finished with value: 90.89788818359375 and parameters: {'num_heads': 4, 'embedding_dim': 184, 'num_layers': 4, 'dropout': 0.10057496318325596, 'learning_rate': 0.006158059199845575, 'batch_size': 256, 'num_epochs': 68}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:49,769] Trial 17 finished with value: 57.70378112792969 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 2, 'dropout': 0.16380049168901567, 'learning_rate': 0.0004022825732525661, 'batch_size': 256, 'num_epochs': 99}. Best is trial 10 with value: 5.281002044677734.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:50,414] Trial 18 finished with value: 103.95336151123047 and parameters: {'num_heads': 2, 'embedding_dim': 106, 'num_layers': 3, 'dropout': 0.052158360829040726, 'learning_rate': 0.0001287256696180853, 'batch_size': 32, 'num_epochs': 53}. Best is trial 10 with value: 5.281002044677734.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:50,962] Trial 19 finished with value: 2.74469256401062 and parameters: {'num_heads': 5, 'embedding_dim': 180, 'num_layers': 1, 'dropout': 0.0066175941726056495, 'learning_rate': 0.0020323297062603087, 'batch_size': 256, 'num_epochs': 75}. Best is trial 19 with value: 2.74469256401062.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:52,002] Trial 20 finished with value: 6.323418140411377 and parameters: {'num_heads': 4, 'embedding_dim': 168, 'num_layers': 3, 'dropout': 0.028734276752224473, 'learning_rate': 0.001842249320042009, 'batch_size': 256, 'num_epochs': 76}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:52,466] Trial 21 finished with value: 4.572091579437256 and parameters: {'num_heads': 5, 'embedding_dim': 185, 'num_layers': 1, 'dropout': 0.07582900226637254, 'learning_rate': 0.006368939550663924, 'batch_size': 256, 'num_epochs': 64}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:52,908] Trial 22 finished with value: 4.515966415405273 and parameters: {'num_heads': 5, 'embedding_dim': 195, 'num_layers': 1, 'dropout': 0.15826287677859194, 'learning_rate': 0.0017598197109962095, 'batch_size': 256, 'num_epochs': 60}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:53,814] Trial 23 finished with value: 14.512372016906738 and parameters: {'num_heads': 6, 'embedding_dim': 192, 'num_layers': 2, 'dropout': 0.16103207474067674, 'learning_rate': 0.001974702260960885, 'batch_size': 256, 'num_epochs': 77}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:54,211] Trial 24 finished with value: 16.329065322875977 and parameters: {'num_heads': 7, 'embedding_dim': 196, 'num_layers': 1, 'dropout': 0.17757488523772974, 'learning_rate': 0.01124328460855679, 'batch_size': 256, 'num_epochs': 55}. Best is trial 19 with value: 2.74469256401062.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:54,738] Trial 25 finished with value: 7.215566158294678 and parameters: {'num_heads': 4, 'embedding_dim': 164, 'num_layers': 2, 'dropout': 0.003621996266396331, 'learning_rate': 0.0033702661205543124, 'batch_size': 256, 'num_epochs': 43}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:55,407] Trial 26 finished with value: 7.208953380584717 and parameters: {'num_heads': 5, 'embedding_dim': 205, 'num_layers': 1, 'dropout': 0.08432552963782369, 'learning_rate': 0.00039651028283294433, 'batch_size': 64, 'num_epochs': 86}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:55,883] Trial 27 finished with value: 18.755489349365234 and parameters: {'num_heads': 6, 'embedding_dim': 150, 'num_layers': 1, 'dropout': 0.13506028493680192, 'learning_rate': 0.004861279573150919, 'batch_size': 32, 'num_epochs': 72}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:56,373] Trial 28 finished with value: 22.926715850830078 and parameters: {'num_heads': 3, 'embedding_dim': 120, 'num_layers': 2, 'dropout': 0.2355781570979976, 'learning_rate': 0.0020286798835243234, 'batch_size': 128, 'num_epochs': 59}. Best is trial 19 with value: 2.74469256401062.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:57,070] Trial 29 finished with value: 51.654052734375 and parameters: {'num_heads': 8, 'embedding_dim': 224, 'num_layers': 5, 'dropout': 0.20161005780992702, 'learning_rate': 0.00023294850837806498, 'batch_size': 128, 'num_epochs': 23}. Best is trial 19 with value: 2.74469256401062.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:57,770] Trial 30 finished with value: 7.861839294433594 and parameters: {'num_heads': 4, 'embedding_dim': 256, 'num_layers': 1, 'dropout': 0.2855433409582599, 'learning_rate': 0.0006089809475038289, 'batch_size': 256, 'num_epochs': 80}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:58,232] Trial 31 finished with value: 3.4363818168640137 and parameters: {'num_heads': 5, 'embedding_dim': 155, 'num_layers': 1, 'dropout': 0.03940952151310004, 'learning_rate': 0.0012464065323723923, 'batch_size': 256, 'num_epochs': 64}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:58,740] Trial 32 finished with value: 4.817664623260498 and parameters: {'num_heads': 5, 'embedding_dim': 160, 'num_layers': 1, 'dropout': 0.06934498697785735, 'learning_rate': 0.0025133978127118133, 'batch_size': 256, 'num_epochs': 72}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:59,218] Trial 33 finished with value: 3.397747039794922 and parameters: {'num_heads': 6, 'embedding_dim': 204, 'num_layers': 1, 'dropout': 0.08781632884987173, 'learning_rate': 0.0013483244697027994, 'batch_size': 256, 'num_epochs': 61}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:23:59,891] Trial 34 finished with value: 14.79455852508545 and parameters: {'num_heads': 7, 'embedding_dim': 210, 'num_layers': 2, 'dropout': 0.137392351139701, 'learning_rate': 0.0014090030688151457, 'batch_size': 256, 'num_epochs': 51}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:00,541] Trial 35 finished with value: 15.795147895812988 and parameters: {'num_heads': 6, 'embedding_dim': 204, 'num_layers': 1, 'dropout': 0.4857888411270735, 'learning_rate': 0.0006522899947128823, 'batch_size': 256, 'num_epochs': 84}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:02,965] Trial 36 finished with value: 8.156411170959473 and parameters: {'num_heads': 7, 'embedding_dim': 231, 'num_layers': 4, 'dropout': 0.00849367447689596, 'learning_rate': 0.0014354122640255927, 'batch_size': 128, 'num_epochs': 60}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:03,537] Trial 37 finished with value: 67.63347625732422 and parameters: {'num_heads': 6, 'embedding_dim': 180, 'num_layers': 2, 'dropout': 0.09226910200516339, 'learning_rate': 0.00022813115287979863, 'batch_size': 64, 'num_epochs': 44}. Best is trial 19 with value: 2.74469256401062.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:04,059] Trial 38 finished with value: 41.578121185302734 and parameters: {'num_heads': 4, 'embedding_dim': 160, 'num_layers': 1, 'dropout': 0.05240665056256763, 'learning_rate': 0.0005099731099477786, 'batch_size': 256, 'num_epochs': 70}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:05,087] Trial 39 finished with value: 42.41852569580078 and parameters: {'num_heads': 3, 'embedding_dim': 120, 'num_layers': 2, 'dropout': 0.43461059107442135, 'learning_rate': 0.0011780202566514702, 'batch_size': 256, 'num_epochs': 93}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:06,217] Trial 40 finished with value: 8.199254989624023 and parameters: {'num_heads': 6, 'embedding_dim': 210, 'num_layers': 1, 'dropout': 0.21346427338725057, 'learning_rate': 0.08477612403816583, 'batch_size': 128, 'num_epochs': 81}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:06,758] Trial 41 finished with value: 3.534100294113159 and parameters: {'num_heads': 5, 'embedding_dim': 190, 'num_layers': 1, 'dropout': 0.07335517631940074, 'learning_rate': 0.0026002552343430156, 'batch_size': 256, 'num_epochs': 65}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:07,246] Trial 42 finished with value: 4.221861839294434 and parameters: {'num_heads': 5, 'embedding_dim': 195, 'num_layers': 1, 'dropout': 0.029527414792567923, 'learning_rate': 0.0027380939028186016, 'batch_size': 256, 'num_epochs': 58}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:07,734] Trial 43 finished with value: 3.0341479778289795 and parameters: {'num_heads': 5, 'embedding_dim': 220, 'num_layers': 1, 'dropout': 0.026138427116485712, 'learning_rate': 0.0024666132130170162, 'batch_size': 256, 'num_epochs': 56}. Best is trial 19 with value: 2.74469256401062.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:08,310] Trial 44 finished with value: 2.5881154537200928 and parameters: {'num_heads': 6, 'embedding_dim': 222, 'num_layers': 1, 'dropout': 0.024202145211287017, 'learning_rate': 0.0045265695878455746, 'batch_size': 256, 'num_epochs': 67}. Best is trial 44 with value: 2.5881154537200928.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:09,060] Trial 45 finished with value: 8.20875358581543 and parameters: {'num_heads': 7, 'embedding_dim': 238, 'num_layers': 2, 'dropout': 0.02631921978941521, 'learning_rate': 0.0045204721085189414, 'batch_size': 64, 'num_epochs': 50}. Best is trial 44 with value: 2.5881154537200928.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:09,693] Trial 46 finished with value: 3.4861233234405518 and parameters: {'num_heads': 6, 'embedding_dim': 222, 'num_layers': 1, 'dropout': 0.0543559497961129, 'learning_rate': 0.008948201728959562, 'batch_size': 256, 'num_epochs': 74}. Best is trial 44 with value: 2.5881154537200928.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:09,923] Trial 47 finished with value: 48.270782470703125 and parameters: {'num_heads': 7, 'embedding_dim': 231, 'num_layers': 2, 'dropout': 0.01949542348237558, 'learning_rate': 0.0038249123499711404, 'batch_size': 32, 'num_epochs': 11}. Best is trial 44 with value: 2.5881154537200928.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:10,536] Trial 48 finished with value: 1.1762311458587646 and parameters: {'num_heads': 6, 'embedding_dim': 252, 'num_layers': 1, 'dropout': 0.04690876367120661, 'learning_rate': 0.020609742032534902, 'batch_size': 256, 'num_epochs': 68}. Best is trial 48 with value: 1.1762311458587646.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2454648724.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:24:11,189] Trial 49 finished with value: 10.746000289916992 and parameters: {'num_heads': 8, 'embedding_dim': 256, 'num_layers': 1, 'dropout': 0.11541168666620555, 'learning_rate': 0.01495719666188772, 'batch_size': 256, 'num_epochs': 67}. Best is trial 48 with value: 1.1762311458587646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_heads: 6\n",
      "embedding_dim: 252\n",
      "num_layers: 1\n",
      "dropout: 0.04690876367120661\n",
      "learning_rate: 0.020609742032534902\n",
      "batch_size: 256\n",
      "num_epochs: 68\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class AutoInt(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers, dropout):\n",
    "        super(AutoInt, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.multi_head_attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(0)  # Add sequence dimension\n",
    "        for attention in self.multi_head_attentions:\n",
    "            x, _ = attention(x, x, x)\n",
    "        x = x.squeeze(0)  # Remove sequence dimension\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', num_heads, 256, step=num_heads)  # Ensure divisibility\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the AutoInt model\n",
    "    model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final AutoInt model with the best hyperparameters\n",
    "best_model = AutoInt(X_train.shape[1], \n",
    "                     best_params['embedding_dim'], \n",
    "                     best_params['num_heads'], \n",
    "                     best_params['num_layers'], \n",
    "                     best_params['dropout']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['AutoInt'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:21:41,082] A new study created in memory with name: no-name-c47986e0-ae22-4f5f-baba-0d189ed3e6a9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:41,622] Trial 0 finished with value: 103.96244812011719 and parameters: {'num_tokens': 58, 'heads': 4, 'dim': 52, 'depth': 1, 'mlp_dim': 90, 'dropout': 0.4294006564307398, 'learning_rate': 0.0018117934028063763, 'batch_size': 64, 'num_epochs': 45}. Best is trial 0 with value: 103.96244812011719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:42,882] Trial 1 finished with value: 107.64453887939453 and parameters: {'num_tokens': 93, 'heads': 1, 'dim': 145, 'depth': 4, 'mlp_dim': 222, 'dropout': 0.43217741185569525, 'learning_rate': 0.009877085966135644, 'batch_size': 128, 'num_epochs': 27}. Best is trial 0 with value: 103.96244812011719.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:44,394] Trial 2 finished with value: 92.91849517822266 and parameters: {'num_tokens': 164, 'heads': 7, 'dim': 196, 'depth': 3, 'mlp_dim': 235, 'dropout': 0.24986271063164328, 'learning_rate': 0.012037356564501252, 'batch_size': 128, 'num_epochs': 16}. Best is trial 2 with value: 92.91849517822266.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:45,282] Trial 3 finished with value: 53.39561462402344 and parameters: {'num_tokens': 178, 'heads': 2, 'dim': 128, 'depth': 1, 'mlp_dim': 68, 'dropout': 0.049205560454434694, 'learning_rate': 0.01565752754540425, 'batch_size': 128, 'num_epochs': 65}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:47,416] Trial 4 finished with value: 109.21041870117188 and parameters: {'num_tokens': 127, 'heads': 3, 'dim': 24, 'depth': 5, 'mlp_dim': 78, 'dropout': 0.22932534084590533, 'learning_rate': 0.0005460210699660324, 'batch_size': 128, 'num_epochs': 66}. Best is trial 3 with value: 53.39561462402344.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:48,808] Trial 5 finished with value: 77.10852813720703 and parameters: {'num_tokens': 42, 'heads': 2, 'dim': 164, 'depth': 3, 'mlp_dim': 212, 'dropout': 0.15303092770913035, 'learning_rate': 0.004253269749877879, 'batch_size': 32, 'num_epochs': 39}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:21:54,174] Trial 6 finished with value: 102.27018737792969 and parameters: {'num_tokens': 34, 'heads': 5, 'dim': 175, 'depth': 5, 'mlp_dim': 236, 'dropout': 0.08263435984676776, 'learning_rate': 0.0001913007068555298, 'batch_size': 64, 'num_epochs': 70}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:00,018] Trial 7 finished with value: 104.48876953125 and parameters: {'num_tokens': 167, 'heads': 5, 'dim': 255, 'depth': 5, 'mlp_dim': 245, 'dropout': 0.37971296863449455, 'learning_rate': 0.014744146763975442, 'batch_size': 64, 'num_epochs': 43}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:01,167] Trial 8 finished with value: 71.32296752929688 and parameters: {'num_tokens': 63, 'heads': 6, 'dim': 42, 'depth': 2, 'mlp_dim': 87, 'dropout': 0.44591222527104335, 'learning_rate': 0.009057909431967295, 'batch_size': 256, 'num_epochs': 67}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:03,506] Trial 9 finished with value: 73.54039001464844 and parameters: {'num_tokens': 66, 'heads': 7, 'dim': 63, 'depth': 5, 'mlp_dim': 60, 'dropout': 9.517237460854311e-05, 'learning_rate': 0.01852569548077901, 'batch_size': 128, 'num_epochs': 62}. Best is trial 3 with value: 53.39561462402344.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:04,848] Trial 10 finished with value: 132.63272094726562 and parameters: {'num_tokens': 235, 'heads': 1, 'dim': 101, 'depth': 1, 'mlp_dim': 146, 'dropout': 0.048642342156703225, 'learning_rate': 0.07858284894702565, 'batch_size': 256, 'num_epochs': 97}. Best is trial 3 with value: 53.39561462402344.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:07,576] Trial 11 finished with value: 48.435604095458984 and parameters: {'num_tokens': 224, 'heads': 6, 'dim': 96, 'depth': 2, 'mlp_dim': 121, 'dropout': 0.3454882352920463, 'learning_rate': 0.08548934643893764, 'batch_size': 256, 'num_epochs': 83}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:10,641] Trial 12 finished with value: 67.22977447509766 and parameters: {'num_tokens': 245, 'heads': 8, 'dim': 104, 'depth': 2, 'mlp_dim': 32, 'dropout': 0.32127674612325735, 'learning_rate': 0.09905030338101457, 'batch_size': 256, 'num_epochs': 92}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:12,674] Trial 13 finished with value: 140.93568420410156 and parameters: {'num_tokens': 198, 'heads': 3, 'dim': 99, 'depth': 2, 'mlp_dim': 131, 'dropout': 0.3312724947137461, 'learning_rate': 0.038036803644113604, 'batch_size': 32, 'num_epochs': 84}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:14,111] Trial 14 finished with value: 87.74053192138672 and parameters: {'num_tokens': 205, 'heads': 4, 'dim': 132, 'depth': 1, 'mlp_dim': 146, 'dropout': 0.19061357360678588, 'learning_rate': 0.03500943023363143, 'batch_size': 256, 'num_epochs': 80}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:16,287] Trial 15 finished with value: 87.76346588134766 and parameters: {'num_tokens': 202, 'heads': 3, 'dim': 78, 'depth': 2, 'mlp_dim': 118, 'dropout': 0.13853860682445693, 'learning_rate': 0.0027663363042562724, 'batch_size': 128, 'num_epochs': 79}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:19,730] Trial 16 finished with value: 122.43695068359375 and parameters: {'num_tokens': 139, 'heads': 6, 'dim': 210, 'depth': 3, 'mlp_dim': 186, 'dropout': 0.30834464933374855, 'learning_rate': 0.03839508824800793, 'batch_size': 256, 'num_epochs': 55}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:20,286] Trial 17 finished with value: 108.78157043457031 and parameters: {'num_tokens': 223, 'heads': 2, 'dim': 12, 'depth': 1, 'mlp_dim': 174, 'dropout': 0.11886013387113742, 'learning_rate': 0.0009178138769975269, 'batch_size': 32, 'num_epochs': 55}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:24,444] Trial 18 finished with value: 71.1798324584961 and parameters: {'num_tokens': 256, 'heads': 6, 'dim': 120, 'depth': 4, 'mlp_dim': 110, 'dropout': 0.37538192377228063, 'learning_rate': 0.005161885068685567, 'batch_size': 128, 'num_epochs': 88}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:25,977] Trial 19 finished with value: 119.29498291015625 and parameters: {'num_tokens': 173, 'heads': 8, 'dim': 80, 'depth': 2, 'mlp_dim': 32, 'dropout': 0.4983123696288602, 'learning_rate': 0.05419298905247309, 'batch_size': 256, 'num_epochs': 73}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:31,613] Trial 20 finished with value: 104.68060302734375 and parameters: {'num_tokens': 116, 'heads': 2, 'dim': 150, 'depth': 6, 'mlp_dim': 60, 'dropout': 0.19323698785959603, 'learning_rate': 0.025764092024492764, 'batch_size': 256, 'num_epochs': 58}. Best is trial 11 with value: 48.435604095458984.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:34,092] Trial 21 finished with value: 143.95803833007812 and parameters: {'num_tokens': 248, 'heads': 8, 'dim': 112, 'depth': 2, 'mlp_dim': 39, 'dropout': 0.2895172873444989, 'learning_rate': 0.09337353889032503, 'batch_size': 256, 'num_epochs': 100}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:35,325] Trial 22 finished with value: 58.82682800292969 and parameters: {'num_tokens': 223, 'heads': 7, 'dim': 91, 'depth': 1, 'mlp_dim': 58, 'dropout': 0.36048068729199145, 'learning_rate': 0.08386697408131581, 'batch_size': 256, 'num_epochs': 91}. Best is trial 11 with value: 48.435604095458984.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:37,078] Trial 23 finished with value: 45.070899963378906 and parameters: {'num_tokens': 222, 'heads': 7, 'dim': 77, 'depth': 1, 'mlp_dim': 67, 'dropout': 0.372376304135032, 'learning_rate': 0.05403330389748849, 'batch_size': 256, 'num_epochs': 76}. Best is trial 23 with value: 45.070899963378906.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:38,014] Trial 24 finished with value: 62.808143615722656 and parameters: {'num_tokens': 198, 'heads': 5, 'dim': 65, 'depth': 1, 'mlp_dim': 108, 'dropout': 0.2789164764508765, 'learning_rate': 0.021797072557701844, 'batch_size': 256, 'num_epochs': 76}. Best is trial 23 with value: 45.070899963378906.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:38,885] Trial 25 finished with value: 95.25434875488281 and parameters: {'num_tokens': 182, 'heads': 6, 'dim': 36, 'depth': 1, 'mlp_dim': 76, 'dropout': 0.4017636290865098, 'learning_rate': 0.05033671772700706, 'batch_size': 128, 'num_epochs': 85}. Best is trial 23 with value: 45.070899963378906.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:40,633] Trial 26 finished with value: 115.49984741210938 and parameters: {'num_tokens': 225, 'heads': 7, 'dim': 140, 'depth': 2, 'mlp_dim': 96, 'dropout': 0.49702708370182314, 'learning_rate': 0.0067276998000075, 'batch_size': 32, 'num_epochs': 75}. Best is trial 23 with value: 45.070899963378906.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:44,631] Trial 27 finished with value: 102.82662200927734 and parameters: {'num_tokens': 148, 'heads': 4, 'dim': 120, 'depth': 3, 'mlp_dim': 171, 'dropout': 0.3372438951832015, 'learning_rate': 0.05277958695915846, 'batch_size': 64, 'num_epochs': 51}. Best is trial 23 with value: 45.070899963378906.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:45,440] Trial 28 finished with value: 40.53166961669922 and parameters: {'num_tokens': 9, 'heads': 6, 'dim': 78, 'depth': 1, 'mlp_dim': 56, 'dropout': 0.006698894855880322, 'learning_rate': 0.02654032166884043, 'batch_size': 256, 'num_epochs': 63}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:46,117] Trial 29 finished with value: 69.62933349609375 and parameters: {'num_tokens': 11, 'heads': 6, 'dim': 54, 'depth': 1, 'mlp_dim': 131, 'dropout': 0.46454777342139414, 'learning_rate': 0.029049835876930243, 'batch_size': 256, 'num_epochs': 45}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:48,277] Trial 30 finished with value: 93.64071655273438 and parameters: {'num_tokens': 91, 'heads': 7, 'dim': 77, 'depth': 2, 'mlp_dim': 97, 'dropout': 0.41787394601043915, 'learning_rate': 0.001706664014297256, 'batch_size': 256, 'num_epochs': 83}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:49,212] Trial 31 finished with value: 70.33401489257812 and parameters: {'num_tokens': 211, 'heads': 5, 'dim': 85, 'depth': 1, 'mlp_dim': 50, 'dropout': 0.0039978635550760745, 'learning_rate': 0.017913636954103116, 'batch_size': 256, 'num_epochs': 66}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:50,168] Trial 32 finished with value: 60.4100456237793 and parameters: {'num_tokens': 178, 'heads': 6, 'dim': 66, 'depth': 1, 'mlp_dim': 71, 'dropout': 0.05163482360176045, 'learning_rate': 0.06263512873967625, 'batch_size': 256, 'num_epochs': 60}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:51,295] Trial 33 finished with value: 50.216739654541016 and parameters: {'num_tokens': 154, 'heads': 7, 'dim': 126, 'depth': 1, 'mlp_dim': 46, 'dropout': 0.0507545109074821, 'learning_rate': 0.009839013036368778, 'batch_size': 128, 'num_epochs': 70}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:52,505] Trial 34 finished with value: 59.80829620361328 and parameters: {'num_tokens': 93, 'heads': 7, 'dim': 154, 'depth': 1, 'mlp_dim': 51, 'dropout': 0.08009476119541925, 'learning_rate': 0.008635109264364501, 'batch_size': 64, 'num_epochs': 71}. Best is trial 28 with value: 40.53166961669922.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:55,086] Trial 35 finished with value: 91.86357116699219 and parameters: {'num_tokens': 150, 'heads': 8, 'dim': 40, 'depth': 2, 'mlp_dim': 85, 'dropout': 0.26150869360457646, 'learning_rate': 0.011553883882264335, 'batch_size': 128, 'num_epochs': 33}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:55,310] Trial 36 finished with value: 98.13340759277344 and parameters: {'num_tokens': 111, 'heads': 7, 'dim': 91, 'depth': 1, 'mlp_dim': 45, 'dropout': 0.026351174393366095, 'learning_rate': 0.02633323756343589, 'batch_size': 128, 'num_epochs': 11}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:22:58,092] Trial 37 finished with value: 104.85537719726562 and parameters: {'num_tokens': 187, 'heads': 6, 'dim': 180, 'depth': 3, 'mlp_dim': 67, 'dropout': 0.2249802942627348, 'learning_rate': 0.04785220811044027, 'batch_size': 256, 'num_epochs': 50}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:00,741] Trial 38 finished with value: 109.65335845947266 and parameters: {'num_tokens': 157, 'heads': 5, 'dim': 5, 'depth': 4, 'mlp_dim': 81, 'dropout': 0.09571582920728862, 'learning_rate': 0.00018214563650189756, 'batch_size': 128, 'num_epochs': 79}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:01,075] Trial 39 finished with value: 110.26134490966797 and parameters: {'num_tokens': 79, 'heads': 7, 'dim': 112, 'depth': 1, 'mlp_dim': 100, 'dropout': 0.17196241469808385, 'learning_rate': 0.00011621269452152714, 'batch_size': 64, 'num_epochs': 22}. Best is trial 28 with value: 40.53166961669922.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:02,294] Trial 40 finished with value: 67.2931900024414 and parameters: {'num_tokens': 14, 'heads': 8, 'dim': 24, 'depth': 2, 'mlp_dim': 130, 'dropout': 0.3553644437040431, 'learning_rate': 0.013335338204316203, 'batch_size': 32, 'num_epochs': 69}. Best is trial 28 with value: 40.53166961669922.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:03,295] Trial 41 finished with value: 118.29234313964844 and parameters: {'num_tokens': 130, 'heads': 1, 'dim': 129, 'depth': 1, 'mlp_dim': 66, 'dropout': 0.0423022939645055, 'learning_rate': 0.006168184468493465, 'batch_size': 128, 'num_epochs': 64}. Best is trial 28 with value: 40.53166961669922.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:04,263] Trial 42 finished with value: 112.32218933105469 and parameters: {'num_tokens': 160, 'heads': 4, 'dim': 132, 'depth': 1, 'mlp_dim': 43, 'dropout': 0.09694918967479013, 'learning_rate': 0.01436157460158078, 'batch_size': 128, 'num_epochs': 63}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:07,010] Trial 43 finished with value: 62.461910247802734 and parameters: {'num_tokens': 215, 'heads': 6, 'dim': 168, 'depth': 2, 'mlp_dim': 75, 'dropout': 0.07194578543229535, 'learning_rate': 0.004075503423279338, 'batch_size': 128, 'num_epochs': 71}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:08,001] Trial 44 finished with value: 67.56967163085938 and parameters: {'num_tokens': 187, 'heads': 5, 'dim': 55, 'depth': 1, 'mlp_dim': 211, 'dropout': 0.015517970804025581, 'learning_rate': 0.03426503819503903, 'batch_size': 128, 'num_epochs': 77}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:09,437] Trial 45 finished with value: 90.09149169921875 and parameters: {'num_tokens': 43, 'heads': 7, 'dim': 105, 'depth': 1, 'mlp_dim': 56, 'dropout': 0.032352275706613975, 'learning_rate': 0.07011404651647474, 'batch_size': 256, 'num_epochs': 93}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:14,902] Trial 46 finished with value: 147.7750244140625 and parameters: {'num_tokens': 234, 'heads': 3, 'dim': 246, 'depth': 3, 'mlp_dim': 87, 'dropout': 0.06122125704623153, 'learning_rate': 0.019033089212999676, 'batch_size': 256, 'num_epochs': 67}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:16,112] Trial 47 finished with value: 93.58717346191406 and parameters: {'num_tokens': 117, 'heads': 7, 'dim': 70, 'depth': 2, 'mlp_dim': 155, 'dropout': 0.4004910246773915, 'learning_rate': 0.002737511692055448, 'batch_size': 128, 'num_epochs': 58}. Best is trial 28 with value: 40.53166961669922.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:17,242] Trial 48 finished with value: 38.71345138549805 and parameters: {'num_tokens': 236, 'heads': 6, 'dim': 120, 'depth': 1, 'mlp_dim': 37, 'dropout': 0.11988702774164357, 'learning_rate': 0.009788473763374921, 'batch_size': 256, 'num_epochs': 81}. Best is trial 48 with value: 38.71345138549805.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1004955102.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:23:18,309] Trial 49 finished with value: 66.87947082519531 and parameters: {'num_tokens': 254, 'heads': 6, 'dim': 96, 'depth': 1, 'mlp_dim': 40, 'dropout': 0.12251770269598378, 'learning_rate': 0.008941916428402318, 'batch_size': 256, 'num_epochs': 84}. Best is trial 48 with value: 38.71345138549805.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_tokens: 236\n",
      "heads: 6\n",
      "dim: 120\n",
      "depth: 1\n",
      "mlp_dim: 37\n",
      "dropout: 0.11988702774164357\n",
      "learning_rate: 0.009788473763374921\n",
      "batch_size: 256\n",
      "num_epochs: 81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_tokens, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super(FTTransformer, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.input_dim = input_dim\n",
    "        self.token_embedding = nn.Embedding(num_tokens * input_dim, dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tokenize the input\n",
    "        x = (x * (self.num_tokens - 1)).long()\n",
    "        x = torch.clamp(x, 0, self.num_tokens - 1)  # Ensure indices are within range\n",
    "        x = x + torch.arange(x.shape[1], device=x.device) * self.num_tokens\n",
    "        x = self.token_embedding(x)\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        \n",
    "        # Pool and predict\n",
    "        x = x.mean(dim=1)\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    num_tokens = trial.suggest_int('num_tokens', 8, 256)\n",
    "    heads = trial.suggest_int('heads', 1, 8)\n",
    "    dim = trial.suggest_int('dim', heads, 256, step=heads)  # Ensure dim is divisible by heads\n",
    "    depth = trial.suggest_int('depth', 1, 6)\n",
    "    mlp_dim = trial.suggest_int('mlp_dim', 32, 256)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the FT Transformer model\n",
    "    model = FTTransformer(X_train.shape[1], num_tokens, dim, depth, heads, mlp_dim, dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final FT Transformer model with the best hyperparameters\n",
    "best_model = FTTransformer(X_train.shape[1], \n",
    "                           best_params['num_tokens'], \n",
    "                           best_params['dim'], \n",
    "                           best_params['depth'], \n",
    "                           best_params['heads'], \n",
    "                           best_params['mlp_dim'], \n",
    "                           best_params['dropout']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['FT-Transformer'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:26:00,400] A new study created in memory with name: no-name-3e541fa6-fe18-4b91-b923-1b72e5976d50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:00,616] Trial 0 finished with value: 87.46815490722656 and parameters: {'n_layers': 5, 'n_units_l0': 201, 'n_units_l1': 170, 'n_units_l2': 158, 'n_units_l3': 145, 'n_units_l4': 221, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu', 'activation_l3': 'sigmoid', 'activation_l4': 'sigmoid', 'learning_rate': 0.010669005980628142, 'batch_size': 64, 'num_epochs': 18}. Best is trial 0 with value: 87.46815490722656.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:00,673] Trial 1 finished with value: 109.96450805664062 and parameters: {'n_layers': 1, 'n_units_l0': 203, 'activation_l0': 'relu', 'learning_rate': 0.0012974182240801542, 'batch_size': 64, 'num_epochs': 16}. Best is trial 0 with value: 87.46815490722656.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:00,759] Trial 2 finished with value: 109.74459075927734 and parameters: {'n_layers': 2, 'n_units_l0': 144, 'n_units_l1': 234, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'learning_rate': 0.0004548285165597232, 'batch_size': 64, 'num_epochs': 17}. Best is trial 0 with value: 87.46815490722656.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:01,199] Trial 3 finished with value: 52.84694290161133 and parameters: {'n_layers': 5, 'n_units_l0': 158, 'n_units_l1': 189, 'n_units_l2': 100, 'n_units_l3': 202, 'n_units_l4': 140, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'tanh', 'activation_l4': 'sigmoid', 'learning_rate': 0.01644753108303947, 'batch_size': 256, 'num_epochs': 63}. Best is trial 3 with value: 52.84694290161133.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:01,413] Trial 4 finished with value: 99.63504028320312 and parameters: {'n_layers': 4, 'n_units_l0': 251, 'n_units_l1': 99, 'n_units_l2': 235, 'n_units_l3': 53, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'sigmoid', 'learning_rate': 0.011572755686615385, 'batch_size': 256, 'num_epochs': 32}. Best is trial 3 with value: 52.84694290161133.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:01,711] Trial 5 finished with value: 34.60691452026367 and parameters: {'n_layers': 3, 'n_units_l0': 108, 'n_units_l1': 67, 'n_units_l2': 168, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'sigmoid', 'learning_rate': 0.09041593144284844, 'batch_size': 256, 'num_epochs': 77}. Best is trial 5 with value: 34.60691452026367.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:01,778] Trial 6 finished with value: 94.89814758300781 and parameters: {'n_layers': 1, 'n_units_l0': 220, 'activation_l0': 'tanh', 'learning_rate': 0.011120942768860389, 'batch_size': 256, 'num_epochs': 26}. Best is trial 5 with value: 34.60691452026367.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:01,897] Trial 7 finished with value: 110.64028930664062 and parameters: {'n_layers': 2, 'n_units_l0': 199, 'n_units_l1': 109, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'learning_rate': 0.00030969536530404485, 'batch_size': 32, 'num_epochs': 22}. Best is trial 5 with value: 34.60691452026367.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:02,134] Trial 8 finished with value: 39.98076629638672 and parameters: {'n_layers': 3, 'n_units_l0': 161, 'n_units_l1': 235, 'n_units_l2': 85, 'activation_l0': 'relu', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'learning_rate': 0.01749417901610995, 'batch_size': 128, 'num_epochs': 52}. Best is trial 5 with value: 34.60691452026367.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:02,234] Trial 9 finished with value: 94.40697479248047 and parameters: {'n_layers': 1, 'n_units_l0': 256, 'activation_l0': 'sigmoid', 'learning_rate': 0.00918801377143919, 'batch_size': 256, 'num_epochs': 42}. Best is trial 5 with value: 34.60691452026367.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:02,589] Trial 10 finished with value: 24.64682960510254 and parameters: {'n_layers': 3, 'n_units_l0': 62, 'n_units_l1': 35, 'n_units_l2': 190, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.09909455693038682, 'batch_size': 128, 'num_epochs': 93}. Best is trial 10 with value: 24.64682960510254.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:02,928] Trial 11 finished with value: 21.76458168029785 and parameters: {'n_layers': 3, 'n_units_l0': 54, 'n_units_l1': 32, 'n_units_l2': 188, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.06601579753914256, 'batch_size': 128, 'num_epochs': 92}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:03,450] Trial 12 finished with value: 104.99867248535156 and parameters: {'n_layers': 4, 'n_units_l0': 56, 'n_units_l1': 33, 'n_units_l2': 224, 'n_units_l3': 249, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.09620997196591721, 'batch_size': 128, 'num_epochs': 98}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:03,889] Trial 13 finished with value: 58.037376403808594 and parameters: {'n_layers': 4, 'n_units_l0': 36, 'n_units_l1': 36, 'n_units_l2': 190, 'n_units_l3': 32, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'tanh', 'learning_rate': 0.04558387996038504, 'batch_size': 128, 'num_epochs': 99}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:04,167] Trial 14 finished with value: 92.24061584472656 and parameters: {'n_layers': 2, 'n_units_l0': 84, 'n_units_l1': 84, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'learning_rate': 0.002470463640692624, 'batch_size': 128, 'num_epochs': 81}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:04,555] Trial 15 finished with value: 22.105567932128906 and parameters: {'n_layers': 3, 'n_units_l0': 84, 'n_units_l1': 134, 'n_units_l2': 126, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.04456706213943256, 'batch_size': 128, 'num_epochs': 84}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:05,002] Trial 16 finished with value: 25.130756378173828 and parameters: {'n_layers': 4, 'n_units_l0': 106, 'n_units_l1': 134, 'n_units_l2': 43, 'n_units_l3': 113, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.03454980416091822, 'batch_size': 128, 'num_epochs': 80}. Best is trial 11 with value: 21.76458168029785.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:05,255] Trial 17 finished with value: 16.7769832611084 and parameters: {'n_layers': 2, 'n_units_l0': 32, 'n_units_l1': 144, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'learning_rate': 0.03686117536092164, 'batch_size': 32, 'num_epochs': 67}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:05,513] Trial 18 finished with value: 110.13581085205078 and parameters: {'n_layers': 2, 'n_units_l0': 39, 'n_units_l1': 193, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'learning_rate': 0.00012125990765836626, 'batch_size': 32, 'num_epochs': 67}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:05,744] Trial 19 finished with value: 74.9688720703125 and parameters: {'n_layers': 2, 'n_units_l0': 116, 'n_units_l1': 166, 'activation_l0': 'relu', 'activation_l1': 'tanh', 'learning_rate': 0.0038698073476361133, 'batch_size': 32, 'num_epochs': 57}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:06,084] Trial 20 finished with value: 23.186908721923828 and parameters: {'n_layers': 3, 'n_units_l0': 71, 'n_units_l1': 67, 'n_units_l2': 253, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'learning_rate': 0.0053619690852924635, 'batch_size': 32, 'num_epochs': 71}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:06,523] Trial 21 finished with value: 20.456815719604492 and parameters: {'n_layers': 3, 'n_units_l0': 86, 'n_units_l1': 131, 'n_units_l2': 126, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.03212008674005777, 'batch_size': 128, 'num_epochs': 88}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:07,032] Trial 22 finished with value: 20.97723388671875 and parameters: {'n_layers': 3, 'n_units_l0': 34, 'n_units_l1': 118, 'n_units_l2': 130, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.028478171405086402, 'batch_size': 32, 'num_epochs': 89}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:07,388] Trial 23 finished with value: 18.69568634033203 and parameters: {'n_layers': 2, 'n_units_l0': 39, 'n_units_l1': 117, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'learning_rate': 0.03339559076285785, 'batch_size': 32, 'num_epochs': 90}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:07,668] Trial 24 finished with value: 22.444786071777344 and parameters: {'n_layers': 2, 'n_units_l0': 83, 'n_units_l1': 152, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'learning_rate': 0.02441412043292776, 'batch_size': 32, 'num_epochs': 73}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:07,814] Trial 25 finished with value: 102.77359771728516 and parameters: {'n_layers': 1, 'n_units_l0': 124, 'activation_l0': 'sigmoid', 'learning_rate': 0.006476961708972829, 'batch_size': 32, 'num_epochs': 50}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:08,102] Trial 26 finished with value: 92.2077865600586 and parameters: {'n_layers': 2, 'n_units_l0': 49, 'n_units_l1': 123, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'learning_rate': 0.0020258187632881636, 'batch_size': 32, 'num_epochs': 85}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:08,350] Trial 27 finished with value: 28.805683135986328 and parameters: {'n_layers': 2, 'n_units_l0': 73, 'n_units_l1': 151, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'learning_rate': 0.05225485832879208, 'batch_size': 32, 'num_epochs': 62}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:08,474] Trial 28 finished with value: 24.381467819213867 and parameters: {'n_layers': 1, 'n_units_l0': 98, 'activation_l0': 'relu', 'learning_rate': 0.028300772609869428, 'batch_size': 32, 'num_epochs': 44}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:08,876] Trial 29 finished with value: 78.5219497680664 and parameters: {'n_layers': 5, 'n_units_l0': 51, 'n_units_l1': 88, 'n_units_l2': 43, 'n_units_l3': 124, 'n_units_l4': 34, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'activation_l3': 'sigmoid', 'activation_l4': 'tanh', 'learning_rate': 0.014098819618634368, 'batch_size': 32, 'num_epochs': 75}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:09,208] Trial 30 finished with value: 24.92996597290039 and parameters: {'n_layers': 2, 'n_units_l0': 32, 'n_units_l1': 200, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'learning_rate': 0.020470319279903178, 'batch_size': 64, 'num_epochs': 88}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:09,627] Trial 31 finished with value: 22.00794219970703 and parameters: {'n_layers': 3, 'n_units_l0': 43, 'n_units_l1': 115, 'n_units_l2': 123, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.030540093626766813, 'batch_size': 32, 'num_epochs': 91}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:10,078] Trial 32 finished with value: 35.99161148071289 and parameters: {'n_layers': 3, 'n_units_l0': 73, 'n_units_l1': 130, 'n_units_l2': 87, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.06474574191760156, 'batch_size': 32, 'num_epochs': 96}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:10,551] Trial 33 finished with value: 16.957788467407227 and parameters: {'n_layers': 4, 'n_units_l0': 33, 'n_units_l1': 104, 'n_units_l2': 133, 'n_units_l3': 254, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.007173063036913294, 'batch_size': 64, 'num_epochs': 86}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:10,995] Trial 34 finished with value: 42.212440490722656 and parameters: {'n_layers': 4, 'n_units_l0': 133, 'n_units_l1': 160, 'n_units_l2': 103, 'n_units_l3': 251, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.0014914153395536478, 'batch_size': 64, 'num_epochs': 68}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:11,650] Trial 35 finished with value: 27.883848190307617 and parameters: {'n_layers': 5, 'n_units_l0': 64, 'n_units_l1': 98, 'n_units_l2': 70, 'n_units_l3': 190, 'n_units_l4': 254, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'activation_l2': 'sigmoid', 'activation_l3': 'relu', 'activation_l4': 'relu', 'learning_rate': 0.007945725326909578, 'batch_size': 64, 'num_epochs': 80}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:12,208] Trial 36 finished with value: 50.72114944458008 and parameters: {'n_layers': 4, 'n_units_l0': 94, 'n_units_l1': 180, 'n_units_l2': 149, 'n_units_l3': 196, 'activation_l0': 'relu', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.0007028274522775789, 'batch_size': 64, 'num_epochs': 85}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:12,414] Trial 37 finished with value: 102.8267822265625 and parameters: {'n_layers': 1, 'n_units_l0': 48, 'activation_l0': 'sigmoid', 'learning_rate': 0.013647332259182935, 'batch_size': 64, 'num_epochs': 61}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:12,970] Trial 38 finished with value: 67.32148742675781 and parameters: {'n_layers': 5, 'n_units_l0': 162, 'n_units_l1': 212, 'n_units_l2': 64, 'n_units_l3': 80, 'n_units_l4': 35, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'activation_l3': 'tanh', 'activation_l4': 'tanh', 'learning_rate': 0.01898499488212817, 'batch_size': 64, 'num_epochs': 100}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:13,532] Trial 39 finished with value: 21.09266471862793 and parameters: {'n_layers': 4, 'n_units_l0': 182, 'n_units_l1': 143, 'n_units_l2': 175, 'n_units_l3': 171, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.03901893027071895, 'batch_size': 256, 'num_epochs': 77}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:13,798] Trial 40 finished with value: 26.170211791992188 and parameters: {'n_layers': 2, 'n_units_l0': 60, 'n_units_l1': 70, 'activation_l0': 'relu', 'activation_l1': 'relu', 'learning_rate': 0.0042797519369353765, 'batch_size': 64, 'num_epochs': 68}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:14,169] Trial 41 finished with value: 20.214399337768555 and parameters: {'n_layers': 3, 'n_units_l0': 32, 'n_units_l1': 115, 'n_units_l2': 129, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.06579016067241164, 'batch_size': 32, 'num_epochs': 89}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:14,569] Trial 42 finished with value: 20.183801651000977 and parameters: {'n_layers': 3, 'n_units_l0': 32, 'n_units_l1': 103, 'n_units_l2': 141, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.0657292587894902, 'batch_size': 32, 'num_epochs': 94}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:14,687] Trial 43 finished with value: 102.0157699584961 and parameters: {'n_layers': 3, 'n_units_l0': 45, 'n_units_l1': 105, 'n_units_l2': 143, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.06875212086994645, 'batch_size': 32, 'num_epochs': 13}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:15,155] Trial 44 finished with value: 24.723785400390625 and parameters: {'n_layers': 3, 'n_units_l0': 32, 'n_units_l1': 90, 'n_units_l2': 109, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'learning_rate': 0.06663481253611554, 'batch_size': 32, 'num_epochs': 95}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:15,575] Trial 45 finished with value: 35.59913635253906 and parameters: {'n_layers': 2, 'n_units_l0': 238, 'n_units_l1': 78, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'learning_rate': 0.05290036715345759, 'batch_size': 32, 'num_epochs': 93}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:16,081] Trial 46 finished with value: 51.5911750793457 and parameters: {'n_layers': 4, 'n_units_l0': 62, 'n_units_l1': 103, 'n_units_l2': 210, 'n_units_l3': 224, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'tanh', 'activation_l3': 'relu', 'learning_rate': 0.07482123517944597, 'batch_size': 32, 'num_epochs': 86}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:17,346] Trial 47 finished with value: 105.0368881225586 and parameters: {'n_layers': 4, 'n_units_l0': 42, 'n_units_l1': 119, 'n_units_l2': 144, 'n_units_l3': 162, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'tanh', 'learning_rate': 0.09875803638962365, 'batch_size': 256, 'num_epochs': 83}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:17,613] Trial 48 finished with value: 99.73649597167969 and parameters: {'n_layers': 1, 'n_units_l0': 54, 'activation_l0': 'sigmoid', 'learning_rate': 0.0092534758003007, 'batch_size': 32, 'num_epochs': 79}. Best is trial 17 with value: 16.7769832611084.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1217925743.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:26:18,251] Trial 49 finished with value: 15.070592880249023 and parameters: {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1': 253, 'n_units_l2': 160, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'learning_rate': 0.022532081021161984, 'batch_size': 32, 'num_epochs': 95}. Best is trial 49 with value: 15.070592880249023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "NAS                 16.521265  0.952756       0.41489            0.0   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "NAS                      18.283068   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "NAS                {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1'...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_layers: 3\n",
      "n_units_l0: 70\n",
      "n_units_l1: 253\n",
      "n_units_l2: 160\n",
      "activation_l0: sigmoid\n",
      "activation_l1: tanh\n",
      "activation_l2: relu\n",
      "learning_rate: 0.022532081021161984\n",
      "batch_size: 32\n",
      "num_epochs: 95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, input_dim, layer_sizes, activations):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for size, activation in zip(layer_sizes, activations):\n",
    "            layers.append(nn.Linear(prev_dim, size))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            prev_dim = size\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "    layer_sizes = [trial.suggest_int(f'n_units_l{i}', 32, 256) for i in range(n_layers)]\n",
    "    activations = [trial.suggest_categorical(f'activation_l{i}', ['relu', 'tanh', 'sigmoid']) for i in range(n_layers)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    model = DynamicNet(X_train.shape[1], layer_sizes, activations).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "layer_sizes = [best_params[f'n_units_l{i}'] for i in range(best_params['n_layers'])]\n",
    "activations = [best_params[f'activation_l{i}'] for i in range(best_params['n_layers'])]\n",
    "best_model = DynamicNet(X_train.shape[1], layer_sizes, activations).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['NAS'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:47:05,834] A new study created in memory with name: no-name-357476c1-4ae8-493f-94fd-34b393b5fb8b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:06,483] Trial 0 finished with value: 110.83451080322266 and parameters: {'hidden_dim': 96, 'num_layers': 3, 'activation': 'sigmoid', 'learning_rate': 0.00019913092117633768, 'batch_size': 32, 'num_epochs': 25}. Best is trial 0 with value: 110.83451080322266.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:06,730] Trial 1 finished with value: 110.6949462890625 and parameters: {'hidden_dim': 123, 'num_layers': 3, 'activation': 'sigmoid', 'learning_rate': 0.0003890037162662886, 'batch_size': 64, 'num_epochs': 15}. Best is trial 1 with value: 110.6949462890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:06,916] Trial 2 finished with value: 104.4683837890625 and parameters: {'hidden_dim': 97, 'num_layers': 3, 'activation': 'sigmoid', 'learning_rate': 0.008446207350313182, 'batch_size': 32, 'num_epochs': 13}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:07,190] Trial 3 finished with value: 109.86067199707031 and parameters: {'hidden_dim': 111, 'num_layers': 3, 'activation': 'tanh', 'learning_rate': 0.000100538811885062, 'batch_size': 64, 'num_epochs': 12}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:07,507] Trial 4 finished with value: 110.67897033691406 and parameters: {'hidden_dim': 62, 'num_layers': 3, 'activation': 'relu', 'learning_rate': 0.0011450826106012035, 'batch_size': 128, 'num_epochs': 23}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:07,706] Trial 5 finished with value: 110.51385498046875 and parameters: {'hidden_dim': 98, 'num_layers': 3, 'activation': 'tanh', 'learning_rate': 0.002100464897489481, 'batch_size': 64, 'num_epochs': 15}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:07,948] Trial 6 finished with value: 107.8154067993164 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'activation': 'tanh', 'learning_rate': 0.0012781205466298308, 'batch_size': 64, 'num_epochs': 18}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:08,212] Trial 7 finished with value: 110.85643005371094 and parameters: {'hidden_dim': 86, 'num_layers': 2, 'activation': 'tanh', 'learning_rate': 0.00034433986536009807, 'batch_size': 32, 'num_epochs': 24}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:08,687] Trial 8 finished with value: 110.76026916503906 and parameters: {'hidden_dim': 120, 'num_layers': 3, 'activation': 'tanh', 'learning_rate': 0.00012388325954917448, 'batch_size': 32, 'num_epochs': 29}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,009] Trial 9 finished with value: 107.79993438720703 and parameters: {'hidden_dim': 95, 'num_layers': 2, 'activation': 'tanh', 'learning_rate': 0.0012026493015147108, 'batch_size': 64, 'num_epochs': 21}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,140] Trial 10 finished with value: 109.27223205566406 and parameters: {'hidden_dim': 35, 'num_layers': 1, 'activation': 'sigmoid', 'learning_rate': 0.008875824629254004, 'batch_size': 128, 'num_epochs': 10}. Best is trial 2 with value: 104.4683837890625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,299] Trial 11 finished with value: 98.43612670898438 and parameters: {'hidden_dim': 67, 'num_layers': 1, 'activation': 'relu', 'learning_rate': 0.009524360582350297, 'batch_size': 32, 'num_epochs': 20}. Best is trial 11 with value: 98.43612670898438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,438] Trial 12 finished with value: 117.56951141357422 and parameters: {'hidden_dim': 60, 'num_layers': 1, 'activation': 'relu', 'learning_rate': 0.009676661894322277, 'batch_size': 32, 'num_epochs': 17}. Best is trial 11 with value: 98.43612670898438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,640] Trial 13 finished with value: 107.6944808959961 and parameters: {'hidden_dim': 70, 'num_layers': 1, 'activation': 'relu', 'learning_rate': 0.003809506679358594, 'batch_size': 32, 'num_epochs': 29}. Best is trial 11 with value: 98.43612670898438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:09,747] Trial 14 finished with value: 110.66460418701172 and parameters: {'hidden_dim': 43, 'num_layers': 1, 'activation': 'sigmoid', 'learning_rate': 0.00394103025474687, 'batch_size': 32, 'num_epochs': 13}. Best is trial 11 with value: 98.43612670898438.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:10,008] Trial 15 finished with value: 74.80815124511719 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.005380146842541144, 'batch_size': 32, 'num_epochs': 20}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:10,318] Trial 16 finished with value: 94.08850860595703 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.004326302653753349, 'batch_size': 32, 'num_epochs': 21}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:10,676] Trial 17 finished with value: 136.51754760742188 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.003973337639998804, 'batch_size': 128, 'num_epochs': 26}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:10,965] Trial 18 finished with value: 110.71766662597656 and parameters: {'hidden_dim': 46, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.0020785104575343186, 'batch_size': 32, 'num_epochs': 22}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:11,138] Trial 19 finished with value: 100.50456237792969 and parameters: {'hidden_dim': 53, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.0053701315958620596, 'batch_size': 32, 'num_epochs': 18}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:11,530] Trial 20 finished with value: 100.99463653564453 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.0022013409822349127, 'batch_size': 128, 'num_epochs': 27}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:11,730] Trial 21 finished with value: 100.46502685546875 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'activation': 'relu', 'learning_rate': 0.005876850815733934, 'batch_size': 32, 'num_epochs': 20}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:12,028] Trial 22 finished with value: 111.70437622070312 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.005480607638578013, 'batch_size': 32, 'num_epochs': 20}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:12,228] Trial 23 finished with value: 109.53388214111328 and parameters: {'hidden_dim': 60, 'num_layers': 1, 'activation': 'relu', 'learning_rate': 0.002866455121036074, 'batch_size': 32, 'num_epochs': 19}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:12,525] Trial 24 finished with value: 84.58797454833984 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.006131525181412138, 'batch_size': 32, 'num_epochs': 22}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:12,859] Trial 25 finished with value: 91.68391418457031 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.006146540848187167, 'batch_size': 32, 'num_epochs': 22}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:13,251] Trial 26 finished with value: 110.49720764160156 and parameters: {'hidden_dim': 86, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.000517081685830879, 'batch_size': 32, 'num_epochs': 23}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:13,665] Trial 27 finished with value: 512.5071411132812 and parameters: {'hidden_dim': 107, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.006848334086968281, 'batch_size': 32, 'num_epochs': 27}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:13,989] Trial 28 finished with value: 109.93569946289062 and parameters: {'hidden_dim': 86, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.0006453551585940064, 'batch_size': 32, 'num_epochs': 16}. Best is trial 15 with value: 74.80815124511719.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\4231878458.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-26 08:47:14,341] Trial 29 finished with value: 84.09479522705078 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'activation': 'relu', 'learning_rate': 0.0028673624615394022, 'batch_size': 128, 'num_epochs': 25}. Best is trial 15 with value: 74.80815124511719.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "NAS                 16.521265  0.952756       0.41489            0.0   \n",
      "KAN                  2.629192  0.998803      0.262269       0.000997   \n",
      "VIME                43.168236  0.677454      0.275263       0.000997   \n",
      "TabNet              20.873161  0.924588      1.741344       0.016957   \n",
      "NODE                46.850529   0.62008      0.244346       0.000996   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "NAS                      18.283068   \n",
      "KAN                      17.044498   \n",
      "VIME                     20.055836   \n",
      "TabNet                  125.814531   \n",
      "NODE                      8.772888   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "NAS                {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1'...  \n",
      "KAN                {'inner_dim': 60, 'outer_dim': 46, 'learning_r...  \n",
      "VIME               {'hidden_dim': 228, 'learning_rate': 0.0124134...  \n",
      "TabNet             {'n_d': 34, 'n_a': 56, 'n_steps': 5, 'gamma': ...  \n",
      "NODE               {'hidden_dim': 75, 'num_layers': 2, 'activatio...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim: 75\n",
      "num_layers: 2\n",
      "activation: relu\n",
      "learning_rate: 0.005380146842541144\n",
      "batch_size: 32\n",
      "num_epochs: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, activation):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, activation):\n",
    "        super(NODE, self).__init__()\n",
    "        self.odefunc = ODEFunc(input_dim, hidden_dim, num_layers, activation)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.tensor([0, 1], dtype=torch.float32).to(x.device)\n",
    "        out = odeint(self.odefunc, x, t, method='rk4')[-1]\n",
    "        return self.fc(out)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 32, 128)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "\n",
    "    # Create the NODE model\n",
    "    model = NODE(X_train.shape[1], hidden_dim, num_layers, activation).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Reduced number of trials for faster tuning\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final NODE model with the best hyperparameters\n",
    "best_model = NODE(X_train.shape[1], \n",
    "                  best_params['hidden_dim'], \n",
    "                  best_params['num_layers'], \n",
    "                  best_params['activation']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['NODE'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:39:52,078] A new study created in memory with name: no-name-c7a724b3-ec54-4626-8671-f94974c00f33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8256.16309| val_0_mse: 23110.13626|  0:00:00s\n",
      "epoch 1  | loss: 8055.23438| val_0_mse: 20637.32674|  0:00:00s\n",
      "epoch 2  | loss: 7819.08838| val_0_mse: 17622.58602|  0:00:00s\n",
      "epoch 3  | loss: 7459.15283| val_0_mse: 14274.93994|  0:00:00s\n",
      "epoch 4  | loss: 7402.29443| val_0_mse: 13705.05766|  0:00:00s\n",
      "epoch 5  | loss: 7144.45703| val_0_mse: 13284.58937|  0:00:00s\n",
      "epoch 6  | loss: 6884.3125| val_0_mse: 12814.47813|  0:00:00s\n",
      "epoch 7  | loss: 6592.36719| val_0_mse: 12400.12692|  0:00:01s\n",
      "epoch 8  | loss: 6331.91016| val_0_mse: 12183.93229|  0:00:01s\n",
      "epoch 9  | loss: 6030.80127| val_0_mse: 11871.92538|  0:00:01s\n",
      "epoch 10 | loss: 5773.92188| val_0_mse: 11227.09959|  0:00:01s\n",
      "epoch 11 | loss: 5508.97852| val_0_mse: 11209.23243|  0:00:01s\n",
      "epoch 12 | loss: 5292.21338| val_0_mse: 11170.52595|  0:00:01s\n",
      "epoch 13 | loss: 5037.39404| val_0_mse: 10963.63377|  0:00:01s\n",
      "epoch 14 | loss: 4804.24707| val_0_mse: 10890.63483|  0:00:01s\n",
      "epoch 15 | loss: 4662.29053| val_0_mse: 10832.59968|  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:39:54,465] Trial 0 finished with value: 103.30708498687908 and parameters: {'n_d': 63, 'n_a': 47, 'n_steps': 6, 'gamma': 1.3616047743555921, 'lambda_sparse': 0.0002613536321293988, 'learning_rate': 0.0029466101498928417, 'batch_size': 256, 'num_epochs': 17}. Best is trial 0 with value: 103.30708498687908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 4477.21826| val_0_mse: 10672.35381|  0:00:02s\n",
      "Stop training because you reached max_epochs = 17 with best_epoch = 16 and best_val_0_mse = 10672.35381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8447.09277| val_0_mse: 12040.16196|  0:00:00s\n",
      "epoch 1  | loss: 8486.91113| val_0_mse: 12033.37028|  0:00:00s\n",
      "epoch 2  | loss: 8473.91699| val_0_mse: 12009.84318|  0:00:00s\n",
      "epoch 3  | loss: 8443.22461| val_0_mse: 11987.46981|  0:00:00s\n",
      "epoch 4  | loss: 8411.21875| val_0_mse: 11970.59511|  0:00:00s\n",
      "epoch 5  | loss: 8369.91016| val_0_mse: 11939.05906|  0:00:00s\n",
      "epoch 6  | loss: 8327.69238| val_0_mse: 11914.73228|  0:00:00s\n",
      "epoch 7  | loss: 8321.55762| val_0_mse: 11892.34383|  0:00:00s\n",
      "epoch 8  | loss: 8257.35938| val_0_mse: 11886.39601|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:39:55,403] Trial 1 finished with value: 108.99574901420779 and parameters: {'n_d': 17, 'n_a': 8, 'n_steps': 5, 'gamma': 1.6306151831579347, 'lambda_sparse': 0.0005263353861934946, 'learning_rate': 0.001930107001570886, 'batch_size': 128, 'num_epochs': 10}. Best is trial 0 with value: 103.30708498687908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 8255.83301| val_0_mse: 11880.0733|  0:00:00s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_mse = 11880.0733\n",
      "epoch 0  | loss: 8721.83594| val_0_mse: 12236.80553|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 8718.60547| val_0_mse: 12141.47293|  0:00:00s\n",
      "epoch 2  | loss: 8691.29492| val_0_mse: 12580.76302|  0:00:00s\n",
      "epoch 3  | loss: 8656.21484| val_0_mse: 12692.31055|  0:00:00s\n",
      "epoch 4  | loss: 8622.55566| val_0_mse: 12508.62363|  0:00:00s\n",
      "epoch 5  | loss: 8621.91309| val_0_mse: 12490.57482|  0:00:00s\n",
      "epoch 6  | loss: 8511.34766| val_0_mse: 12422.76036|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 12141.47293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:39:56,189] Trial 2 finished with value: 110.18835204111991 and parameters: {'n_d': 56, 'n_a': 9, 'n_steps': 5, 'gamma': 1.0751051664224747, 'lambda_sparse': 0.0008356128793584141, 'learning_rate': 0.0011677641538001243, 'batch_size': 32, 'num_epochs': 12}. Best is trial 0 with value: 103.30708498687908.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8762.39941| val_0_mse: 14167.0732|  0:00:00s\n",
      "epoch 1  | loss: 8494.30957| val_0_mse: 13616.13619|  0:00:00s\n",
      "epoch 2  | loss: 8389.37207| val_0_mse: 13074.23848|  0:00:00s\n",
      "epoch 3  | loss: 8329.07324| val_0_mse: 12572.67852|  0:00:00s\n",
      "epoch 4  | loss: 8244.71582| val_0_mse: 11785.38729|  0:00:00s\n",
      "epoch 5  | loss: 8085.28418| val_0_mse: 11689.03825|  0:00:01s\n",
      "epoch 6  | loss: 7941.13867| val_0_mse: 11619.87173|  0:00:01s\n",
      "epoch 7  | loss: 7886.42676| val_0_mse: 11226.8241|  0:00:01s\n",
      "epoch 8  | loss: 7795.61426| val_0_mse: 11482.0034|  0:00:01s\n",
      "epoch 9  | loss: 7510.39746| val_0_mse: 11344.4644|  0:00:01s\n",
      "epoch 10 | loss: 7806.18066| val_0_mse: 11356.17586|  0:00:01s\n",
      "epoch 11 | loss: 7375.67285| val_0_mse: 11151.73966|  0:00:02s\n",
      "epoch 12 | loss: 7237.68018| val_0_mse: 10922.44476|  0:00:02s\n",
      "epoch 13 | loss: 7340.42432| val_0_mse: 10700.24652|  0:00:02s\n",
      "epoch 14 | loss: 7070.95312| val_0_mse: 10546.18326|  0:00:02s\n",
      "epoch 15 | loss: 7164.77588| val_0_mse: 10297.10055|  0:00:02s\n",
      "epoch 16 | loss: 6861.19922| val_0_mse: 10178.64976|  0:00:03s\n",
      "epoch 17 | loss: 7080.06445| val_0_mse: 10087.32117|  0:00:03s\n",
      "epoch 18 | loss: 7131.5249| val_0_mse: 9973.231|  0:00:03s\n",
      "epoch 19 | loss: 6745.83789| val_0_mse: 9766.26256|  0:00:03s\n",
      "epoch 20 | loss: 6230.56934| val_0_mse: 10165.94408|  0:00:03s\n",
      "epoch 21 | loss: 6455.68555| val_0_mse: 10390.305|  0:00:03s\n",
      "epoch 22 | loss: 6295.51367| val_0_mse: 10483.7732|  0:00:04s\n",
      "epoch 23 | loss: 6531.03711| val_0_mse: 10516.76638|  0:00:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:00,861] Trial 3 finished with value: 98.82440267473338 and parameters: {'n_d': 57, 'n_a': 49, 'n_steps': 9, 'gamma': 1.568857219419653, 'lambda_sparse': 0.0007881207158268259, 'learning_rate': 0.00514873933645679, 'batch_size': 32, 'num_epochs': 66}. Best is trial 3 with value: 98.82440267473338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | loss: 6356.10547| val_0_mse: 10379.56495|  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_mse = 9766.26256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8512.85547| val_0_mse: 11306.63029|  0:00:00s\n",
      "epoch 1  | loss: 8344.90137| val_0_mse: 11035.73472|  0:00:00s\n",
      "epoch 2  | loss: 8215.30176| val_0_mse: 10874.33832|  0:00:00s\n",
      "epoch 3  | loss: 8060.75391| val_0_mse: 10778.93539|  0:00:00s\n",
      "epoch 4  | loss: 7915.10352| val_0_mse: 10620.3018|  0:00:00s\n",
      "epoch 5  | loss: 7785.61963| val_0_mse: 10244.85773|  0:00:00s\n",
      "epoch 6  | loss: 7644.0708| val_0_mse: 10089.66333|  0:00:00s\n",
      "epoch 7  | loss: 7529.88135| val_0_mse: 10000.2191|  0:00:00s\n",
      "epoch 8  | loss: 7379.44092| val_0_mse: 9913.56042|  0:00:00s\n",
      "epoch 9  | loss: 7278.05078| val_0_mse: 9698.06843|  0:00:01s\n",
      "epoch 10 | loss: 7139.39404| val_0_mse: 9409.78731|  0:00:01s\n",
      "epoch 11 | loss: 7007.36084| val_0_mse: 9077.4135|  0:00:01s\n",
      "epoch 12 | loss: 6904.41357| val_0_mse: 8850.79072|  0:00:01s\n",
      "epoch 13 | loss: 6778.73193| val_0_mse: 8613.46412|  0:00:01s\n",
      "epoch 14 | loss: 6646.71729| val_0_mse: 8456.08498|  0:00:01s\n",
      "epoch 15 | loss: 6506.62305| val_0_mse: 8310.29217|  0:00:02s\n",
      "epoch 16 | loss: 6376.4585| val_0_mse: 8094.296|  0:00:02s\n",
      "epoch 17 | loss: 6226.20508| val_0_mse: 7522.56197|  0:00:02s\n",
      "epoch 18 | loss: 6090.5332| val_0_mse: 7448.03336|  0:00:02s\n",
      "epoch 19 | loss: 5937.04443| val_0_mse: 7322.84688|  0:00:02s\n",
      "epoch 20 | loss: 5775.00537| val_0_mse: 7167.90851|  0:00:02s\n",
      "epoch 21 | loss: 5619.69775| val_0_mse: 7023.38108|  0:00:02s\n",
      "epoch 22 | loss: 5475.35303| val_0_mse: 6920.07772|  0:00:03s\n",
      "epoch 23 | loss: 5310.7832| val_0_mse: 6810.75214|  0:00:03s\n",
      "epoch 24 | loss: 5154.48096| val_0_mse: 6661.74902|  0:00:03s\n",
      "epoch 25 | loss: 4995.80957| val_0_mse: 6424.80822|  0:00:03s\n",
      "epoch 26 | loss: 4869.93506| val_0_mse: 6243.83933|  0:00:03s\n",
      "epoch 27 | loss: 4710.9624| val_0_mse: 6023.19722|  0:00:03s\n",
      "epoch 28 | loss: 4554.30518| val_0_mse: 5768.68221|  0:00:03s\n",
      "epoch 29 | loss: 4403.19092| val_0_mse: 5581.76647|  0:00:03s\n",
      "epoch 30 | loss: 4245.59521| val_0_mse: 5381.94819|  0:00:03s\n",
      "epoch 31 | loss: 4088.79712| val_0_mse: 5167.50175|  0:00:03s\n",
      "epoch 32 | loss: 3929.03418| val_0_mse: 4970.77767|  0:00:03s\n",
      "epoch 33 | loss: 3774.77344| val_0_mse: 4808.12755|  0:00:04s\n",
      "epoch 34 | loss: 3612.55005| val_0_mse: 4621.5737|  0:00:04s\n",
      "epoch 35 | loss: 3461.57544| val_0_mse: 4435.55707|  0:00:04s\n",
      "epoch 36 | loss: 3296.73511| val_0_mse: 4249.03766|  0:00:04s\n",
      "epoch 37 | loss: 3146.35229| val_0_mse: 4069.84876|  0:00:04s\n",
      "epoch 38 | loss: 2985.65918| val_0_mse: 3889.90478|  0:00:04s\n",
      "epoch 39 | loss: 2837.2207| val_0_mse: 3616.03624|  0:00:04s\n",
      "epoch 40 | loss: 2685.70337| val_0_mse: 3396.51313|  0:00:04s\n",
      "epoch 41 | loss: 2532.42603| val_0_mse: 3264.55418|  0:00:04s\n",
      "epoch 42 | loss: 2382.08813| val_0_mse: 3147.58904|  0:00:04s\n",
      "epoch 43 | loss: 2235.76929| val_0_mse: 3029.91139|  0:00:04s\n",
      "epoch 44 | loss: 2089.41187| val_0_mse: 2949.32201|  0:00:04s\n",
      "epoch 45 | loss: 1948.63672| val_0_mse: 2905.31577|  0:00:04s\n",
      "epoch 46 | loss: 1808.2915| val_0_mse: 2878.7972|  0:00:04s\n",
      "epoch 47 | loss: 1673.76025| val_0_mse: 2861.81111|  0:00:05s\n",
      "epoch 48 | loss: 1544.70325| val_0_mse: 2793.19997|  0:00:05s\n",
      "epoch 49 | loss: 1419.70667| val_0_mse: 2127.806|  0:00:05s\n",
      "epoch 50 | loss: 1309.5946| val_0_mse: 1779.30548|  0:00:05s\n",
      "epoch 51 | loss: 1207.29004| val_0_mse: 1607.76314|  0:00:05s\n",
      "epoch 52 | loss: 1117.70349| val_0_mse: 1532.26235|  0:00:05s\n",
      "epoch 53 | loss: 1016.94666| val_0_mse: 1440.58426|  0:00:05s\n",
      "epoch 54 | loss: 932.86292| val_0_mse: 1301.96172|  0:00:05s\n",
      "epoch 55 | loss: 846.09235| val_0_mse: 1247.4437|  0:00:05s\n",
      "epoch 56 | loss: 762.92682| val_0_mse: 1252.44966|  0:00:05s\n",
      "epoch 57 | loss: 687.2533| val_0_mse: 1194.50235|  0:00:05s\n",
      "epoch 58 | loss: 613.28186| val_0_mse: 1081.23809|  0:00:05s\n",
      "epoch 59 | loss: 547.46198| val_0_mse: 985.72477|  0:00:05s\n",
      "epoch 60 | loss: 479.73184| val_0_mse: 947.81328|  0:00:05s\n",
      "epoch 61 | loss: 416.96469| val_0_mse: 883.51972|  0:00:06s\n",
      "epoch 62 | loss: 365.33548| val_0_mse: 834.097 |  0:00:06s\n",
      "epoch 63 | loss: 311.10471| val_0_mse: 764.49241|  0:00:06s\n",
      "epoch 64 | loss: 263.01178| val_0_mse: 779.68623|  0:00:06s\n",
      "epoch 65 | loss: 217.15338| val_0_mse: 808.31202|  0:00:06s\n",
      "epoch 66 | loss: 179.09412| val_0_mse: 905.56755|  0:00:06s\n",
      "epoch 67 | loss: 153.36658| val_0_mse: 891.3439|  0:00:06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:07,504] Trial 4 finished with value: 27.649455942998415 and parameters: {'n_d': 21, 'n_a': 15, 'n_steps': 6, 'gamma': 1.1563402160779976, 'lambda_sparse': 4.92728781179181e-05, 'learning_rate': 0.00732366909994917, 'batch_size': 256, 'num_epochs': 82}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 | loss: 114.92049| val_0_mse: 809.06785|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 63 and best_val_0_mse = 764.49241\n",
      "epoch 0  | loss: 8401.36035| val_0_mse: 10956.0983|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 8424.4541| val_0_mse: 10832.04422|  0:00:00s\n",
      "epoch 2  | loss: 8296.53613| val_0_mse: 10690.29723|  0:00:00s\n",
      "epoch 3  | loss: 8346.74512| val_0_mse: 10937.42856|  0:00:00s\n",
      "epoch 4  | loss: 8300.81543| val_0_mse: 10785.23161|  0:00:00s\n",
      "epoch 5  | loss: 8221.76562| val_0_mse: 10677.48368|  0:00:00s\n",
      "epoch 6  | loss: 8112.52539| val_0_mse: 10756.70303|  0:00:00s\n",
      "epoch 7  | loss: 8155.13867| val_0_mse: 10668.20769|  0:00:00s\n",
      "epoch 8  | loss: 7962.42578| val_0_mse: 10138.99787|  0:00:00s\n",
      "epoch 9  | loss: 8010.38525| val_0_mse: 10119.21585|  0:00:01s\n",
      "epoch 10 | loss: 7801.07568| val_0_mse: 10008.58214|  0:00:01s\n",
      "epoch 11 | loss: 7845.75 | val_0_mse: 9890.54286|  0:00:01s\n",
      "epoch 12 | loss: 7731.3833| val_0_mse: 9727.28594|  0:00:01s\n",
      "epoch 13 | loss: 7767.06885| val_0_mse: 9677.57137|  0:00:01s\n",
      "epoch 14 | loss: 7721.44873| val_0_mse: 9736.5516|  0:00:01s\n",
      "epoch 15 | loss: 7865.17969| val_0_mse: 10405.46784|  0:00:01s\n",
      "epoch 16 | loss: 7452.96924| val_0_mse: 10481.80226|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:09,468] Trial 5 finished with value: 98.37464799316282 and parameters: {'n_d': 9, 'n_a': 59, 'n_steps': 6, 'gamma': 1.7745717377848533, 'lambda_sparse': 0.00015995084229871053, 'learning_rate': 0.010309222764741771, 'batch_size': 32, 'num_epochs': 83}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 7507.26221| val_0_mse: 10412.31426|  0:00:01s\n",
      "epoch 18 | loss: 7249.41846| val_0_mse: 10308.10437|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_mse = 9677.57137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8647.73828| val_0_mse: 8165.95436|  0:00:00s\n",
      "epoch 1  | loss: 8717.12988| val_0_mse: 8754.86121|  0:00:00s\n",
      "epoch 2  | loss: 8693.1084| val_0_mse: 8750.04019|  0:00:00s\n",
      "epoch 3  | loss: 8547.6748| val_0_mse: 9316.04376|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:10,491] Trial 6 finished with value: 90.36567027498232 and parameters: {'n_d': 20, 'n_a': 58, 'n_steps': 10, 'gamma': 1.4554309266617094, 'lambda_sparse': 4.549304793959321e-05, 'learning_rate': 0.0001363839100393466, 'batch_size': 32, 'num_epochs': 93}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 8565.22168| val_0_mse: 9974.06982|  0:00:00s\n",
      "epoch 5  | loss: 8527.91504| val_0_mse: 9976.38421|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 8165.95436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8417.88184| val_0_mse: 13349.37306|  0:00:00s\n",
      "epoch 1  | loss: 8106.97168| val_0_mse: 11542.97814|  0:00:00s\n",
      "epoch 2  | loss: 7832.44629| val_0_mse: 11850.71225|  0:00:00s\n",
      "epoch 3  | loss: 7611.81104| val_0_mse: 12422.78975|  0:00:00s\n",
      "epoch 4  | loss: 7193.55762| val_0_mse: 13089.49233|  0:00:00s\n",
      "epoch 5  | loss: 7097.72559| val_0_mse: 11446.08326|  0:00:00s\n",
      "epoch 6  | loss: 6644.7168| val_0_mse: 10332.67524|  0:00:00s\n",
      "epoch 7  | loss: 6097.85205| val_0_mse: 8640.22926|  0:00:00s\n",
      "epoch 8  | loss: 5728.21143| val_0_mse: 6792.77256|  0:00:00s\n",
      "epoch 9  | loss: 5556.45898| val_0_mse: 4237.99897|  0:00:00s\n",
      "epoch 10 | loss: 5001.85352| val_0_mse: 4181.14304|  0:00:00s\n",
      "epoch 11 | loss: 4870.01855| val_0_mse: 4392.91384|  0:00:00s\n",
      "epoch 12 | loss: 4173.75586| val_0_mse: 4984.08078|  0:00:00s\n",
      "epoch 13 | loss: 4129.77148| val_0_mse: 5879.81956|  0:00:00s\n",
      "epoch 14 | loss: 3037.05371| val_0_mse: 11055.47178|  0:00:01s\n",
      "epoch 15 | loss: 3297.3833| val_0_mse: 17241.28624|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_mse = 4181.14304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:11,661] Trial 7 finished with value: 64.6617587388766 and parameters: {'n_d': 27, 'n_a': 56, 'n_steps': 4, 'gamma': 1.9356130686768203, 'lambda_sparse': 0.0006750188745237965, 'learning_rate': 0.02340030399656239, 'batch_size': 64, 'num_epochs': 90}. Best is trial 4 with value: 27.649455942998415.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8459.98145| val_0_mse: 13362.60305|  0:00:00s\n",
      "epoch 1  | loss: 8406.10449| val_0_mse: 13117.14256|  0:00:00s\n",
      "epoch 2  | loss: 8319.35254| val_0_mse: 13075.89923|  0:00:00s\n",
      "epoch 3  | loss: 8230.79492| val_0_mse: 13025.22159|  0:00:00s\n",
      "epoch 4  | loss: 8164.08398| val_0_mse: 13024.63646|  0:00:00s\n",
      "epoch 5  | loss: 8112.86768| val_0_mse: 13067.19502|  0:00:00s\n",
      "epoch 6  | loss: 8054.97656| val_0_mse: 13080.03873|  0:00:00s\n",
      "epoch 7  | loss: 7993.62109| val_0_mse: 13095.32582|  0:00:00s\n",
      "epoch 8  | loss: 7947.22949| val_0_mse: 13101.84963|  0:00:00s\n",
      "epoch 9  | loss: 7884.82861| val_0_mse: 12373.37184|  0:00:00s\n",
      "epoch 10 | loss: 7825.28174| val_0_mse: 13107.80332|  0:00:00s\n",
      "epoch 11 | loss: 7762.11719| val_0_mse: 13484.4894|  0:00:00s\n",
      "epoch 12 | loss: 7754.479| val_0_mse: 12542.22396|  0:00:00s\n",
      "epoch 13 | loss: 7661.52002| val_0_mse: 12427.89825|  0:00:00s\n",
      "epoch 14 | loss: 7649.26562| val_0_mse: 12465.82038|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:12,772] Trial 8 finished with value: 111.23565902368959 and parameters: {'n_d': 9, 'n_a': 47, 'n_steps': 5, 'gamma': 1.4884660272398018, 'lambda_sparse': 0.0004609322731393331, 'learning_rate': 0.002701315731345864, 'batch_size': 128, 'num_epochs': 58}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_mse = 12373.37184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8434.80371| val_0_mse: 10547.79266|  0:00:02s\n",
      "epoch 1  | loss: 8158.34131| val_0_mse: 12745.97085|  0:00:02s\n",
      "epoch 2  | loss: 7911.22705| val_0_mse: 10976.70784|  0:00:02s\n",
      "epoch 3  | loss: 7855.74658| val_0_mse: 11533.42978|  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:16,137] Trial 9 finished with value: 102.70244721953317 and parameters: {'n_d': 28, 'n_a': 17, 'n_steps': 9, 'gamma': 1.6146435157821433, 'lambda_sparse': 0.000306150795524749, 'learning_rate': 0.016132475371065286, 'batch_size': 256, 'num_epochs': 29}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 7550.71436| val_0_mse: 11297.02554|  0:00:03s\n",
      "epoch 5  | loss: 7543.32178| val_0_mse: 11076.71253|  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 10547.79266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8478.72949| val_0_mse: 10122.652|  0:00:00s\n",
      "epoch 1  | loss: 7673.36084| val_0_mse: 16488.38675|  0:00:00s\n",
      "epoch 2  | loss: 6335.55859| val_0_mse: 33507.81995|  0:00:00s\n",
      "epoch 3  | loss: 4785.95557| val_0_mse: 28763.55291|  0:00:00s\n",
      "epoch 4  | loss: 3199.34277| val_0_mse: 53987.92402|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:16,546] Trial 10 finished with value: 100.61139098155233 and parameters: {'n_d': 42, 'n_a': 30, 'n_steps': 3, 'gamma': 1.0522916095225474, 'lambda_sparse': 1.6775133152212835e-05, 'learning_rate': 0.09044593082214665, 'batch_size': 256, 'num_epochs': 42}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 1831.31189| val_0_mse: 163710.41369|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 10122.652\n",
      "epoch 0  | loss: 8348.31152| val_0_mse: 16343.96324|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 7894.73438| val_0_mse: 12546.3436|  0:00:00s\n",
      "epoch 2  | loss: 6921.69971| val_0_mse: 9374.81437|  0:00:00s\n",
      "epoch 3  | loss: 6281.90283| val_0_mse: 5436.85223|  0:00:00s\n",
      "epoch 4  | loss: 5544.02637| val_0_mse: 6782.4344|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:17,187] Trial 11 finished with value: 73.73501362737736 and parameters: {'n_d': 35, 'n_a': 29, 'n_steps': 3, 'gamma': 1.9355043278729966, 'lambda_sparse': 0.0005840956170830762, 'learning_rate': 0.046149488002888435, 'batch_size': 64, 'num_epochs': 100}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 5324.3374| val_0_mse: 9767.09567|  0:00:00s\n",
      "epoch 6  | loss: 4530.66846| val_0_mse: 13007.69635|  0:00:00s\n",
      "epoch 7  | loss: 3656.68188| val_0_mse: 14173.7584|  0:00:00s\n",
      "epoch 8  | loss: 2670.19287| val_0_mse: 19871.60028|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_mse = 5436.85223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8517.91895| val_0_mse: 12002.75445|  0:00:00s\n",
      "epoch 1  | loss: 8182.55566| val_0_mse: 12467.76104|  0:00:00s\n",
      "epoch 2  | loss: 7917.96436| val_0_mse: 11646.99842|  0:00:00s\n",
      "epoch 3  | loss: 7738.11035| val_0_mse: 11704.67132|  0:00:00s\n",
      "epoch 4  | loss: 7386.04736| val_0_mse: 12631.79629|  0:00:00s\n",
      "epoch 5  | loss: 7166.44824| val_0_mse: 12140.38302|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:18,325] Trial 12 finished with value: 107.92126030070835 and parameters: {'n_d': 27, 'n_a': 21, 'n_steps': 7, 'gamma': 1.2594794457044132, 'lambda_sparse': 0.0009814253235156639, 'learning_rate': 0.022514823550334916, 'batch_size': 64, 'num_epochs': 77}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 6835.06934| val_0_mse: 11864.60449|  0:00:00s\n",
      "epoch 7  | loss: 6246.25049| val_0_mse: 12339.21175|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_mse = 11646.99842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8439.87793| val_0_mse: 12504.84436|  0:00:00s\n",
      "epoch 1  | loss: 8540.82324| val_0_mse: 13582.63158|  0:00:00s\n",
      "epoch 2  | loss: 8536.84863| val_0_mse: 12953.7427|  0:00:00s\n",
      "epoch 3  | loss: 8530.77246| val_0_mse: 12917.94193|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:18,850] Trial 13 finished with value: 111.82506139957556 and parameters: {'n_d': 42, 'n_a': 38, 'n_steps': 4, 'gamma': 1.9761419056839338, 'lambda_sparse': 0.0006867909143118176, 'learning_rate': 0.000577877890462599, 'batch_size': 64, 'num_epochs': 77}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 8441.04297| val_0_mse: 12985.39373|  0:00:00s\n",
      "epoch 5  | loss: 8522.66016| val_0_mse: 12721.1228|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 12504.84436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8098.49268| val_0_mse: 10442.88745|  0:00:00s\n",
      "epoch 1  | loss: 7982.875| val_0_mse: 11865.3432|  0:00:00s\n",
      "epoch 2  | loss: 7723.03809| val_0_mse: 12024.59961|  0:00:00s\n",
      "epoch 3  | loss: 7638.67773| val_0_mse: 11188.25584|  0:00:00s\n",
      "epoch 4  | loss: 7348.64307| val_0_mse: 10162.92335|  0:00:00s\n",
      "epoch 5  | loss: 7017.71729| val_0_mse: 12896.19634|  0:00:00s\n",
      "epoch 6  | loss: 7022.67236| val_0_mse: 13877.40786|  0:00:00s\n",
      "epoch 7  | loss: 6913.0791| val_0_mse: 16112.94954|  0:00:00s\n",
      "epoch 8  | loss: 6475.12744| val_0_mse: 11924.54515|  0:00:00s\n",
      "epoch 9  | loss: 6218.85547| val_0_mse: 9809.30984|  0:00:01s\n",
      "epoch 10 | loss: 6027.46729| val_0_mse: 11102.93594|  0:00:01s\n",
      "epoch 11 | loss: 5693.11523| val_0_mse: 10578.60835|  0:00:01s\n",
      "epoch 12 | loss: 5258.85205| val_0_mse: 10408.06044|  0:00:01s\n",
      "epoch 13 | loss: 5088.94922| val_0_mse: 10161.0425|  0:00:01s\n",
      "epoch 14 | loss: 4567.67383| val_0_mse: 9570.65062|  0:00:01s\n",
      "epoch 15 | loss: 5096.55957| val_0_mse: 8998.85723|  0:00:01s\n",
      "epoch 16 | loss: 4024.05493| val_0_mse: 8189.64617|  0:00:01s\n",
      "epoch 17 | loss: 4556.90137| val_0_mse: 7648.76772|  0:00:01s\n",
      "epoch 18 | loss: 3600.18359| val_0_mse: 7084.34755|  0:00:02s\n",
      "epoch 19 | loss: 3154.69263| val_0_mse: 6543.6591|  0:00:02s\n",
      "epoch 20 | loss: 4495.34375| val_0_mse: 5891.64275|  0:00:02s\n",
      "epoch 21 | loss: 2711.87939| val_0_mse: 5303.34127|  0:00:02s\n",
      "epoch 22 | loss: 3299.68359| val_0_mse: 4947.20327|  0:00:02s\n",
      "epoch 23 | loss: 2750.56738| val_0_mse: 4766.17494|  0:00:02s\n",
      "epoch 24 | loss: 2377.41528| val_0_mse: 4398.58645|  0:00:02s\n",
      "epoch 25 | loss: 2059.49585| val_0_mse: 4245.96176|  0:00:02s\n",
      "epoch 26 | loss: 2167.17358| val_0_mse: 4092.18055|  0:00:03s\n",
      "epoch 27 | loss: 1385.13953| val_0_mse: 3713.15257|  0:00:03s\n",
      "epoch 28 | loss: 2603.44556| val_0_mse: 3398.89403|  0:00:03s\n",
      "epoch 29 | loss: 1106.6781| val_0_mse: 3419.32707|  0:00:03s\n",
      "epoch 30 | loss: 1090.5022| val_0_mse: 3914.74023|  0:00:03s\n",
      "epoch 31 | loss: 1512.81201| val_0_mse: 3938.84694|  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:22,888] Trial 14 finished with value: 58.30003454807836 and parameters: {'n_d': 23, 'n_a': 64, 'n_steps': 7, 'gamma': 1.173499615297842, 'lambda_sparse': 0.00038472899568851393, 'learning_rate': 0.008206684245246607, 'batch_size': 64, 'num_epochs': 88}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 | loss: 652.87177| val_0_mse: 3910.77863|  0:00:03s\n",
      "epoch 33 | loss: 1030.03394| val_0_mse: 3946.37408|  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 28 and best_val_0_mse = 3398.89403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8475.87695| val_0_mse: 12999.25196|  0:00:00s\n",
      "epoch 1  | loss: 8337.9541| val_0_mse: 11814.80009|  0:00:00s\n",
      "epoch 2  | loss: 8134.41846| val_0_mse: 11894.96521|  0:00:00s\n",
      "epoch 3  | loss: 8002.27197| val_0_mse: 10980.08327|  0:00:00s\n",
      "epoch 4  | loss: 7807.4248| val_0_mse: 11064.37763|  0:00:00s\n",
      "epoch 5  | loss: 7660.98291| val_0_mse: 10881.39763|  0:00:00s\n",
      "epoch 6  | loss: 7514.87402| val_0_mse: 10784.56832|  0:00:00s\n",
      "epoch 7  | loss: 7345.81055| val_0_mse: 10873.50094|  0:00:00s\n",
      "epoch 8  | loss: 7169.50586| val_0_mse: 10924.5151|  0:00:00s\n",
      "epoch 9  | loss: 7005.66064| val_0_mse: 11148.84828|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:24,159] Trial 15 finished with value: 103.84877619155472 and parameters: {'n_d': 17, 'n_a': 37, 'n_steps': 7, 'gamma': 1.2129856033019195, 'lambda_sparse': 0.00036112893330574313, 'learning_rate': 0.007433476232629437, 'batch_size': 256, 'num_epochs': 68}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 6853.87939| val_0_mse: 11109.14483|  0:00:01s\n",
      "epoch 11 | loss: 6696.84766| val_0_mse: 11043.80542|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_mse = 10784.56832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8534.08887| val_0_mse: 14846.39928|  0:00:00s\n",
      "epoch 1  | loss: 8494.70996| val_0_mse: 13753.67167|  0:00:00s\n",
      "epoch 2  | loss: 8404.25781| val_0_mse: 15706.88417|  0:00:00s\n",
      "epoch 3  | loss: 8364.69727| val_0_mse: 14624.85772|  0:00:00s\n",
      "epoch 4  | loss: 8285.87598| val_0_mse: 15030.30744|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:25,152] Trial 16 finished with value: 117.27604899465608 and parameters: {'n_d': 37, 'n_a': 63, 'n_steps': 8, 'gamma': 1.1518595354921382, 'lambda_sparse': 0.00016400907510827903, 'learning_rate': 0.000628170794608913, 'batch_size': 256, 'num_epochs': 46}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 8200.27441| val_0_mse: 14522.65178|  0:00:00s\n",
      "epoch 6  | loss: 8137.50537| val_0_mse: 14078.73237|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 13753.67167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8759.14258| val_0_mse: 9408.953|  0:00:00s\n",
      "epoch 1  | loss: 8722.23145| val_0_mse: 8739.94878|  0:00:00s\n",
      "epoch 2  | loss: 8530.28125| val_0_mse: 9171.18396|  0:00:00s\n",
      "epoch 3  | loss: 8416.44141| val_0_mse: 9300.8242|  0:00:00s\n",
      "epoch 4  | loss: 8307.04688| val_0_mse: 8610.6008|  0:00:00s\n",
      "epoch 5  | loss: 8275.26074| val_0_mse: 8710.49568|  0:00:00s\n",
      "epoch 6  | loss: 8265.41504| val_0_mse: 8668.76834|  0:00:00s\n",
      "epoch 7  | loss: 8073.14453| val_0_mse: 8712.9013|  0:00:00s\n",
      "epoch 8  | loss: 8064.89453| val_0_mse: 8463.80609|  0:00:00s\n",
      "epoch 9  | loss: 7979.64697| val_0_mse: 8506.39666|  0:00:00s\n",
      "epoch 10 | loss: 7820.58398| val_0_mse: 8988.5091|  0:00:00s\n",
      "epoch 11 | loss: 7786.22363| val_0_mse: 8875.68708|  0:00:01s\n",
      "epoch 12 | loss: 7685.01074| val_0_mse: 8935.4048|  0:00:01s\n",
      "epoch 13 | loss: 7572.24463| val_0_mse: 8906.39879|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_mse = 8463.80609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:26,542] Trial 17 finished with value: 91.99894614917595 and parameters: {'n_d': 21, 'n_a': 18, 'n_steps': 8, 'gamma': 1.3227342254236643, 'lambda_sparse': 0.00042919447217360933, 'learning_rate': 0.005238643860091839, 'batch_size': 64, 'num_epochs': 88}. Best is trial 4 with value: 27.649455942998415.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8333.87402| val_0_mse: 10212.7298|  0:00:00s\n",
      "epoch 1  | loss: 7897.12891| val_0_mse: 11096.3134|  0:00:00s\n",
      "epoch 2  | loss: 7553.37354| val_0_mse: 10529.96485|  0:00:00s\n",
      "epoch 3  | loss: 7039.26318| val_0_mse: 10054.43549|  0:00:00s\n",
      "epoch 4  | loss: 6655.29297| val_0_mse: 10193.36486|  0:00:00s\n",
      "epoch 5  | loss: 6438.22217| val_0_mse: 10919.96788|  0:00:00s\n",
      "epoch 6  | loss: 6081.28369| val_0_mse: 11376.89273|  0:00:00s\n",
      "epoch 7  | loss: 5729.15381| val_0_mse: 12208.12455|  0:00:00s\n",
      "epoch 8  | loss: 5532.33643| val_0_mse: 11842.12394|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:27,590] Trial 18 finished with value: 100.27180807221315 and parameters: {'n_d': 31, 'n_a': 27, 'n_steps': 8, 'gamma': 1.1352576120017848, 'lambda_sparse': 0.00017286614235118532, 'learning_rate': 0.011387622196006392, 'batch_size': 128, 'num_epochs': 69}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_mse = 10054.43549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8700.98438| val_0_mse: 13489.32815|  0:00:00s\n",
      "epoch 1  | loss: 8456.41309| val_0_mse: 13215.3966|  0:00:00s\n",
      "epoch 2  | loss: 8301.95996| val_0_mse: 11168.41757|  0:00:00s\n",
      "epoch 3  | loss: 8041.729| val_0_mse: 9682.64917|  0:00:00s\n",
      "epoch 4  | loss: 7808.14453| val_0_mse: 10575.48904|  0:00:00s\n",
      "epoch 5  | loss: 7490.77881| val_0_mse: 10954.46787|  0:00:00s\n",
      "epoch 6  | loss: 7087.11279| val_0_mse: 10572.47979|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:28,714] Trial 19 finished with value: 98.40045308221238 and parameters: {'n_d': 14, 'n_a': 41, 'n_steps': 7, 'gamma': 1.3931488648915051, 'lambda_sparse': 0.0002382459694088816, 'learning_rate': 0.03644843868541327, 'batch_size': 64, 'num_epochs': 97}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 7324.38281| val_0_mse: 10072.18798|  0:00:00s\n",
      "epoch 8  | loss: 6820.21875| val_0_mse: 10828.68965|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_mse = 9682.64917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8348.66016| val_0_mse: 13817.53519|  0:00:00s\n",
      "epoch 1  | loss: 8249.56934| val_0_mse: 13613.38777|  0:00:00s\n",
      "epoch 2  | loss: 8176.41064| val_0_mse: 13253.21967|  0:00:00s\n",
      "epoch 3  | loss: 8105.86572| val_0_mse: 13362.83593|  0:00:00s\n",
      "epoch 4  | loss: 8054.76807| val_0_mse: 13538.32029|  0:00:00s\n",
      "epoch 5  | loss: 8004.17041| val_0_mse: 13569.88317|  0:00:00s\n",
      "epoch 6  | loss: 7958.80859| val_0_mse: 13438.54999|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:29,409] Trial 20 finished with value: 115.12262882125101 and parameters: {'n_d': 23, 'n_a': 13, 'n_steps': 6, 'gamma': 1.0033150726666615, 'lambda_sparse': 7.494233009174575e-05, 'learning_rate': 0.0010251628167877867, 'batch_size': 256, 'num_epochs': 53}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 7905.51025| val_0_mse: 13359.51107|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_mse = 13253.21967\n",
      "epoch 0  | loss: 8364.0166| val_0_mse: 14710.9107|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 8116.69385| val_0_mse: 12718.47163|  0:00:00s\n",
      "epoch 2  | loss: 7941.78613| val_0_mse: 12697.38232|  0:00:00s\n",
      "epoch 3  | loss: 7557.5415| val_0_mse: 13547.19845|  0:00:00s\n",
      "epoch 4  | loss: 7288.2334| val_0_mse: 13778.12605|  0:00:00s\n",
      "epoch 5  | loss: 6679.53809| val_0_mse: 12928.76855|  0:00:00s\n",
      "epoch 6  | loss: 6351.2207| val_0_mse: 7367.17947|  0:00:00s\n",
      "epoch 7  | loss: 6258.1958| val_0_mse: 6670.97295|  0:00:00s\n",
      "epoch 8  | loss: 5338.7959| val_0_mse: 6103.93674|  0:00:00s\n",
      "epoch 9  | loss: 4717.07764| val_0_mse: 6060.35834|  0:00:00s\n",
      "epoch 10 | loss: 4201.81055| val_0_mse: 5914.01781|  0:00:00s\n",
      "epoch 11 | loss: 3316.05469| val_0_mse: 5937.27532|  0:00:00s\n",
      "epoch 12 | loss: 3170.19238| val_0_mse: 7141.56569|  0:00:01s\n",
      "epoch 13 | loss: 2339.31738| val_0_mse: 10842.36123|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:30,779] Trial 21 finished with value: 76.90265148771591 and parameters: {'n_d': 28, 'n_a': 54, 'n_steps': 4, 'gamma': 1.8423628654311677, 'lambda_sparse': 0.0006138658556408038, 'learning_rate': 0.03158831546202484, 'batch_size': 64, 'num_epochs': 84}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 2058.63843| val_0_mse: 13555.09677|  0:00:01s\n",
      "epoch 15 | loss: 2697.16528| val_0_mse: 16921.1855|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_mse = 5914.01781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8603.8252| val_0_mse: 16446.10364|  0:00:01s\n",
      "epoch 1  | loss: 8010.84033| val_0_mse: 17027.98676|  0:00:01s\n",
      "epoch 2  | loss: 7360.90723| val_0_mse: 7857.58894|  0:00:01s\n",
      "epoch 3  | loss: 6562.45947| val_0_mse: 19892.83817|  0:00:01s\n",
      "epoch 4  | loss: 5898.22754| val_0_mse: 27635.84816|  0:00:01s\n",
      "epoch 5  | loss: 5069.37646| val_0_mse: 35588.78001|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:33,086] Trial 22 finished with value: 88.64304224554238 and parameters: {'n_d': 24, 'n_a': 64, 'n_steps': 4, 'gamma': 1.7525437034029765, 'lambda_sparse': 0.0007371531572096128, 'learning_rate': 0.07922193575912555, 'batch_size': 64, 'num_epochs': 89}. Best is trial 4 with value: 27.649455942998415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 4088.83887| val_0_mse: 45452.48631|  0:00:01s\n",
      "epoch 7  | loss: 3578.79541| val_0_mse: 62352.48314|  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_mse = 7857.58894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8633.33496| val_0_mse: 6839.38934|  0:00:00s\n",
      "epoch 1  | loss: 8273.11816| val_0_mse: 7378.03251|  0:00:00s\n",
      "epoch 2  | loss: 7855.49609| val_0_mse: 7280.71715|  0:00:00s\n",
      "epoch 3  | loss: 7432.57324| val_0_mse: 8492.0348|  0:00:00s\n",
      "epoch 4  | loss: 7011.39258| val_0_mse: 8269.32929|  0:00:00s\n",
      "epoch 5  | loss: 6462.71191| val_0_mse: 2769.09869|  0:00:00s\n",
      "epoch 6  | loss: 6330.91406| val_0_mse: 2407.87857|  0:00:00s\n",
      "epoch 7  | loss: 5603.52783| val_0_mse: 1757.27259|  0:00:00s\n",
      "epoch 8  | loss: 5215.52246| val_0_mse: 1056.32327|  0:00:00s\n",
      "epoch 9  | loss: 5177.76025| val_0_mse: 816.5443|  0:00:01s\n",
      "epoch 10 | loss: 4581.8584| val_0_mse: 1285.6773|  0:00:01s\n",
      "epoch 11 | loss: 4421.73877| val_0_mse: 1041.33952|  0:00:01s\n",
      "epoch 12 | loss: 4181.3125| val_0_mse: 1351.37448|  0:00:01s\n",
      "epoch 13 | loss: 3395.61475| val_0_mse: 1016.18194|  0:00:01s\n",
      "epoch 14 | loss: 3319.2395| val_0_mse: 435.68885|  0:00:01s\n",
      "epoch 15 | loss: 2820.70264| val_0_mse: 617.68171|  0:00:01s\n",
      "epoch 16 | loss: 2626.62671| val_0_mse: 729.00361|  0:00:01s\n",
      "epoch 17 | loss: 2864.80908| val_0_mse: 1334.96304|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:35,193] Trial 23 finished with value: 20.873160944553998 and parameters: {'n_d': 34, 'n_a': 56, 'n_steps': 5, 'gamma': 1.2462081377166927, 'lambda_sparse': 0.0009268419160555327, 'learning_rate': 0.015156122967260719, 'batch_size': 64, 'num_epochs': 77}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 1928.90674| val_0_mse: 1884.98449|  0:00:01s\n",
      "epoch 19 | loss: 1884.50793| val_0_mse: 3271.46177|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_mse = 435.68885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8253.48047| val_0_mse: 7164.36276|  0:00:00s\n",
      "epoch 1  | loss: 8151.28369| val_0_mse: 7353.36826|  0:00:00s\n",
      "epoch 2  | loss: 7929.4917| val_0_mse: 7382.36032|  0:00:00s\n",
      "epoch 3  | loss: 7793.46631| val_0_mse: 7316.62926|  0:00:00s\n",
      "epoch 4  | loss: 7492.88135| val_0_mse: 8387.2165|  0:00:00s\n",
      "epoch 5  | loss: 7278.59326| val_0_mse: 8695.66873|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 7164.36276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:36,038] Trial 24 finished with value: 84.64255881116017 and parameters: {'n_d': 34, 'n_a': 51, 'n_steps': 6, 'gamma': 1.2017805829449242, 'lambda_sparse': 0.0009872445593222686, 'learning_rate': 0.00813544942415561, 'batch_size': 64, 'num_epochs': 76}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8632.00684| val_0_mse: 6979.04378|  0:00:00s\n",
      "epoch 1  | loss: 8570.22852| val_0_mse: 7975.48361|  0:00:00s\n",
      "epoch 2  | loss: 8332.11426| val_0_mse: 8565.93149|  0:00:00s\n",
      "epoch 3  | loss: 8215.37598| val_0_mse: 8875.68118|  0:00:00s\n",
      "epoch 4  | loss: 8077.62109| val_0_mse: 9284.09412|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:37,186] Trial 25 finished with value: 83.54067142390585 and parameters: {'n_d': 41, 'n_a': 43, 'n_steps': 5, 'gamma': 1.2851053539324238, 'lambda_sparse': 0.00037477666707584183, 'learning_rate': 0.0049163194650405926, 'batch_size': 64, 'num_epochs': 61}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 7855.12012| val_0_mse: 9008.48052|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 6979.04378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8573.23047| val_0_mse: 13732.16397|  0:00:00s\n",
      "epoch 1  | loss: 7742.34668| val_0_mse: 10668.11821|  0:00:00s\n",
      "epoch 2  | loss: 6763.96582| val_0_mse: 8420.10499|  0:00:00s\n",
      "epoch 3  | loss: 5722.22217| val_0_mse: 4699.55229|  0:00:00s\n",
      "epoch 4  | loss: 4815.81299| val_0_mse: 3540.52068|  0:00:00s\n",
      "epoch 5  | loss: 3898.35913| val_0_mse: 2835.86132|  0:00:00s\n",
      "epoch 6  | loss: 3115.88574| val_0_mse: 3736.51201|  0:00:00s\n",
      "epoch 7  | loss: 2501.09863| val_0_mse: 6237.95018|  0:00:01s\n",
      "epoch 8  | loss: 1874.93433| val_0_mse: 6757.67669|  0:00:01s\n",
      "epoch 9  | loss: 1362.80969| val_0_mse: 9358.23623|  0:00:01s\n",
      "epoch 10 | loss: 956.88953| val_0_mse: 9237.17687|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:38,705] Trial 26 finished with value: 53.25280571058216 and parameters: {'n_d': 48, 'n_a': 60, 'n_steps': 7, 'gamma': 1.1340984120771724, 'lambda_sparse': 0.000883182523177946, 'learning_rate': 0.015313022521698838, 'batch_size': 256, 'num_epochs': 81}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_mse = 2835.86132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8462.97266| val_0_mse: 10635.43365|  0:00:00s\n",
      "epoch 1  | loss: 7918.49268| val_0_mse: 10637.20026|  0:00:00s\n",
      "epoch 2  | loss: 7619.87354| val_0_mse: 8550.70157|  0:00:00s\n",
      "epoch 3  | loss: 6891.22119| val_0_mse: 6509.61407|  0:00:00s\n",
      "epoch 4  | loss: 6120.9624| val_0_mse: 2925.95262|  0:00:00s\n",
      "epoch 5  | loss: 5484.12988| val_0_mse: 1440.36669|  0:00:01s\n",
      "epoch 6  | loss: 4787.46143| val_0_mse: 2722.61423|  0:00:01s\n",
      "epoch 7  | loss: 4160.9126| val_0_mse: 2431.02254|  0:00:01s\n",
      "epoch 8  | loss: 3561.90332| val_0_mse: 5439.26076|  0:00:01s\n",
      "epoch 9  | loss: 2973.95679| val_0_mse: 19138.52414|  0:00:01s\n",
      "epoch 10 | loss: 2418.08789| val_0_mse: 28673.04774|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_mse = 1440.36669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:40,875] Trial 27 finished with value: 37.952163217155665 and parameters: {'n_d': 48, 'n_a': 52, 'n_steps': 6, 'gamma': 1.0899367738630545, 'lambda_sparse': 0.0008931543831694979, 'learning_rate': 0.01582417103070356, 'batch_size': 256, 'num_epochs': 77}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8412.11523| val_0_mse: 6288.88784|  0:00:00s\n",
      "epoch 1  | loss: 7003.77051| val_0_mse: 3025.08195|  0:00:00s\n",
      "epoch 2  | loss: 4869.94922| val_0_mse: 5453.51049|  0:00:00s\n",
      "epoch 3  | loss: 3060.41138| val_0_mse: 121129.07259|  0:00:00s\n",
      "epoch 4  | loss: 1729.38904| val_0_mse: 167639.61171|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:41,888] Trial 28 finished with value: 55.0007449821262 and parameters: {'n_d': 51, 'n_a': 33, 'n_steps': 5, 'gamma': 1.0732020293566589, 'lambda_sparse': 0.0009145053614118248, 'learning_rate': 0.063095489565229, 'batch_size': 256, 'num_epochs': 72}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 774.93433| val_0_mse: 266359.8791|  0:00:00s\n",
      "epoch 6  | loss: 240.93373| val_0_mse: 265120.88546|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 3025.08195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8754.36133| val_0_mse: 10756.38726|  0:00:00s\n",
      "epoch 1  | loss: 8588.54297| val_0_mse: 11379.01842|  0:00:00s\n",
      "epoch 2  | loss: 8375.8916| val_0_mse: 11002.0112|  0:00:00s\n",
      "epoch 3  | loss: 8150.87695| val_0_mse: 10294.03961|  0:00:00s\n",
      "epoch 4  | loss: 7953.40674| val_0_mse: 10387.83364|  0:00:00s\n",
      "epoch 5  | loss: 7775.61377| val_0_mse: 10331.52622|  0:00:00s\n",
      "epoch 6  | loss: 7500.56445| val_0_mse: 10188.2733|  0:00:00s\n",
      "epoch 7  | loss: 7254.16406| val_0_mse: 10030.92646|  0:00:00s\n",
      "epoch 8  | loss: 7028.37793| val_0_mse: 8755.81387|  0:00:01s\n",
      "epoch 9  | loss: 6779.64062| val_0_mse: 8654.41025|  0:00:01s\n",
      "epoch 10 | loss: 6532.85596| val_0_mse: 8772.00134|  0:00:01s\n",
      "epoch 11 | loss: 6272.84033| val_0_mse: 8259.51024|  0:00:01s\n",
      "epoch 12 | loss: 6034.20557| val_0_mse: 8700.4995|  0:00:01s\n",
      "epoch 13 | loss: 5799.96875| val_0_mse: 8484.71711|  0:00:01s\n",
      "epoch 14 | loss: 5571.93994| val_0_mse: 8066.84574|  0:00:01s\n",
      "epoch 15 | loss: 5359.46973| val_0_mse: 7667.0361|  0:00:01s\n",
      "epoch 16 | loss: 5150.38379| val_0_mse: 7044.50398|  0:00:02s\n",
      "epoch 17 | loss: 4945.34473| val_0_mse: 6712.93379|  0:00:02s\n",
      "epoch 18 | loss: 4750.28027| val_0_mse: 6496.65192|  0:00:02s\n",
      "epoch 19 | loss: 4550.03027| val_0_mse: 6201.78693|  0:00:02s\n",
      "epoch 20 | loss: 4354.68457| val_0_mse: 5666.2397|  0:00:02s\n",
      "epoch 21 | loss: 4159.88672| val_0_mse: 5200.71444|  0:00:03s\n",
      "epoch 22 | loss: 3969.63721| val_0_mse: 4298.8048|  0:00:03s\n",
      "epoch 23 | loss: 3781.49097| val_0_mse: 3741.86317|  0:00:03s\n",
      "epoch 24 | loss: 3593.21704| val_0_mse: 3569.71455|  0:00:03s\n",
      "epoch 25 | loss: 3406.48975| val_0_mse: 3216.59246|  0:00:03s\n",
      "epoch 26 | loss: 3226.00562| val_0_mse: 3029.40111|  0:00:03s\n",
      "epoch 27 | loss: 3054.02271| val_0_mse: 3179.84969|  0:00:03s\n",
      "epoch 28 | loss: 2888.58936| val_0_mse: 3148.9562|  0:00:03s\n",
      "epoch 29 | loss: 2732.06909| val_0_mse: 3245.17664|  0:00:04s\n",
      "epoch 30 | loss: 2580.46509| val_0_mse: 3218.3103|  0:00:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:46,308] Trial 29 finished with value: 55.03999553087951 and parameters: {'n_d': 63, 'n_a': 44, 'n_steps': 6, 'gamma': 1.3711231025377473, 'lambda_sparse': 0.0009201161212385012, 'learning_rate': 0.002896361774723192, 'batch_size': 256, 'num_epochs': 50}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 | loss: 2427.52197| val_0_mse: 3080.45881|  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 26 and best_val_0_mse = 3029.40111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8625.7041| val_0_mse: 14609.63871|  0:00:00s\n",
      "epoch 1  | loss: 8102.56836| val_0_mse: 14960.03002|  0:00:00s\n",
      "epoch 2  | loss: 7484.67676| val_0_mse: 18471.27518|  0:00:00s\n",
      "epoch 3  | loss: 6820.81836| val_0_mse: 14518.06632|  0:00:00s\n",
      "epoch 4  | loss: 6201.49414| val_0_mse: 13616.95813|  0:00:00s\n",
      "epoch 5  | loss: 5583.68555| val_0_mse: 13542.20558|  0:00:00s\n",
      "epoch 6  | loss: 5042.46826| val_0_mse: 6066.62889|  0:00:00s\n",
      "epoch 7  | loss: 4503.33691| val_0_mse: 5658.53284|  0:00:00s\n",
      "epoch 8  | loss: 4002.89136| val_0_mse: 5462.93253|  0:00:00s\n",
      "epoch 9  | loss: 3500.6416| val_0_mse: 6038.54955|  0:00:01s\n",
      "epoch 10 | loss: 3027.53662| val_0_mse: 12894.79689|  0:00:01s\n",
      "epoch 11 | loss: 2754.49219| val_0_mse: 19007.6954|  0:00:01s\n",
      "epoch 12 | loss: 2236.40161| val_0_mse: 19849.59378|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:48,310] Trial 30 finished with value: 73.91165351689088 and parameters: {'n_d': 47, 'n_a': 52, 'n_steps': 5, 'gamma': 1.2672448427349126, 'lambda_sparse': 0.0007931804146531453, 'learning_rate': 0.01599693065635172, 'batch_size': 256, 'num_epochs': 29}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 1920.23987| val_0_mse: 22110.88912|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_mse = 5462.93253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8860.92578| val_0_mse: 8275.72205|  0:00:00s\n",
      "epoch 1  | loss: 7869.1416| val_0_mse: 6877.26272|  0:00:00s\n",
      "epoch 2  | loss: 7132.50439| val_0_mse: 10935.65351|  0:00:00s\n",
      "epoch 3  | loss: 6411.44287| val_0_mse: 7596.59278|  0:00:00s\n",
      "epoch 4  | loss: 5754.06299| val_0_mse: 2952.19092|  0:00:00s\n",
      "epoch 5  | loss: 4976.2373| val_0_mse: 2171.94766|  0:00:00s\n",
      "epoch 6  | loss: 4335.896| val_0_mse: 2916.75901|  0:00:00s\n",
      "epoch 7  | loss: 3574.52051| val_0_mse: 1742.28951|  0:00:01s\n",
      "epoch 8  | loss: 2998.16992| val_0_mse: 1720.01305|  0:00:01s\n",
      "epoch 9  | loss: 2428.53784| val_0_mse: 1461.53771|  0:00:01s\n",
      "epoch 10 | loss: 1894.4043| val_0_mse: 9646.40203|  0:00:01s\n",
      "epoch 11 | loss: 1426.32507| val_0_mse: 12614.76745|  0:00:01s\n",
      "epoch 12 | loss: 1026.06665| val_0_mse: 16888.33817|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:50,631] Trial 31 finished with value: 38.23006289117313 and parameters: {'n_d': 48, 'n_a': 61, 'n_steps': 7, 'gamma': 1.099131613159436, 'lambda_sparse': 0.0009028817404990103, 'learning_rate': 0.014115693344120127, 'batch_size': 256, 'num_epochs': 79}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 680.66376| val_0_mse: 18191.91005|  0:00:01s\n",
      "epoch 14 | loss: 405.68607| val_0_mse: 30329.40495|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_mse = 1461.53771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8552.54492| val_0_mse: 12103.2638|  0:00:00s\n",
      "epoch 1  | loss: 7551.36865| val_0_mse: 9084.35388|  0:00:00s\n",
      "epoch 2  | loss: 6228.06787| val_0_mse: 6957.18739|  0:00:00s\n",
      "epoch 3  | loss: 4844.90283| val_0_mse: 4773.16083|  0:00:00s\n",
      "epoch 4  | loss: 3718.83594| val_0_mse: 4943.02276|  0:00:00s\n",
      "epoch 5  | loss: 2785.12769| val_0_mse: 5426.45675|  0:00:01s\n",
      "epoch 6  | loss: 1952.92419| val_0_mse: 11880.66934|  0:00:01s\n",
      "epoch 7  | loss: 1271.77759| val_0_mse: 14678.82983|  0:00:01s\n",
      "epoch 8  | loss: 736.11053| val_0_mse: 17239.54458|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_mse = 4773.16083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:52,442] Trial 32 finished with value: 69.08806575457692 and parameters: {'n_d': 52, 'n_a': 55, 'n_steps': 6, 'gamma': 1.0051308958156209, 'lambda_sparse': 0.0008533005184101107, 'learning_rate': 0.023759814466703024, 'batch_size': 256, 'num_epochs': 62}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8237.55566| val_0_mse: 5446.6578|  0:00:00s\n",
      "epoch 1  | loss: 7840.15332| val_0_mse: 6397.65811|  0:00:00s\n",
      "epoch 2  | loss: 7565.14795| val_0_mse: 6007.47341|  0:00:00s\n",
      "epoch 3  | loss: 7317.13232| val_0_mse: 5925.99805|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:40:53,292] Trial 33 finished with value: 73.80147561652501 and parameters: {'n_d': 45, 'n_a': 47, 'n_steps': 6, 'gamma': 1.1097080965780892, 'lambda_sparse': 0.0007962216002552824, 'learning_rate': 0.004222662022431126, 'batch_size': 256, 'num_epochs': 73}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 7006.89404| val_0_mse: 6085.49343|  0:00:00s\n",
      "epoch 5  | loss: 6748.66699| val_0_mse: 6536.25658|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 5446.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8306.72266| val_0_mse: 13593.30672|  0:00:00s\n",
      "epoch 1  | loss: 7866.22559| val_0_mse: 12923.1597|  0:00:00s\n",
      "epoch 2  | loss: 7726.06885| val_0_mse: 12449.39886|  0:00:00s\n",
      "epoch 3  | loss: 7463.77197| val_0_mse: 11942.77288|  0:00:00s\n",
      "epoch 4  | loss: 7219.08691| val_0_mse: 11560.8532|  0:00:00s\n",
      "epoch 5  | loss: 7031.68701| val_0_mse: 11136.54226|  0:00:03s\n",
      "epoch 6  | loss: 6741.6084| val_0_mse: 10026.75426|  0:00:03s\n",
      "epoch 7  | loss: 6514.14697| val_0_mse: 9716.46926|  0:00:03s\n",
      "epoch 8  | loss: 6303.89111| val_0_mse: 9711.19029|  0:00:03s\n",
      "epoch 9  | loss: 6092.84082| val_0_mse: 9668.05568|  0:00:03s\n",
      "epoch 10 | loss: 5889.98096| val_0_mse: 9727.01506|  0:00:04s\n",
      "epoch 11 | loss: 5691.50635| val_0_mse: 9649.12877|  0:00:04s\n",
      "epoch 12 | loss: 5486.3208| val_0_mse: 9578.00132|  0:00:04s\n",
      "epoch 13 | loss: 5285.22754| val_0_mse: 9480.45259|  0:00:04s\n",
      "epoch 14 | loss: 5082.31445| val_0_mse: 9357.15998|  0:00:04s\n",
      "epoch 15 | loss: 4882.68408| val_0_mse: 9217.50399|  0:00:04s\n",
      "epoch 16 | loss: 4695.72705| val_0_mse: 9130.0814|  0:00:05s\n",
      "epoch 17 | loss: 4522.87793| val_0_mse: 9063.63225|  0:00:05s\n",
      "epoch 18 | loss: 4350.63232| val_0_mse: 8964.8535|  0:00:05s\n",
      "epoch 19 | loss: 4192.41992| val_0_mse: 8861.3758|  0:00:05s\n",
      "epoch 20 | loss: 4043.70679| val_0_mse: 8782.42579|  0:00:05s\n",
      "epoch 21 | loss: 3898.95142| val_0_mse: 8715.58777|  0:00:05s\n",
      "epoch 22 | loss: 3751.77051| val_0_mse: 8630.39096|  0:00:05s\n",
      "epoch 23 | loss: 3605.44531| val_0_mse: 8524.88909|  0:00:06s\n",
      "epoch 24 | loss: 3474.49585| val_0_mse: 8410.24539|  0:00:06s\n",
      "epoch 25 | loss: 3339.32129| val_0_mse: 8285.06165|  0:00:06s\n",
      "epoch 26 | loss: 3205.40405| val_0_mse: 8185.22919|  0:00:07s\n",
      "epoch 27 | loss: 3074.29468| val_0_mse: 8101.76404|  0:00:07s\n",
      "epoch 28 | loss: 2941.74878| val_0_mse: 8030.18828|  0:00:07s\n",
      "epoch 29 | loss: 2813.86255| val_0_mse: 7969.65298|  0:00:07s\n",
      "epoch 30 | loss: 2691.40112| val_0_mse: 7913.93474|  0:00:07s\n",
      "epoch 31 | loss: 2587.43335| val_0_mse: 7856.23233|  0:00:07s\n",
      "epoch 32 | loss: 2459.99146| val_0_mse: 7778.65121|  0:00:08s\n",
      "epoch 33 | loss: 2346.14819| val_0_mse: 7678.57211|  0:00:08s\n",
      "epoch 34 | loss: 2240.38354| val_0_mse: 7560.67431|  0:00:08s\n",
      "epoch 35 | loss: 2130.53076| val_0_mse: 7446.87569|  0:00:08s\n",
      "epoch 36 | loss: 2029.12134| val_0_mse: 7326.39171|  0:00:08s\n",
      "epoch 37 | loss: 1923.89893| val_0_mse: 7208.53426|  0:00:08s\n",
      "epoch 38 | loss: 1817.62622| val_0_mse: 7082.15258|  0:00:08s\n",
      "epoch 39 | loss: 1718.01929| val_0_mse: 6962.40792|  0:00:09s\n",
      "epoch 40 | loss: 1655.11816| val_0_mse: 6845.54281|  0:00:09s\n",
      "epoch 41 | loss: 1539.8136| val_0_mse: 6723.7237|  0:00:09s\n",
      "epoch 42 | loss: 1498.43018| val_0_mse: 6597.93338|  0:00:09s\n",
      "epoch 43 | loss: 1394.65308| val_0_mse: 6461.19849|  0:00:09s\n",
      "epoch 44 | loss: 1307.59619| val_0_mse: 6320.49807|  0:00:09s\n",
      "epoch 45 | loss: 1227.16626| val_0_mse: 6182.12144|  0:00:10s\n",
      "epoch 46 | loss: 1170.13135| val_0_mse: 6055.5952|  0:00:10s\n",
      "epoch 47 | loss: 1097.4231| val_0_mse: 5930.51833|  0:00:10s\n",
      "epoch 48 | loss: 1028.97144| val_0_mse: 5797.43204|  0:00:10s\n",
      "epoch 49 | loss: 959.6076| val_0_mse: 5656.78031|  0:00:10s\n",
      "epoch 50 | loss: 896.12762| val_0_mse: 5530.80403|  0:00:10s\n",
      "epoch 51 | loss: 836.16351| val_0_mse: 5402.2092|  0:00:11s\n",
      "epoch 52 | loss: 790.84125| val_0_mse: 5274.3053|  0:00:11s\n",
      "epoch 53 | loss: 729.28107| val_0_mse: 5141.72311|  0:00:11s\n",
      "epoch 54 | loss: 683.66931| val_0_mse: 5010.16404|  0:00:12s\n",
      "epoch 55 | loss: 631.13226| val_0_mse: 4884.1492|  0:00:12s\n",
      "epoch 56 | loss: 585.65759| val_0_mse: 4752.48041|  0:00:12s\n",
      "epoch 57 | loss: 537.69318| val_0_mse: 4627.18362|  0:00:12s\n",
      "epoch 58 | loss: 487.30798| val_0_mse: 4507.69773|  0:00:12s\n",
      "epoch 59 | loss: 443.22165| val_0_mse: 4376.84716|  0:00:12s\n",
      "epoch 60 | loss: 404.37244| val_0_mse: 4239.36591|  0:00:13s\n",
      "epoch 61 | loss: 367.14307| val_0_mse: 4100.27289|  0:00:13s\n",
      "epoch 62 | loss: 324.79645| val_0_mse: 3954.0635|  0:00:13s\n",
      "epoch 63 | loss: 292.27435| val_0_mse: 3828.34899|  0:00:13s\n",
      "epoch 64 | loss: 273.54807| val_0_mse: 3715.48526|  0:00:13s\n",
      "epoch 65 | loss: 239.46451| val_0_mse: 3596.11284|  0:00:13s\n",
      "epoch 66 | loss: 215.06584| val_0_mse: 3501.66961|  0:00:14s\n",
      "epoch 67 | loss: 187.32114| val_0_mse: 3414.13232|  0:00:14s\n",
      "epoch 68 | loss: 167.67484| val_0_mse: 3310.86777|  0:00:14s\n",
      "epoch 69 | loss: 142.31425| val_0_mse: 3214.50834|  0:00:14s\n",
      "epoch 70 | loss: 123.53449| val_0_mse: 3121.62653|  0:00:14s\n",
      "epoch 71 | loss: 105.5032| val_0_mse: 3022.09487|  0:00:15s\n",
      "epoch 72 | loss: 86.63697| val_0_mse: 2908.18673|  0:00:15s\n",
      "epoch 73 | loss: 76.59422| val_0_mse: 2807.2875|  0:00:15s\n",
      "epoch 74 | loss: 58.45284| val_0_mse: 2712.42962|  0:00:16s\n",
      "epoch 75 | loss: 46.0123 | val_0_mse: 2617.94972|  0:00:16s\n",
      "epoch 76 | loss: 36.58451| val_0_mse: 2529.30821|  0:00:16s\n",
      "epoch 77 | loss: 25.38073| val_0_mse: 2432.55904|  0:00:16s\n",
      "epoch 78 | loss: 20.80793| val_0_mse: 2353.3271|  0:00:16s\n",
      "epoch 79 | loss: 29.00966| val_0_mse: 2281.6227|  0:00:16s\n",
      "epoch 80 | loss: 22.07301| val_0_mse: 2221.29181|  0:00:16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:10,698] Trial 34 finished with value: 45.922281127079216 and parameters: {'n_d': 58, 'n_a': 23, 'n_steps': 8, 'gamma': 1.078475871959083, 'lambda_sparse': 0.0009397838555029976, 'learning_rate': 0.0017092337561769259, 'batch_size': 128, 'num_epochs': 82}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 | loss: 11.68875| val_0_mse: 2108.8559|  0:00:17s\n",
      "Stop training because you reached max_epochs = 82 with best_epoch = 81 and best_val_0_mse = 2108.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8386.4209| val_0_mse: 11483.10936|  0:00:00s\n",
      "epoch 1  | loss: 7901.47949| val_0_mse: 13716.57217|  0:00:00s\n",
      "epoch 2  | loss: 7288.05957| val_0_mse: 12632.98742|  0:00:00s\n",
      "epoch 3  | loss: 6875.58008| val_0_mse: 10978.08551|  0:00:00s\n",
      "epoch 4  | loss: 6310.00439| val_0_mse: 10221.04327|  0:00:00s\n",
      "epoch 5  | loss: 5966.84766| val_0_mse: 9818.94095|  0:00:00s\n",
      "epoch 6  | loss: 5168.75342| val_0_mse: 8976.50541|  0:00:01s\n",
      "epoch 7  | loss: 4700.07129| val_0_mse: 7379.33203|  0:00:01s\n",
      "epoch 8  | loss: 4189.94922| val_0_mse: 7841.8222|  0:00:01s\n",
      "epoch 9  | loss: 3717.63354| val_0_mse: 15675.99591|  0:00:01s\n",
      "epoch 10 | loss: 3284.33203| val_0_mse: 8912.13679|  0:00:01s\n",
      "epoch 11 | loss: 2862.40771| val_0_mse: 10207.22712|  0:00:01s\n",
      "epoch 12 | loss: 2429.57959| val_0_mse: 4717.53826|  0:00:01s\n",
      "epoch 13 | loss: 2039.02759| val_0_mse: 3220.91902|  0:00:01s\n",
      "epoch 14 | loss: 1676.32764| val_0_mse: 3720.47285|  0:00:01s\n",
      "epoch 15 | loss: 1337.54956| val_0_mse: 4885.82259|  0:00:02s\n",
      "epoch 16 | loss: 1030.88953| val_0_mse: 7197.98867|  0:00:02s\n",
      "epoch 17 | loss: 756.30402| val_0_mse: 10237.4254|  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:13,705] Trial 35 finished with value: 56.753141060714185 and parameters: {'n_d': 38, 'n_a': 59, 'n_steps': 6, 'gamma': 1.2334923484710107, 'lambda_sparse': 0.0007431679157062545, 'learning_rate': 0.01260500927418449, 'batch_size': 256, 'num_epochs': 65}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 569.19952| val_0_mse: 10956.78722|  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_mse = 3220.91902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8551.14551| val_0_mse: 11360.80746|  0:00:00s\n",
      "epoch 1  | loss: 8135.37158| val_0_mse: 9567.86933|  0:00:00s\n",
      "epoch 2  | loss: 7488.93848| val_0_mse: 10507.93666|  0:00:00s\n",
      "epoch 3  | loss: 6767.31689| val_0_mse: 2422.18363|  0:00:00s\n",
      "epoch 4  | loss: 6053.44971| val_0_mse: 4493.18912|  0:00:00s\n",
      "epoch 5  | loss: 5123.00732| val_0_mse: 18946.65196|  0:00:00s\n",
      "epoch 6  | loss: 3863.54443| val_0_mse: 26418.73575|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:15,069] Trial 36 finished with value: 49.215684782467875 and parameters: {'n_d': 53, 'n_a': 50, 'n_steps': 5, 'gamma': 1.3402833334154456, 'lambda_sparse': 0.0005429298042020002, 'learning_rate': 0.050707047364063576, 'batch_size': 32, 'num_epochs': 93}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 3200.00732| val_0_mse: 58973.80562|  0:00:01s\n",
      "epoch 8  | loss: 2263.62842| val_0_mse: 80513.71241|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_mse = 2422.18363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8317.14062| val_0_mse: 12521.48659|  0:00:00s\n",
      "epoch 1  | loss: 8210.30469| val_0_mse: 12521.32448|  0:00:00s\n",
      "epoch 2  | loss: 7900.07227| val_0_mse: 12334.84626|  0:00:00s\n",
      "epoch 3  | loss: 7624.08594| val_0_mse: 11426.92976|  0:00:00s\n",
      "epoch 4  | loss: 7280.1958| val_0_mse: 14744.66417|  0:00:00s\n",
      "epoch 5  | loss: 7059.44727| val_0_mse: 16531.01815|  0:00:00s\n",
      "epoch 6  | loss: 6679.71143| val_0_mse: 16077.35882|  0:00:00s\n",
      "epoch 7  | loss: 6391.32715| val_0_mse: 9180.617|  0:00:01s\n",
      "epoch 8  | loss: 5936.96729| val_0_mse: 7680.34933|  0:00:01s\n",
      "epoch 9  | loss: 5561.53955| val_0_mse: 14623.52454|  0:00:01s\n",
      "epoch 10 | loss: 5220.25977| val_0_mse: 13083.60701|  0:00:01s\n",
      "epoch 11 | loss: 4856.34473| val_0_mse: 5530.55654|  0:00:01s\n",
      "epoch 12 | loss: 4457.72119| val_0_mse: 5145.7789|  0:00:01s\n",
      "epoch 13 | loss: 4108.51562| val_0_mse: 4527.13437|  0:00:01s\n",
      "epoch 14 | loss: 3766.03735| val_0_mse: 4095.46555|  0:00:02s\n",
      "epoch 15 | loss: 3426.68213| val_0_mse: 4566.59699|  0:00:02s\n",
      "epoch 16 | loss: 3073.11743| val_0_mse: 5629.63113|  0:00:02s\n",
      "epoch 17 | loss: 2741.99292| val_0_mse: 20747.42294|  0:00:02s\n",
      "epoch 18 | loss: 2417.9585| val_0_mse: 21079.50732|  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:17,997] Trial 37 finished with value: 63.99582448360724 and parameters: {'n_d': 32, 'n_a': 61, 'n_steps': 7, 'gamma': 1.410611178318575, 'lambda_sparse': 0.0008458443106048299, 'learning_rate': 0.007172895657782093, 'batch_size': 256, 'num_epochs': 83}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 2129.40479| val_0_mse: 20662.92509|  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_mse = 4095.46555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8425.61719| val_0_mse: 12944.69835|  0:00:00s\n",
      "epoch 1  | loss: 7322.01807| val_0_mse: 9314.64281|  0:00:00s\n",
      "epoch 2  | loss: 6263.61816| val_0_mse: 7997.68843|  0:00:01s\n",
      "epoch 3  | loss: 5386.40723| val_0_mse: 5365.41701|  0:00:01s\n",
      "epoch 4  | loss: 4258.91699| val_0_mse: 3777.32355|  0:00:01s\n",
      "epoch 5  | loss: 3349.86304| val_0_mse: 3176.179|  0:00:02s\n",
      "epoch 6  | loss: 3204.79126| val_0_mse: 4131.91714|  0:00:02s\n",
      "epoch 7  | loss: 1779.92114| val_0_mse: 3753.4809|  0:00:02s\n",
      "epoch 8  | loss: 1527.93005| val_0_mse: 6386.67193|  0:00:02s\n",
      "epoch 9  | loss: 1295.95288| val_0_mse: 8407.2764|  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:21,118] Trial 38 finished with value: 56.357599292097326 and parameters: {'n_d': 60, 'n_a': 56, 'n_steps': 9, 'gamma': 1.1852187431368035, 'lambda_sparse': 0.0006902389674819551, 'learning_rate': 0.022975924925499096, 'batch_size': 32, 'num_epochs': 74}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 920.3772| val_0_mse: 8326.18219|  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_mse = 3176.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8548.01758| val_0_mse: 14714.04787|  0:00:00s\n",
      "epoch 1  | loss: 8522.44531| val_0_mse: 14543.32728|  0:00:00s\n",
      "epoch 2  | loss: 8507.3916| val_0_mse: 14444.19517|  0:00:00s\n",
      "epoch 3  | loss: 8496.91113| val_0_mse: 14278.8817|  0:00:00s\n",
      "epoch 4  | loss: 8487.85352| val_0_mse: 14121.00491|  0:00:00s\n",
      "epoch 5  | loss: 8478.31934| val_0_mse: 14025.89476|  0:00:00s\n",
      "epoch 6  | loss: 8468.57031| val_0_mse: 13901.30229|  0:00:00s\n",
      "epoch 7  | loss: 8458.47266| val_0_mse: 13823.7482|  0:00:00s\n",
      "epoch 8  | loss: 8447.81641| val_0_mse: 13745.9951|  0:00:00s\n",
      "epoch 9  | loss: 8436.83398| val_0_mse: 13677.08089|  0:00:00s\n",
      "epoch 10 | loss: 8425.87207| val_0_mse: 13617.39014|  0:00:00s\n",
      "epoch 11 | loss: 8415.25 | val_0_mse: 13565.46377|  0:00:01s\n",
      "epoch 12 | loss: 8405.125| val_0_mse: 13518.34271|  0:00:01s\n",
      "epoch 13 | loss: 8395.31641| val_0_mse: 13471.05939|  0:00:01s\n",
      "epoch 14 | loss: 8385.54492| val_0_mse: 13425.92625|  0:00:01s\n",
      "epoch 15 | loss: 8375.79102| val_0_mse: 13384.65617|  0:00:01s\n",
      "epoch 16 | loss: 8366.00781| val_0_mse: 13346.79973|  0:00:01s\n",
      "epoch 17 | loss: 8356.28516| val_0_mse: 13311.55979|  0:00:02s\n",
      "epoch 18 | loss: 8346.60156| val_0_mse: 13279.16|  0:00:02s\n",
      "epoch 19 | loss: 8336.68359| val_0_mse: 13248.03136|  0:00:03s\n",
      "epoch 20 | loss: 8326.71289| val_0_mse: 13218.82721|  0:00:03s\n",
      "epoch 21 | loss: 8316.72852| val_0_mse: 13191.54278|  0:00:03s\n",
      "epoch 22 | loss: 8306.83008| val_0_mse: 13166.64807|  0:00:03s\n",
      "epoch 23 | loss: 8296.85547| val_0_mse: 13143.27712|  0:00:03s\n",
      "epoch 24 | loss: 8287.01465| val_0_mse: 13121.21288|  0:00:03s\n",
      "epoch 25 | loss: 8277.27148| val_0_mse: 13101.1542|  0:00:03s\n",
      "epoch 26 | loss: 8267.55957| val_0_mse: 13082.44475|  0:00:03s\n",
      "epoch 27 | loss: 8257.8125| val_0_mse: 13064.27098|  0:00:03s\n",
      "epoch 28 | loss: 8248.01953| val_0_mse: 13046.9598|  0:00:03s\n",
      "epoch 29 | loss: 8238.14453| val_0_mse: 13030.26674|  0:00:04s\n",
      "epoch 30 | loss: 8228.36523| val_0_mse: 13014.28097|  0:00:04s\n",
      "epoch 31 | loss: 8218.64355| val_0_mse: 12999.1206|  0:00:04s\n",
      "epoch 32 | loss: 8209.01758| val_0_mse: 12984.52973|  0:00:04s\n",
      "epoch 33 | loss: 8199.63965| val_0_mse: 12970.50729|  0:00:04s\n",
      "epoch 34 | loss: 8190.4126| val_0_mse: 12956.71454|  0:00:04s\n",
      "epoch 35 | loss: 8181.16846| val_0_mse: 12943.2165|  0:00:04s\n",
      "epoch 36 | loss: 8171.80322| val_0_mse: 12929.90613|  0:00:04s\n",
      "epoch 37 | loss: 8162.34229| val_0_mse: 12916.84821|  0:00:04s\n",
      "epoch 38 | loss: 8152.87207| val_0_mse: 12904.08415|  0:00:04s\n",
      "epoch 39 | loss: 8143.50098| val_0_mse: 12891.87454|  0:00:04s\n",
      "epoch 40 | loss: 8134.06201| val_0_mse: 12880.2742|  0:00:04s\n",
      "epoch 41 | loss: 8124.729| val_0_mse: 12869.07684|  0:00:05s\n",
      "epoch 42 | loss: 8115.56201| val_0_mse: 12857.87572|  0:00:05s\n",
      "epoch 43 | loss: 8106.44678| val_0_mse: 12846.51038|  0:00:05s\n",
      "epoch 44 | loss: 8097.29932| val_0_mse: 12834.81348|  0:00:05s\n",
      "epoch 45 | loss: 8088.16064| val_0_mse: 12822.82394|  0:00:05s\n",
      "epoch 46 | loss: 8078.97168| val_0_mse: 12810.81322|  0:00:05s\n",
      "epoch 47 | loss: 8069.7373| val_0_mse: 12798.75783|  0:00:05s\n",
      "epoch 48 | loss: 8060.56348| val_0_mse: 12786.57138|  0:00:05s\n",
      "epoch 49 | loss: 8051.48145| val_0_mse: 12774.34248|  0:00:05s\n",
      "epoch 50 | loss: 8042.43213| val_0_mse: 12762.53587|  0:00:05s\n",
      "epoch 51 | loss: 8034.26318| val_0_mse: 12751.05244|  0:00:05s\n",
      "epoch 52 | loss: 8026.0791| val_0_mse: 12739.81521|  0:00:05s\n",
      "epoch 53 | loss: 8018.00537| val_0_mse: 12728.6357|  0:00:05s\n",
      "epoch 54 | loss: 8009.95557| val_0_mse: 12717.68549|  0:00:06s\n",
      "epoch 55 | loss: 8001.73584| val_0_mse: 12707.20429|  0:00:06s\n",
      "epoch 56 | loss: 7993.38916| val_0_mse: 12697.3359|  0:00:06s\n",
      "epoch 57 | loss: 7985.09033| val_0_mse: 12687.6035|  0:00:06s\n",
      "epoch 58 | loss: 7976.83691| val_0_mse: 12678.144|  0:00:06s\n",
      "epoch 59 | loss: 7968.62451| val_0_mse: 12668.94105|  0:00:06s\n",
      "epoch 60 | loss: 7960.33496| val_0_mse: 12659.9017|  0:00:06s\n",
      "epoch 61 | loss: 7952.0498| val_0_mse: 12651.10862|  0:00:06s\n",
      "epoch 62 | loss: 7943.79639| val_0_mse: 12642.80604|  0:00:06s\n",
      "epoch 63 | loss: 7935.52002| val_0_mse: 12634.70033|  0:00:06s\n",
      "epoch 64 | loss: 7927.24707| val_0_mse: 12627.01693|  0:00:06s\n",
      "epoch 65 | loss: 7918.94043| val_0_mse: 12619.3863|  0:00:06s\n",
      "epoch 66 | loss: 7910.6416| val_0_mse: 12611.77537|  0:00:06s\n",
      "epoch 67 | loss: 7902.3833| val_0_mse: 12604.51124|  0:00:06s\n",
      "epoch 68 | loss: 7894.23145| val_0_mse: 12597.19309|  0:00:07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:28,374] Trial 39 finished with value: 112.23721793467367 and parameters: {'n_d': 45, 'n_a': 9, 'n_steps': 4, 'gamma': 1.0451311703058161, 'lambda_sparse': 0.0009930492560720047, 'learning_rate': 0.00014450478287569922, 'batch_size': 128, 'num_epochs': 69}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 69 with best_epoch = 68 and best_val_0_mse = 12597.19309\n",
      "epoch 0  | loss: 8473.35254| val_0_mse: 7121.36044|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 8314.01953| val_0_mse: 7239.76313|  0:00:00s\n",
      "epoch 2  | loss: 8164.79883| val_0_mse: 7523.19155|  0:00:00s\n",
      "epoch 3  | loss: 8005.32422| val_0_mse: 7692.12436|  0:00:00s\n",
      "epoch 4  | loss: 7813.81152| val_0_mse: 7724.19352|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:28,962] Trial 40 finished with value: 84.38815343286088 and parameters: {'n_d': 14, 'n_a': 53, 'n_steps': 5, 'gamma': 1.5731477260566833, 'lambda_sparse': 0.0008792038462802367, 'learning_rate': 0.003667845742508077, 'batch_size': 256, 'num_epochs': 93}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 7707.76807| val_0_mse: 10158.42951|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_mse = 7121.36044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8372.73145| val_0_mse: 9693.44444|  0:00:00s\n",
      "epoch 1  | loss: 8347.53711| val_0_mse: 9291.15022|  0:00:00s\n",
      "epoch 2  | loss: 8203.45508| val_0_mse: 9559.06754|  0:00:00s\n",
      "epoch 3  | loss: 7972.2085| val_0_mse: 9489.38998|  0:00:00s\n",
      "epoch 4  | loss: 7950.07129| val_0_mse: 9747.65965|  0:00:00s\n",
      "epoch 5  | loss: 7753.12061| val_0_mse: 9272.9204|  0:00:00s\n",
      "epoch 6  | loss: 7625.46582| val_0_mse: 9889.38005|  0:00:00s\n",
      "epoch 7  | loss: 7437.77051| val_0_mse: 8205.76411|  0:00:00s\n",
      "epoch 8  | loss: 7284.59863| val_0_mse: 9784.80625|  0:00:01s\n",
      "epoch 9  | loss: 7119.95654| val_0_mse: 8842.1658|  0:00:01s\n",
      "epoch 10 | loss: 6979.00488| val_0_mse: 9951.00377|  0:00:01s\n",
      "epoch 11 | loss: 6813.35205| val_0_mse: 9885.02243|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:30,608] Trial 41 finished with value: 90.58567276903717 and parameters: {'n_d': 58, 'n_a': 20, 'n_steps': 8, 'gamma': 1.0922642700553533, 'lambda_sparse': 0.0009342625036619423, 'learning_rate': 0.0017549513373475798, 'batch_size': 128, 'num_epochs': 80}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 6635.0376| val_0_mse: 9712.85953|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_mse = 8205.76411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8208.46289| val_0_mse: 12261.92595|  0:00:00s\n",
      "epoch 1  | loss: 8078.63428| val_0_mse: 11127.96133|  0:00:00s\n",
      "epoch 2  | loss: 7785.43066| val_0_mse: 14742.45948|  0:00:00s\n",
      "epoch 3  | loss: 7725.04199| val_0_mse: 11395.73438|  0:00:00s\n",
      "epoch 4  | loss: 7508.46582| val_0_mse: 11283.25216|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:31,535] Trial 42 finished with value: 105.48915266725105 and parameters: {'n_d': 55, 'n_a': 24, 'n_steps': 8, 'gamma': 1.0604609662216795, 'lambda_sparse': 0.0009432214537631839, 'learning_rate': 0.0019736803316609034, 'batch_size': 128, 'num_epochs': 85}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 7326.21826| val_0_mse: 11659.43654|  0:00:00s\n",
      "epoch 6  | loss: 7136.82861| val_0_mse: 11248.6974|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 11127.96133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8247.88672| val_0_mse: 15099.06547|  0:00:00s\n",
      "epoch 1  | loss: 8117.5376| val_0_mse: 14090.49162|  0:00:00s\n",
      "epoch 2  | loss: 8037.46777| val_0_mse: 13503.84086|  0:00:00s\n",
      "epoch 3  | loss: 7865.08887| val_0_mse: 13420.68171|  0:00:00s\n",
      "epoch 4  | loss: 7663.7915| val_0_mse: 13419.44044|  0:00:00s\n",
      "epoch 5  | loss: 7507.56494| val_0_mse: 13092.88806|  0:00:01s\n",
      "epoch 6  | loss: 7348.37598| val_0_mse: 13144.3836|  0:00:02s\n",
      "epoch 7  | loss: 7174.28027| val_0_mse: 12917.16229|  0:00:02s\n",
      "epoch 8  | loss: 7023.86816| val_0_mse: 12622.77939|  0:00:02s\n",
      "epoch 9  | loss: 7071.2666| val_0_mse: 12534.29479|  0:00:02s\n",
      "epoch 10 | loss: 6844.28613| val_0_mse: 12485.56507|  0:00:02s\n",
      "epoch 11 | loss: 6679.94189| val_0_mse: 12343.08665|  0:00:03s\n",
      "epoch 12 | loss: 6539.98926| val_0_mse: 12189.56578|  0:00:03s\n",
      "epoch 13 | loss: 6416.22217| val_0_mse: 11957.03612|  0:00:03s\n",
      "epoch 14 | loss: 6276.00635| val_0_mse: 11894.41291|  0:00:03s\n",
      "epoch 15 | loss: 6155.3667| val_0_mse: 11839.5701|  0:00:03s\n",
      "epoch 16 | loss: 6015.49902| val_0_mse: 11740.66769|  0:00:03s\n",
      "epoch 17 | loss: 5928.84326| val_0_mse: 11482.72384|  0:00:04s\n",
      "epoch 18 | loss: 5760.55127| val_0_mse: 11326.45783|  0:00:04s\n",
      "epoch 19 | loss: 5648.41309| val_0_mse: 11221.55031|  0:00:04s\n",
      "epoch 20 | loss: 5532.57373| val_0_mse: 11048.17189|  0:00:04s\n",
      "epoch 21 | loss: 5389.84277| val_0_mse: 10951.24125|  0:00:04s\n",
      "epoch 22 | loss: 5249.00928| val_0_mse: 10931.27421|  0:00:04s\n",
      "epoch 23 | loss: 5113.69092| val_0_mse: 10908.11518|  0:00:04s\n",
      "epoch 24 | loss: 4980.34277| val_0_mse: 10868.39448|  0:00:05s\n",
      "epoch 25 | loss: 4844.02002| val_0_mse: 10802.11696|  0:00:05s\n",
      "epoch 26 | loss: 4710.32031| val_0_mse: 10727.54106|  0:00:05s\n",
      "epoch 27 | loss: 4580.32666| val_0_mse: 10621.37658|  0:00:05s\n",
      "epoch 28 | loss: 4444.13135| val_0_mse: 10507.44513|  0:00:05s\n",
      "epoch 29 | loss: 4333.93115| val_0_mse: 10382.34493|  0:00:05s\n",
      "epoch 30 | loss: 4191.54736| val_0_mse: 10276.97838|  0:00:05s\n",
      "epoch 31 | loss: 4092.35449| val_0_mse: 10154.07655|  0:00:06s\n",
      "epoch 32 | loss: 3990.42065| val_0_mse: 10006.23429|  0:00:06s\n",
      "epoch 33 | loss: 3897.25317| val_0_mse: 9872.04467|  0:00:06s\n",
      "epoch 34 | loss: 3785.16113| val_0_mse: 9788.64151|  0:00:06s\n",
      "epoch 35 | loss: 3661.83594| val_0_mse: 9636.97871|  0:00:06s\n",
      "epoch 36 | loss: 3559.78784| val_0_mse: 9433.39815|  0:00:06s\n",
      "epoch 37 | loss: 3459.19849| val_0_mse: 9337.75389|  0:00:07s\n",
      "epoch 38 | loss: 3355.63452| val_0_mse: 9189.47656|  0:00:07s\n",
      "epoch 39 | loss: 3254.60718| val_0_mse: 8954.95943|  0:00:07s\n",
      "epoch 40 | loss: 3155.30371| val_0_mse: 8690.05804|  0:00:07s\n",
      "epoch 41 | loss: 3053.84668| val_0_mse: 8426.96991|  0:00:07s\n",
      "epoch 42 | loss: 2952.35083| val_0_mse: 8126.99717|  0:00:07s\n",
      "epoch 43 | loss: 2848.27954| val_0_mse: 7829.53867|  0:00:07s\n",
      "epoch 44 | loss: 2742.3877| val_0_mse: 7559.68249|  0:00:08s\n",
      "epoch 45 | loss: 2640.79053| val_0_mse: 7358.55807|  0:00:08s\n",
      "epoch 46 | loss: 2539.57886| val_0_mse: 7160.20107|  0:00:08s\n",
      "epoch 47 | loss: 2443.31348| val_0_mse: 6989.52251|  0:00:08s\n",
      "epoch 48 | loss: 2345.17212| val_0_mse: 6826.33704|  0:00:08s\n",
      "epoch 49 | loss: 2248.06055| val_0_mse: 6667.20473|  0:00:08s\n",
      "epoch 50 | loss: 2151.25171| val_0_mse: 6538.26942|  0:00:08s\n",
      "epoch 51 | loss: 2065.91968| val_0_mse: 6412.53816|  0:00:09s\n",
      "epoch 52 | loss: 1984.69177| val_0_mse: 6291.7526|  0:00:09s\n",
      "epoch 53 | loss: 1903.21533| val_0_mse: 6178.80545|  0:00:09s\n",
      "epoch 54 | loss: 1821.61316| val_0_mse: 6071.49474|  0:00:09s\n",
      "epoch 55 | loss: 1745.40527| val_0_mse: 5957.36601|  0:00:09s\n",
      "epoch 56 | loss: 1669.24072| val_0_mse: 5840.42348|  0:00:09s\n",
      "epoch 57 | loss: 1599.9873| val_0_mse: 5739.28472|  0:00:09s\n",
      "epoch 58 | loss: 1523.91113| val_0_mse: 5630.69246|  0:00:10s\n",
      "epoch 59 | loss: 1452.67957| val_0_mse: 5526.82093|  0:00:10s\n",
      "epoch 60 | loss: 1391.5835| val_0_mse: 5447.0773|  0:00:10s\n",
      "epoch 61 | loss: 1324.87524| val_0_mse: 5359.77801|  0:00:10s\n",
      "epoch 62 | loss: 1261.90576| val_0_mse: 5260.33449|  0:00:10s\n",
      "epoch 63 | loss: 1207.47412| val_0_mse: 5169.84135|  0:00:10s\n",
      "epoch 64 | loss: 1148.03003| val_0_mse: 5073.69448|  0:00:10s\n",
      "epoch 65 | loss: 1096.33337| val_0_mse: 4982.8881|  0:00:10s\n",
      "epoch 66 | loss: 1039.59607| val_0_mse: 4905.01037|  0:00:11s\n",
      "epoch 67 | loss: 997.93787| val_0_mse: 4825.02981|  0:00:11s\n",
      "epoch 68 | loss: 939.78412| val_0_mse: 4733.72615|  0:00:11s\n",
      "epoch 69 | loss: 912.16718| val_0_mse: 4660.03023|  0:00:11s\n",
      "epoch 70 | loss: 860.42908| val_0_mse: 4603.44156|  0:00:11s\n",
      "epoch 71 | loss: 811.79047| val_0_mse: 4551.48344|  0:00:11s\n",
      "epoch 72 | loss: 776.31226| val_0_mse: 4499.01439|  0:00:11s\n",
      "epoch 73 | loss: 734.0639| val_0_mse: 4436.0208|  0:00:12s\n",
      "epoch 74 | loss: 695.15784| val_0_mse: 4362.92753|  0:00:12s\n",
      "epoch 75 | loss: 654.99426| val_0_mse: 4279.74466|  0:00:12s\n",
      "epoch 76 | loss: 618.37909| val_0_mse: 4198.27885|  0:00:12s\n",
      "epoch 77 | loss: 577.42047| val_0_mse: 4124.19367|  0:00:12s\n",
      "epoch 78 | loss: 540.2807| val_0_mse: 4073.58472|  0:00:12s\n",
      "epoch 79 | loss: 505.91699| val_0_mse: 4022.11179|  0:00:12s\n",
      "Stop training because you reached max_epochs = 80 with best_epoch = 79 and best_val_0_mse = 4022.11179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:44,758] Trial 43 finished with value: 63.42012132220404 and parameters: {'n_d': 64, 'n_a': 13, 'n_steps': 9, 'gamma': 1.1186087250640335, 'lambda_sparse': 0.0008268526075009608, 'learning_rate': 0.0012000172839192395, 'batch_size': 128, 'num_epochs': 80}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8578.6582| val_0_mse: 10516.09246|  0:00:00s\n",
      "epoch 1  | loss: 8473.75684| val_0_mse: 10356.12176|  0:00:00s\n",
      "epoch 2  | loss: 8128.8501| val_0_mse: 10192.3672|  0:00:00s\n",
      "epoch 3  | loss: 7868.57715| val_0_mse: 9563.09009|  0:00:00s\n",
      "epoch 4  | loss: 7544.38721| val_0_mse: 9074.24542|  0:00:00s\n",
      "epoch 5  | loss: 7444.22559| val_0_mse: 11012.50166|  0:00:00s\n",
      "epoch 6  | loss: 7303.72559| val_0_mse: 10674.21014|  0:00:00s\n",
      "epoch 7  | loss: 6981.70215| val_0_mse: 10731.19903|  0:00:01s\n",
      "epoch 8  | loss: 6770.92871| val_0_mse: 10723.94826|  0:00:01s\n",
      "epoch 9  | loss: 6529.0332| val_0_mse: 10650.85821|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_mse = 9074.24542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:46,309] Trial 44 finished with value: 95.25883379028986 and parameters: {'n_d': 49, 'n_a': 23, 'n_steps': 10, 'gamma': 1.2975813150280087, 'lambda_sparse': 0.0007407678357806717, 'learning_rate': 0.006194482200904681, 'batch_size': 128, 'num_epochs': 96}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8609.1875| val_0_mse: 9548.21465|  0:00:00s\n",
      "epoch 1  | loss: 8078.28271| val_0_mse: 8665.74289|  0:00:00s\n",
      "epoch 2  | loss: 7821.70557| val_0_mse: 9250.2609|  0:00:00s\n",
      "epoch 3  | loss: 7635.89209| val_0_mse: 9399.69039|  0:00:00s\n",
      "epoch 4  | loss: 7329.5498| val_0_mse: 9041.65559|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:47,211] Trial 45 finished with value: 93.08997202596397 and parameters: {'n_d': 60, 'n_a': 14, 'n_steps': 7, 'gamma': 1.1692021283036722, 'lambda_sparse': 0.0009563178299581709, 'learning_rate': 0.012543558942659705, 'batch_size': 32, 'num_epochs': 57}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 6761.85107| val_0_mse: 9094.42145|  0:00:00s\n",
      "epoch 6  | loss: 6346.97705| val_0_mse: 9077.40492|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 8665.74289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8556.85547| val_0_mse: 13390.70274|  0:00:00s\n",
      "epoch 1  | loss: 8267.03711| val_0_mse: 12414.25416|  0:00:00s\n",
      "epoch 2  | loss: 7621.33447| val_0_mse: 12268.57089|  0:00:00s\n",
      "epoch 3  | loss: 7041.17529| val_0_mse: 11469.37095|  0:00:00s\n",
      "epoch 4  | loss: 6440.3999| val_0_mse: 11254.63788|  0:00:00s\n",
      "epoch 5  | loss: 5875.55811| val_0_mse: 10524.21426|  0:00:00s\n",
      "epoch 6  | loss: 5340.99658| val_0_mse: 9571.25612|  0:00:00s\n",
      "epoch 7  | loss: 4872.00146| val_0_mse: 9543.97607|  0:00:00s\n",
      "epoch 8  | loss: 4419.04688| val_0_mse: 9182.25565|  0:00:00s\n",
      "epoch 9  | loss: 3990.94897| val_0_mse: 8406.4853|  0:00:01s\n",
      "epoch 10 | loss: 3580.94067| val_0_mse: 4927.95549|  0:00:01s\n",
      "epoch 11 | loss: 3189.74902| val_0_mse: 6429.39374|  0:00:01s\n",
      "epoch 12 | loss: 2826.24634| val_0_mse: 5335.39991|  0:00:01s\n",
      "epoch 13 | loss: 2476.02588| val_0_mse: 5009.65954|  0:00:01s\n",
      "epoch 14 | loss: 2149.65137| val_0_mse: 5766.44209|  0:00:01s\n",
      "epoch 15 | loss: 1843.37903| val_0_mse: 6748.97862|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_mse = 4927.95549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:49,013] Trial 46 finished with value: 70.19939804730274 and parameters: {'n_d': 39, 'n_a': 57, 'n_steps': 6, 'gamma': 1.0261263322231047, 'lambda_sparse': 0.000895646202660909, 'learning_rate': 0.009770135770208308, 'batch_size': 256, 'num_epochs': 78}. Best is trial 23 with value: 20.873160944553998.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8414.08984| val_0_mse: 13292.00981|  0:00:00s\n",
      "epoch 1  | loss: 6944.19043| val_0_mse: 3093.82785|  0:00:00s\n",
      "epoch 2  | loss: 5298.78125| val_0_mse: 3511.40451|  0:00:00s\n",
      "epoch 3  | loss: 3872.47314| val_0_mse: 24654.26501|  0:00:00s\n",
      "epoch 4  | loss: 2498.27002| val_0_mse: 45182.44317|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:49,972] Trial 47 finished with value: 55.62218848101857 and parameters: {'n_d': 54, 'n_a': 46, 'n_steps': 7, 'gamma': 1.0795042561881187, 'lambda_sparse': 0.0006331012510364267, 'learning_rate': 0.029996232930594217, 'batch_size': 128, 'num_epochs': 85}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5  | loss: 1348.40125| val_0_mse: 101050.60802|  0:00:00s\n",
      "epoch 6  | loss: 613.2735| val_0_mse: 233394.96182|  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_mse = 3093.82785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8469.82227| val_0_mse: 6562.53158|  0:00:00s\n",
      "epoch 1  | loss: 7880.1543| val_0_mse: 9368.74111|  0:00:00s\n",
      "epoch 2  | loss: 6920.66846| val_0_mse: 5331.89077|  0:00:00s\n",
      "epoch 3  | loss: 6167.62744| val_0_mse: 8387.6256|  0:00:00s\n",
      "epoch 4  | loss: 5456.9668| val_0_mse: 4233.53098|  0:00:00s\n",
      "epoch 5  | loss: 4704.30713| val_0_mse: 3983.07991|  0:00:00s\n",
      "epoch 6  | loss: 3767.64014| val_0_mse: 7924.55165|  0:00:00s\n",
      "epoch 7  | loss: 3351.23853| val_0_mse: 8938.55131|  0:00:00s\n",
      "epoch 8  | loss: 2611.43652| val_0_mse: 5303.9157|  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:51,429] Trial 48 finished with value: 63.11164642087494 and parameters: {'n_d': 44, 'n_a': 33, 'n_steps': 8, 'gamma': 1.2299870568440276, 'lambda_sparse': 0.0005078887895404565, 'learning_rate': 0.019036255680937857, 'batch_size': 256, 'num_epochs': 71}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 1932.27917| val_0_mse: 5589.64427|  0:00:01s\n",
      "epoch 10 | loss: 1361.82776| val_0_mse: 5204.45158|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_mse = 3983.07991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\2903159880.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8665.9375| val_0_mse: 13189.87302|  0:00:00s\n",
      "epoch 1  | loss: 8659.88184| val_0_mse: 13155.67646|  0:00:00s\n",
      "epoch 2  | loss: 8609.62305| val_0_mse: 13243.7367|  0:00:00s\n",
      "epoch 3  | loss: 8567.88184| val_0_mse: 13284.06595|  0:00:00s\n",
      "epoch 4  | loss: 8532.4707| val_0_mse: 13093.53628|  0:00:01s\n",
      "epoch 5  | loss: 8505.46387| val_0_mse: 13353.36838|  0:00:01s\n",
      "epoch 6  | loss: 8473.07129| val_0_mse: 13217.03202|  0:00:01s\n",
      "epoch 7  | loss: 8441.77441| val_0_mse: 12863.75926|  0:00:01s\n",
      "epoch 8  | loss: 8422.89453| val_0_mse: 12743.47572|  0:00:01s\n",
      "epoch 9  | loss: 8395.17871| val_0_mse: 12710.85796|  0:00:01s\n",
      "epoch 10 | loss: 8369.17676| val_0_mse: 12718.80167|  0:00:01s\n",
      "epoch 11 | loss: 8344.41797| val_0_mse: 12683.13186|  0:00:01s\n",
      "epoch 12 | loss: 8324.32422| val_0_mse: 12825.18767|  0:00:01s\n",
      "epoch 13 | loss: 8303.06543| val_0_mse: 12730.47161|  0:00:01s\n",
      "epoch 14 | loss: 8279.33789| val_0_mse: 12634.37808|  0:00:01s\n",
      "epoch 15 | loss: 8254.62305| val_0_mse: 12546.46949|  0:00:01s\n",
      "epoch 16 | loss: 8235.78906| val_0_mse: 12444.44548|  0:00:02s\n",
      "epoch 17 | loss: 8209.47656| val_0_mse: 12392.75194|  0:00:02s\n",
      "epoch 18 | loss: 8191.65234| val_0_mse: 12370.8172|  0:00:02s\n",
      "epoch 19 | loss: 8171.68018| val_0_mse: 12343.44326|  0:00:02s\n",
      "epoch 20 | loss: 8146.71143| val_0_mse: 12313.65592|  0:00:02s\n",
      "epoch 21 | loss: 8125.96289| val_0_mse: 12284.06853|  0:00:02s\n",
      "epoch 22 | loss: 8134.96777| val_0_mse: 12251.65796|  0:00:02s\n",
      "epoch 23 | loss: 8098.70996| val_0_mse: 12247.01731|  0:00:02s\n",
      "epoch 24 | loss: 8079.90527| val_0_mse: 12224.85854|  0:00:02s\n",
      "epoch 25 | loss: 8064.51953| val_0_mse: 12206.18651|  0:00:02s\n",
      "epoch 26 | loss: 8044.81494| val_0_mse: 12185.88067|  0:00:02s\n",
      "epoch 27 | loss: 8021.84082| val_0_mse: 12159.53123|  0:00:03s\n",
      "epoch 28 | loss: 7997.05518| val_0_mse: 12135.11299|  0:00:03s\n",
      "epoch 29 | loss: 8023.33252| val_0_mse: 12110.79601|  0:00:03s\n",
      "epoch 30 | loss: 7991.35059| val_0_mse: 12091.38597|  0:00:03s\n",
      "epoch 31 | loss: 7943.32471| val_0_mse: 12080.14795|  0:00:03s\n",
      "epoch 32 | loss: 7925.45996| val_0_mse: 12067.05206|  0:00:03s\n",
      "epoch 33 | loss: 7913.54492| val_0_mse: 12052.42181|  0:00:03s\n",
      "epoch 34 | loss: 7896.93457| val_0_mse: 12039.14136|  0:00:03s\n",
      "epoch 35 | loss: 7880.00244| val_0_mse: 12028.8862|  0:00:03s\n",
      "epoch 36 | loss: 7858.6875| val_0_mse: 12018.70788|  0:00:03s\n",
      "epoch 37 | loss: 7842.81543| val_0_mse: 12012.03044|  0:00:03s\n",
      "epoch 38 | loss: 7819.95703| val_0_mse: 12005.98253|  0:00:04s\n",
      "epoch 39 | loss: 7804.12549| val_0_mse: 12000.71838|  0:00:04s\n",
      "epoch 40 | loss: 7792.74805| val_0_mse: 12005.91821|  0:00:04s\n",
      "epoch 41 | loss: 7767.63672| val_0_mse: 12052.53688|  0:00:04s\n",
      "epoch 42 | loss: 7762.33105| val_0_mse: 12086.13037|  0:00:04s\n",
      "epoch 43 | loss: 7746.95947| val_0_mse: 12092.09958|  0:00:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 08:41:56,097] Trial 49 finished with value: 109.54779040742179 and parameters: {'n_d': 57, 'n_a': 17, 'n_steps': 5, 'gamma': 1.4793420463559992, 'lambda_sparse': 0.0008450546734865454, 'learning_rate': 0.0002658764086456467, 'batch_size': 256, 'num_epochs': 64}. Best is trial 23 with value: 20.873160944553998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 | loss: 7735.33984| val_0_mse: 12085.09503|  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 39 and best_val_0_mse = 12000.71838\n",
      "epoch 0  | loss: 8633.33496| val_0_mse: 6839.38934|  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 8273.11816| val_0_mse: 7378.03251|  0:00:00s\n",
      "epoch 2  | loss: 7855.49609| val_0_mse: 7280.71715|  0:00:00s\n",
      "epoch 3  | loss: 7432.57324| val_0_mse: 8492.0348|  0:00:00s\n",
      "epoch 4  | loss: 7011.39258| val_0_mse: 8269.32929|  0:00:00s\n",
      "epoch 5  | loss: 6462.71191| val_0_mse: 2769.09869|  0:00:00s\n",
      "epoch 6  | loss: 6330.91406| val_0_mse: 2407.87857|  0:00:00s\n",
      "epoch 7  | loss: 5603.52783| val_0_mse: 1757.27259|  0:00:00s\n",
      "epoch 8  | loss: 5215.52246| val_0_mse: 1056.32327|  0:00:00s\n",
      "epoch 9  | loss: 5177.76025| val_0_mse: 816.5443|  0:00:00s\n",
      "epoch 10 | loss: 4581.8584| val_0_mse: 1285.6773|  0:00:00s\n",
      "epoch 11 | loss: 4421.73877| val_0_mse: 1041.33952|  0:00:01s\n",
      "epoch 12 | loss: 4181.3125| val_0_mse: 1351.37448|  0:00:01s\n",
      "epoch 13 | loss: 3395.61475| val_0_mse: 1016.18194|  0:00:01s\n",
      "epoch 14 | loss: 3319.2395| val_0_mse: 435.68885|  0:00:01s\n",
      "epoch 15 | loss: 2820.70264| val_0_mse: 617.68171|  0:00:01s\n",
      "epoch 16 | loss: 2626.62671| val_0_mse: 729.00361|  0:00:01s\n",
      "epoch 17 | loss: 2864.80908| val_0_mse: 1334.96304|  0:00:01s\n",
      "epoch 18 | loss: 1928.90674| val_0_mse: 1884.98449|  0:00:01s\n",
      "epoch 19 | loss: 1884.50793| val_0_mse: 3271.46177|  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_mse = 435.68885\n",
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "NAS                 16.521265  0.952756       0.41489            0.0   \n",
      "KAN                  2.629192  0.998803      0.262269       0.000997   \n",
      "VIME                43.168236  0.677454      0.275263       0.000997   \n",
      "TabNet              20.873161  0.924588      1.741344       0.016957   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "NAS                      18.283068   \n",
      "KAN                      17.044498   \n",
      "VIME                     20.055836   \n",
      "TabNet                  125.814531   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "NAS                {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1'...  \n",
      "KAN                {'inner_dim': 60, 'outer_dim': 46, 'learning_r...  \n",
      "VIME               {'hidden_dim': 228, 'learning_rate': 0.0124134...  \n",
      "TabNet             {'n_d': 34, 'n_a': 56, 'n_steps': 5, 'gamma': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_d: 34\n",
      "n_a: 56\n",
      "n_steps: 5\n",
      "gamma: 1.2462081377166927\n",
      "lambda_sparse: 0.0009268419160555327\n",
      "learning_rate: 0.015156122967260719\n",
      "batch_size: 64\n",
      "num_epochs: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape y_train and y_test to 2D arrays\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    n_d = trial.suggest_int('n_d', 8, 64)\n",
    "    n_a = trial.suggest_int('n_a', 8, 64)\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the TabNet model\n",
    "    model = TabNetRegressor(\n",
    "        n_d=n_d, n_a=n_a, n_steps=n_steps, gamma=gamma, lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=5,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=batch_size // 4,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final TabNet model with the best hyperparameters\n",
    "best_model = TabNetRegressor(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "    scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax'\n",
    ")\n",
    "\n",
    "training_start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_test_scaled, y_test)],\n",
    "    max_epochs=best_params['num_epochs'],\n",
    "    patience=5,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    virtual_batch_size=best_params['batch_size'] // 4,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "inference_start_time = time.time()\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['TabNet'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:35:27,756] A new study created in memory with name: no-name-96957bfb-727e-4ca1-9508-7751ece5b423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:28,116] Trial 0 finished with value: 47.61610412597656 and parameters: {'inner_dim': 16, 'outer_dim': 47, 'learning_rate': 0.024741840873254223, 'batch_size': 128, 'num_epochs': 35}. Best is trial 0 with value: 47.61610412597656.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:28,573] Trial 1 finished with value: 33.1011848449707 and parameters: {'inner_dim': 41, 'outer_dim': 29, 'learning_rate': 0.015935626011767175, 'batch_size': 32, 'num_epochs': 44}. Best is trial 1 with value: 33.1011848449707.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:29,026] Trial 2 finished with value: 28.40443229675293 and parameters: {'inner_dim': 21, 'outer_dim': 30, 'learning_rate': 0.011506847037759158, 'batch_size': 128, 'num_epochs': 56}. Best is trial 2 with value: 28.40443229675293.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:29,132] Trial 3 finished with value: 136.32568359375 and parameters: {'inner_dim': 9, 'outer_dim': 11, 'learning_rate': 0.042048770245315226, 'batch_size': 256, 'num_epochs': 23}. Best is trial 2 with value: 28.40443229675293.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:29,635] Trial 4 finished with value: 110.56909942626953 and parameters: {'inner_dim': 28, 'outer_dim': 19, 'learning_rate': 0.00011659398143980287, 'batch_size': 128, 'num_epochs': 85}. Best is trial 2 with value: 28.40443229675293.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:29,742] Trial 5 finished with value: 110.5545425415039 and parameters: {'inner_dim': 34, 'outer_dim': 35, 'learning_rate': 0.0005630005071618103, 'batch_size': 256, 'num_epochs': 19}. Best is trial 2 with value: 28.40443229675293.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:29,992] Trial 6 finished with value: 10.7085599899292 and parameters: {'inner_dim': 48, 'outer_dim': 42, 'learning_rate': 0.08165020960114677, 'batch_size': 128, 'num_epochs': 58}. Best is trial 6 with value: 10.7085599899292.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:30,046] Trial 7 finished with value: 110.63373565673828 and parameters: {'inner_dim': 13, 'outer_dim': 28, 'learning_rate': 0.0004026934696090153, 'batch_size': 256, 'num_epochs': 11}. Best is trial 6 with value: 10.7085599899292.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:30,178] Trial 8 finished with value: 109.24362182617188 and parameters: {'inner_dim': 32, 'outer_dim': 15, 'learning_rate': 0.0052161893316685, 'batch_size': 128, 'num_epochs': 22}. Best is trial 6 with value: 10.7085599899292.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:30,548] Trial 9 finished with value: 6.281011581420898 and parameters: {'inner_dim': 62, 'outer_dim': 30, 'learning_rate': 0.017348460603633053, 'batch_size': 64, 'num_epochs': 82}. Best is trial 9 with value: 6.281011581420898.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:30,997] Trial 10 finished with value: 94.14451599121094 and parameters: {'inner_dim': 62, 'outer_dim': 61, 'learning_rate': 0.0019325335842202683, 'batch_size': 64, 'num_epochs': 100}. Best is trial 9 with value: 6.281011581420898.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:31,403] Trial 11 finished with value: 1.7156435251235962 and parameters: {'inner_dim': 60, 'outer_dim': 46, 'learning_rate': 0.07643967149509091, 'batch_size': 64, 'num_epochs': 70}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:31,715] Trial 12 finished with value: 8.581356048583984 and parameters: {'inner_dim': 64, 'outer_dim': 52, 'learning_rate': 0.08382610677254491, 'batch_size': 64, 'num_epochs': 73}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:32,088] Trial 13 finished with value: 51.98821258544922 and parameters: {'inner_dim': 53, 'outer_dim': 56, 'learning_rate': 0.004241285875162759, 'batch_size': 64, 'num_epochs': 76}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:32,511] Trial 14 finished with value: 8.170241355895996 and parameters: {'inner_dim': 56, 'outer_dim': 41, 'learning_rate': 0.033893644426430544, 'batch_size': 64, 'num_epochs': 94}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:32,813] Trial 15 finished with value: 42.99568557739258 and parameters: {'inner_dim': 47, 'outer_dim': 20, 'learning_rate': 0.009818926608443756, 'batch_size': 64, 'num_epochs': 70}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:33,180] Trial 16 finished with value: 8.05672836303711 and parameters: {'inner_dim': 57, 'outer_dim': 38, 'learning_rate': 0.05252397797514457, 'batch_size': 32, 'num_epochs': 85}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:33,464] Trial 17 finished with value: 109.0099105834961 and parameters: {'inner_dim': 42, 'outer_dim': 48, 'learning_rate': 0.0015797673554776698, 'batch_size': 64, 'num_epochs': 62}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:33,868] Trial 18 finished with value: 9.522369384765625 and parameters: {'inner_dim': 52, 'outer_dim': 64, 'learning_rate': 0.022400191324483357, 'batch_size': 64, 'num_epochs': 84}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:34,065] Trial 19 finished with value: 71.57182312011719 and parameters: {'inner_dim': 60, 'outer_dim': 24, 'learning_rate': 0.009434280160343547, 'batch_size': 64, 'num_epochs': 43}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:34,435] Trial 20 finished with value: 9.591522216796875 and parameters: {'inner_dim': 41, 'outer_dim': 34, 'learning_rate': 0.08577366979270272, 'batch_size': 32, 'num_epochs': 68}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:34,847] Trial 21 finished with value: 10.811080932617188 and parameters: {'inner_dim': 58, 'outer_dim': 40, 'learning_rate': 0.046428116260842374, 'batch_size': 32, 'num_epochs': 84}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:35,213] Trial 22 finished with value: 7.727831840515137 and parameters: {'inner_dim': 64, 'outer_dim': 46, 'learning_rate': 0.04386638235719305, 'batch_size': 32, 'num_epochs': 89}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:35,608] Trial 23 finished with value: 7.843449592590332 and parameters: {'inner_dim': 63, 'outer_dim': 46, 'learning_rate': 0.019874932340585016, 'batch_size': 32, 'num_epochs': 93}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:36,011] Trial 24 finished with value: 32.377662658691406 and parameters: {'inner_dim': 51, 'outer_dim': 54, 'learning_rate': 0.007533528869571057, 'batch_size': 32, 'num_epochs': 78}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:36,441] Trial 25 finished with value: 11.659945487976074 and parameters: {'inner_dim': 64, 'outer_dim': 50, 'learning_rate': 0.0975883622861381, 'batch_size': 64, 'num_epochs': 92}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:36,766] Trial 26 finished with value: 14.809103965759277 and parameters: {'inner_dim': 56, 'outer_dim': 45, 'learning_rate': 0.03512515770303062, 'batch_size': 64, 'num_epochs': 65}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:37,111] Trial 27 finished with value: 3.2107760906219482 and parameters: {'inner_dim': 60, 'outer_dim': 59, 'learning_rate': 0.054678342390054445, 'batch_size': 32, 'num_epochs': 78}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:37,337] Trial 28 finished with value: 9.026999473571777 and parameters: {'inner_dim': 45, 'outer_dim': 59, 'learning_rate': 0.05895522080240003, 'batch_size': 256, 'num_epochs': 49}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:37,676] Trial 29 finished with value: 5.549515724182129 and parameters: {'inner_dim': 22, 'outer_dim': 57, 'learning_rate': 0.02515307502014175, 'batch_size': 64, 'num_epochs': 78}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:37,949] Trial 30 finished with value: 13.135465621948242 and parameters: {'inner_dim': 22, 'outer_dim': 56, 'learning_rate': 0.030390482744137477, 'batch_size': 32, 'num_epochs': 61}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:38,327] Trial 31 finished with value: 21.183856964111328 and parameters: {'inner_dim': 25, 'outer_dim': 60, 'learning_rate': 0.015402213415007273, 'batch_size': 64, 'num_epochs': 79}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:38,657] Trial 32 finished with value: 11.741250038146973 and parameters: {'inner_dim': 15, 'outer_dim': 51, 'learning_rate': 0.02363734650072168, 'batch_size': 64, 'num_epochs': 72}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:38,996] Trial 33 finished with value: 27.740154266357422 and parameters: {'inner_dim': 19, 'outer_dim': 64, 'learning_rate': 0.015142148826533176, 'batch_size': 64, 'num_epochs': 78}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:39,239] Trial 34 finished with value: 7.314434051513672 and parameters: {'inner_dim': 60, 'outer_dim': 31, 'learning_rate': 0.06172186376897619, 'batch_size': 64, 'num_epochs': 51}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:39,667] Trial 35 finished with value: 6.468154430389404 and parameters: {'inner_dim': 9, 'outer_dim': 58, 'learning_rate': 0.027452114809662868, 'batch_size': 128, 'num_epochs': 100}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:40,047] Trial 36 finished with value: 34.188560485839844 and parameters: {'inner_dim': 37, 'outer_dim': 8, 'learning_rate': 0.014352328913337566, 'batch_size': 64, 'num_epochs': 81}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:40,380] Trial 37 finished with value: 2.9892306327819824 and parameters: {'inner_dim': 29, 'outer_dim': 27, 'learning_rate': 0.05870184897092203, 'batch_size': 256, 'num_epochs': 65}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:41,078] Trial 38 finished with value: 3.529696226119995 and parameters: {'inner_dim': 26, 'outer_dim': 53, 'learning_rate': 0.06311285734534254, 'batch_size': 256, 'num_epochs': 66}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:41,590] Trial 39 finished with value: 13.934160232543945 and parameters: {'inner_dim': 30, 'outer_dim': 22, 'learning_rate': 0.06457974793362331, 'batch_size': 256, 'num_epochs': 39}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:41,952] Trial 40 finished with value: 110.33489227294922 and parameters: {'inner_dim': 27, 'outer_dim': 27, 'learning_rate': 0.0003798148484862867, 'batch_size': 256, 'num_epochs': 55}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:42,354] Trial 41 finished with value: 8.597166061401367 and parameters: {'inner_dim': 23, 'outer_dim': 57, 'learning_rate': 0.04037775885445373, 'batch_size': 256, 'num_epochs': 63}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:42,652] Trial 42 finished with value: 2.1548209190368652 and parameters: {'inner_dim': 35, 'outer_dim': 53, 'learning_rate': 0.061884122298554466, 'batch_size': 256, 'num_epochs': 69}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:42,972] Trial 43 finished with value: 40.50236892700195 and parameters: {'inner_dim': 36, 'outer_dim': 52, 'learning_rate': 0.06969668670000267, 'batch_size': 256, 'num_epochs': 68}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:43,253] Trial 44 finished with value: 5.7074666023254395 and parameters: {'inner_dim': 32, 'outer_dim': 44, 'learning_rate': 0.09450190115574963, 'batch_size': 256, 'num_epochs': 59}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:43,580] Trial 45 finished with value: 110.6042251586914 and parameters: {'inner_dim': 39, 'outer_dim': 49, 'learning_rate': 0.00011804699164565661, 'batch_size': 256, 'num_epochs': 74}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:43,729] Trial 46 finished with value: 20.10564613342285 and parameters: {'inner_dim': 28, 'outer_dim': 54, 'learning_rate': 0.04897554727846856, 'batch_size': 256, 'num_epochs': 30}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:43,973] Trial 47 finished with value: 12.173396110534668 and parameters: {'inner_dim': 18, 'outer_dim': 61, 'learning_rate': 0.06840624319832024, 'batch_size': 256, 'num_epochs': 54}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:44,257] Trial 48 finished with value: 103.99225616455078 and parameters: {'inner_dim': 31, 'outer_dim': 15, 'learning_rate': 0.002577736588283512, 'batch_size': 256, 'num_epochs': 66}. Best is trial 11 with value: 1.7156435251235962.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\1506177012.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 08:35:44,517] Trial 49 finished with value: 6.822533130645752 and parameters: {'inner_dim': 33, 'outer_dim': 54, 'learning_rate': 0.03659972349924591, 'batch_size': 128, 'num_epochs': 59}. Best is trial 11 with value: 1.7156435251235962.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "NAS                 16.521265  0.952756       0.41489            0.0   \n",
      "KAN                  2.629192  0.998803      0.262269       0.000997   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "NAS                      18.283068   \n",
      "KAN                      17.044498   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "NAS                {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1'...  \n",
      "KAN                {'inner_dim': 60, 'outer_dim': 46, 'learning_r...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "inner_dim: 60\n",
      "outer_dim: 46\n",
      "learning_rate: 0.07643967149509091\n",
      "batch_size: 64\n",
      "num_epochs: 70\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, inner_dim, outer_dim):\n",
    "        super(KAN, self).__init__()\n",
    "        self.inner_functions = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(1, inner_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inner_dim, 1)\n",
    "        ) for _ in range(input_dim)])\n",
    "        \n",
    "        self.outer_function = nn.Sequential(\n",
    "            nn.Linear(input_dim, outer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(outer_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inner_outputs = [f(x[:, i].unsqueeze(1)) for i, f in enumerate(self.inner_functions)]\n",
    "        inner_concat = torch.cat(inner_outputs, dim=1)\n",
    "        return self.outer_function(inner_concat)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    inner_dim = trial.suggest_int('inner_dim', 8, 64)\n",
    "    outer_dim = trial.suggest_int('outer_dim', 8, 64)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the KAN model\n",
    "    model = KAN(X_train.shape[1], inner_dim, outer_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final KAN model with the best hyperparameters\n",
    "best_model = KAN(X_train.shape[1], best_params['inner_dim'], best_params['outer_dim']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['KAN'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:50:32,296] A new study created in memory with name: no-name-6270cf44-c4c4-4bea-a86c-6125d3d0accf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:37,669] Trial 0 finished with value: 89.4405746459961 and parameters: {'num_heads': 6, 'embed_dim': 18, 'num_layers': 3, 'dropout': 0.3546954209666189, 'learning_rate': 0.0029695035123851275, 'batch_size': 64, 'num_epochs': 38}. Best is trial 0 with value: 89.4405746459961.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:39,868] Trial 1 finished with value: 108.74946594238281 and parameters: {'num_heads': 7, 'embed_dim': 147, 'num_layers': 2, 'dropout': 0.39053803731951175, 'learning_rate': 0.013605052614586608, 'batch_size': 256, 'num_epochs': 46}. Best is trial 0 with value: 89.4405746459961.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:43,417] Trial 2 finished with value: 88.06293487548828 and parameters: {'num_heads': 8, 'embed_dim': 136, 'num_layers': 4, 'dropout': 0.4101466384136102, 'learning_rate': 0.039748764803192164, 'batch_size': 32, 'num_epochs': 27}. Best is trial 2 with value: 88.06293487548828.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:47,294] Trial 3 finished with value: 86.90370178222656 and parameters: {'num_heads': 8, 'embed_dim': 152, 'num_layers': 6, 'dropout': 0.12701148373087906, 'learning_rate': 0.0005016811999298732, 'batch_size': 32, 'num_epochs': 19}. Best is trial 3 with value: 86.90370178222656.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:48,337] Trial 4 finished with value: 86.89129638671875 and parameters: {'num_heads': 7, 'embed_dim': 84, 'num_layers': 3, 'dropout': 0.22532883101578705, 'learning_rate': 0.0014875147408064627, 'batch_size': 128, 'num_epochs': 24}. Best is trial 4 with value: 86.89129638671875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:52,371] Trial 5 finished with value: 84.66770935058594 and parameters: {'num_heads': 1, 'embed_dim': 41, 'num_layers': 5, 'dropout': 0.4159648873874836, 'learning_rate': 0.0017218233850280283, 'batch_size': 128, 'num_epochs': 94}. Best is trial 5 with value: 84.66770935058594.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:52,749] Trial 6 finished with value: 87.38859558105469 and parameters: {'num_heads': 1, 'embed_dim': 110, 'num_layers': 2, 'dropout': 0.46713096248579483, 'learning_rate': 0.0017639227976040549, 'batch_size': 32, 'num_epochs': 12}. Best is trial 5 with value: 84.66770935058594.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:50:59,145] Trial 7 finished with value: 88.5810546875 and parameters: {'num_heads': 4, 'embed_dim': 216, 'num_layers': 4, 'dropout': 0.13836128170767542, 'learning_rate': 0.033693683295215465, 'batch_size': 256, 'num_epochs': 78}. Best is trial 5 with value: 84.66770935058594.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:51:04,416] Trial 8 finished with value: 88.46607971191406 and parameters: {'num_heads': 8, 'embed_dim': 112, 'num_layers': 5, 'dropout': 0.04397087398239813, 'learning_rate': 0.06559863236402778, 'batch_size': 64, 'num_epochs': 76}. Best is trial 5 with value: 84.66770935058594.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 08:51:09,289] Trial 9 finished with value: 77.47196197509766 and parameters: {'num_heads': 5, 'embed_dim': 110, 'num_layers': 6, 'dropout': 0.30497831425086036, 'learning_rate': 0.002631373757669195, 'batch_size': 64, 'num_epochs': 53}. Best is trial 9 with value: 77.47196197509766.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[W 2024-07-26 08:51:10,100] Trial 10 failed with parameters: {'num_heads': 4, 'embed_dim': 256, 'num_layers': 6, 'dropout': 0.2802157885319575, 'learning_rate': 0.00013775493458181355, 'batch_size': 64, 'num_epochs': 62} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_260\\3223521253.py\", line 85, in objective\n",
      "    optimizer.step()\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 391, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py\", line 168, in step\n",
      "    adam(\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py\", line 318, in adam\n",
      "    func(params,\n",
      "  File \"c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py\", line 443, in _single_tensor_adam\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "[W 2024-07-26 08:51:10,105] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter tuning with Optuna\u001b[39;00m\n\u001b[0;32m     97\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# You can adjust the number of trials\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m    101\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[0;32m     63\u001b[0m             study,\n\u001b[0;32m     64\u001b[0m             func,\n\u001b[0;32m     65\u001b[0m             n_trials,\n\u001b[0;32m     66\u001b[0m             timeout,\n\u001b[0;32m     67\u001b[0m             catch,\n\u001b[0;32m     68\u001b[0m             callbacks,\n\u001b[0;32m     69\u001b[0m             gc_after_trial,\n\u001b[0;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     83\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     84\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 85\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:443\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(0)  # Add sequence dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(0)  # Remove sequence dimension\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embed_dim = trial.suggest_int('embed_dim', num_heads, 256, step=num_heads)  # Ensure divisibility\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 6)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the SAINT model\n",
    "    model = SAINT(X_train.shape[1], embed_dim, num_heads, num_layers, dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final SAINT model with the best hyperparameters\n",
    "best_model = SAINT(X_train.shape[1], \n",
    "                   best_params['embed_dim'], \n",
    "                   best_params['num_heads'], \n",
    "                   best_params['num_layers'], \n",
    "                   best_params['dropout']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['SAINT'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 08:36:20,514] A new study created in memory with name: no-name-42b9754c-a810-47f1-ad68-e644121d8d81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:21,003] Trial 0 finished with value: 109.2194595336914 and parameters: {'hidden_dim': 60, 'learning_rate': 0.0005548669955034724, 'batch_size': 64, 'num_epochs': 65, 'alpha': 4.828197004000649, 'beta': 4.270061549002121}. Best is trial 0 with value: 109.2194595336914.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:21,534] Trial 1 finished with value: 34.86105728149414 and parameters: {'hidden_dim': 254, 'learning_rate': 0.0016483977508461843, 'batch_size': 128, 'num_epochs': 93, 'alpha': 1.5974197870695082, 'beta': 0.27458449894138787}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:21,755] Trial 2 finished with value: 107.09423065185547 and parameters: {'hidden_dim': 109, 'learning_rate': 0.0010213960130444786, 'batch_size': 32, 'num_epochs': 46, 'alpha': 1.1328263343986797, 'beta': 0.18139505404052333}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:22,028] Trial 3 finished with value: 110.0946044921875 and parameters: {'hidden_dim': 87, 'learning_rate': 0.0002357879187549457, 'batch_size': 32, 'num_epochs': 47, 'alpha': 3.2443826482415985, 'beta': 0.25737598687259033}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:22,333] Trial 4 finished with value: 55.845916748046875 and parameters: {'hidden_dim': 53, 'learning_rate': 0.06517992980811906, 'batch_size': 128, 'num_epochs': 79, 'alpha': 0.518721708376411, 'beta': 6.728094410933878}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:22,727] Trial 5 finished with value: 104.10179138183594 and parameters: {'hidden_dim': 104, 'learning_rate': 0.0006700192605776028, 'batch_size': 32, 'num_epochs': 94, 'alpha': 0.16693138542202623, 'beta': 0.22942405784124517}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:22,789] Trial 6 finished with value: 110.29354095458984 and parameters: {'hidden_dim': 57, 'learning_rate': 0.001981814720000476, 'batch_size': 256, 'num_epochs': 11, 'alpha': 0.2857788223904932, 'beta': 0.1925220371969617}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:22,862] Trial 7 finished with value: 91.4979019165039 and parameters: {'hidden_dim': 120, 'learning_rate': 0.01372952959840712, 'batch_size': 64, 'num_epochs': 13, 'alpha': 0.22409481244282312, 'beta': 4.288003145348896}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:23,284] Trial 8 finished with value: 100.56537628173828 and parameters: {'hidden_dim': 156, 'learning_rate': 0.0009995721759622925, 'batch_size': 128, 'num_epochs': 80, 'alpha': 0.32261526414957337, 'beta': 0.11764278951667057}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:23,823] Trial 9 finished with value: 104.92611694335938 and parameters: {'hidden_dim': 251, 'learning_rate': 0.00020123438978759432, 'batch_size': 64, 'num_epochs': 97, 'alpha': 6.581098719702378, 'beta': 7.4936161781360395}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:24,081] Trial 10 finished with value: 35.47970962524414 and parameters: {'hidden_dim': 254, 'learning_rate': 0.010804123387615706, 'batch_size': 128, 'num_epochs': 34, 'alpha': 1.7252234426120059, 'beta': 0.8855458308364426}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:24,295] Trial 11 finished with value: 41.264522552490234 and parameters: {'hidden_dim': 250, 'learning_rate': 0.0065975884004466, 'batch_size': 128, 'num_epochs': 31, 'alpha': 1.6633556332969746, 'beta': 0.7605056375545538}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:24,527] Trial 12 finished with value: 90.12202453613281 and parameters: {'hidden_dim': 197, 'learning_rate': 0.02006414785298622, 'batch_size': 128, 'num_epochs': 34, 'alpha': 2.0490773484221405, 'beta': 0.7488712260694931}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:24,898] Trial 13 finished with value: 99.74820709228516 and parameters: {'hidden_dim': 204, 'learning_rate': 0.004202633909744782, 'batch_size': 128, 'num_epochs': 63, 'alpha': 0.8256621749350803, 'beta': 1.575059661603974}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:25,093] Trial 14 finished with value: 38.508304595947266 and parameters: {'hidden_dim': 218, 'learning_rate': 0.047399175147576224, 'batch_size': 256, 'num_epochs': 27, 'alpha': 2.744460224552796, 'beta': 0.511106457602936}. Best is trial 1 with value: 34.86105728149414.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:25,557] Trial 15 finished with value: 30.488788604736328 and parameters: {'hidden_dim': 165, 'learning_rate': 0.008978259034329273, 'batch_size': 128, 'num_epochs': 79, 'alpha': 0.8138558493426183, 'beta': 0.4199950597272541}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:26,106] Trial 16 finished with value: 43.01782989501953 and parameters: {'hidden_dim': 157, 'learning_rate': 0.0021583499877732585, 'batch_size': 128, 'num_epochs': 85, 'alpha': 0.6010129304943777, 'beta': 0.3981010837594335}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:26,522] Trial 17 finished with value: 40.74557876586914 and parameters: {'hidden_dim': 190, 'learning_rate': 0.031426711915213804, 'batch_size': 128, 'num_epochs': 69, 'alpha': 0.9355100767487541, 'beta': 1.590028020997087}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:27,066] Trial 18 finished with value: 110.02849578857422 and parameters: {'hidden_dim': 171, 'learning_rate': 0.00010498544455177771, 'batch_size': 256, 'num_epochs': 100, 'alpha': 0.500238248867939, 'beta': 0.4153777669173217}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:27,584] Trial 19 finished with value: 41.44753646850586 and parameters: {'hidden_dim': 230, 'learning_rate': 0.005546885303368442, 'batch_size': 128, 'num_epochs': 85, 'alpha': 9.80222855225035, 'beta': 1.451483417474294}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:28,037] Trial 20 finished with value: 45.07960891723633 and parameters: {'hidden_dim': 130, 'learning_rate': 0.0024168080286159866, 'batch_size': 128, 'num_epochs': 73, 'alpha': 0.13485117339472286, 'beta': 0.10546396423088213}. Best is trial 15 with value: 30.488788604736328.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:28,399] Trial 21 finished with value: 26.333717346191406 and parameters: {'hidden_dim': 232, 'learning_rate': 0.015829973872465472, 'batch_size': 128, 'num_epochs': 54, 'alpha': 1.4470621310120337, 'beta': 1.0921248426592012}. Best is trial 21 with value: 26.333717346191406.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:28,751] Trial 22 finished with value: 24.931629180908203 and parameters: {'hidden_dim': 228, 'learning_rate': 0.012413492732038789, 'batch_size': 128, 'num_epochs': 55, 'alpha': 1.2113826761359088, 'beta': 2.455177837276202}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:29,072] Trial 23 finished with value: 41.96944046020508 and parameters: {'hidden_dim': 227, 'learning_rate': 0.009537530400427832, 'batch_size': 128, 'num_epochs': 50, 'alpha': 1.2341699306243898, 'beta': 2.5749269000234616}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:29,438] Trial 24 finished with value: 40.35520553588867 and parameters: {'hidden_dim': 180, 'learning_rate': 0.02029450341735607, 'batch_size': 128, 'num_epochs': 59, 'alpha': 0.7013061887112022, 'beta': 2.6080226886500455}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:29,777] Trial 25 finished with value: 56.84671401977539 and parameters: {'hidden_dim': 214, 'learning_rate': 0.0969259025854142, 'batch_size': 128, 'num_epochs': 54, 'alpha': 2.923543412097102, 'beta': 1.178874739250429}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:30,049] Trial 26 finished with value: 44.5126953125 and parameters: {'hidden_dim': 237, 'learning_rate': 0.02763268493343423, 'batch_size': 32, 'num_epochs': 43, 'alpha': 0.36759619000241284, 'beta': 2.3217621654600937}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:30,391] Trial 27 finished with value: 51.10199737548828 and parameters: {'hidden_dim': 147, 'learning_rate': 0.007207834140405876, 'batch_size': 256, 'num_epochs': 57, 'alpha': 1.1631040498320373, 'beta': 0.5890891951927599}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:30,692] Trial 28 finished with value: 58.43772506713867 and parameters: {'hidden_dim': 170, 'learning_rate': 0.003959282515762748, 'batch_size': 64, 'num_epochs': 41, 'alpha': 2.2428122386684564, 'beta': 3.3617828418193416}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:31,018] Trial 29 finished with value: 41.90913391113281 and parameters: {'hidden_dim': 77, 'learning_rate': 0.014999482434994743, 'batch_size': 64, 'num_epochs': 65, 'alpha': 4.5314011639637055, 'beta': 5.0195416476128365}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:31,438] Trial 30 finished with value: 42.67442321777344 and parameters: {'hidden_dim': 211, 'learning_rate': 0.036791640613405174, 'batch_size': 128, 'num_epochs': 73, 'alpha': 4.3153795745116845, 'beta': 1.1002867855000997}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:31,963] Trial 31 finished with value: 42.35325622558594 and parameters: {'hidden_dim': 238, 'learning_rate': 0.0013845461227555947, 'batch_size': 128, 'num_epochs': 89, 'alpha': 1.4953933259625438, 'beta': 0.34948421099749477}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:32,470] Trial 32 finished with value: 35.42828369140625 and parameters: {'hidden_dim': 240, 'learning_rate': 0.0030629707136576264, 'batch_size': 128, 'num_epochs': 91, 'alpha': 0.8743290608622675, 'beta': 0.303009228682624}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:32,782] Trial 33 finished with value: 46.15351867675781 and parameters: {'hidden_dim': 222, 'learning_rate': 0.00904313979232268, 'batch_size': 128, 'num_epochs': 52, 'alpha': 1.4246079438349186, 'beta': 0.15387619393838683}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:33,595] Trial 34 finished with value: 108.4913558959961 and parameters: {'hidden_dim': 189, 'learning_rate': 0.0004011892189489918, 'batch_size': 32, 'num_epochs': 77, 'alpha': 0.4519049558664427, 'beta': 1.9876314662362577}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:34,355] Trial 35 finished with value: 45.3718376159668 and parameters: {'hidden_dim': 207, 'learning_rate': 0.01868451423450759, 'batch_size': 128, 'num_epochs': 61, 'alpha': 0.6955275967553061, 'beta': 0.5356618499771726}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:34,806] Trial 36 finished with value: 66.56250762939453 and parameters: {'hidden_dim': 241, 'learning_rate': 0.0011975437187076777, 'batch_size': 32, 'num_epochs': 70, 'alpha': 2.145838774236154, 'beta': 0.23933193293801264}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:35,137] Trial 37 finished with value: 43.858665466308594 and parameters: {'hidden_dim': 256, 'learning_rate': 0.004131190370235696, 'batch_size': 128, 'num_epochs': 47, 'alpha': 1.2324033316327285, 'beta': 0.15833435615788888}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:35,599] Trial 38 finished with value: 105.88150024414062 and parameters: {'hidden_dim': 108, 'learning_rate': 0.000580551589013333, 'batch_size': 128, 'num_epochs': 81, 'alpha': 3.557755781657254, 'beta': 0.705411294271578}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:36,201] Trial 39 finished with value: 66.22606658935547 and parameters: {'hidden_dim': 130, 'learning_rate': 0.0015817504594671597, 'batch_size': 256, 'num_epochs': 93, 'alpha': 1.0138986631787787, 'beta': 0.26399226787706714}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:36,336] Trial 40 finished with value: 110.31913757324219 and parameters: {'hidden_dim': 72, 'learning_rate': 0.0008783965689799034, 'batch_size': 64, 'num_epochs': 20, 'alpha': 0.2209061190498297, 'beta': 3.5577108039111813}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:36,878] Trial 41 finished with value: 79.96875 and parameters: {'hidden_dim': 242, 'learning_rate': 0.003330956713934826, 'batch_size': 128, 'num_epochs': 92, 'alpha': 0.8801099003802375, 'beta': 0.33051048841721936}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:37,407] Trial 42 finished with value: 45.930213928222656 and parameters: {'hidden_dim': 231, 'learning_rate': 0.0026468424118014453, 'batch_size': 128, 'num_epochs': 89, 'alpha': 0.7385100815207968, 'beta': 0.28763615870914927}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:37,960] Trial 43 finished with value: 39.623817443847656 and parameters: {'hidden_dim': 245, 'learning_rate': 0.005769898295186414, 'batch_size': 128, 'num_epochs': 98, 'alpha': 1.80996492517643, 'beta': 0.19610262247143975}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:38,384] Trial 44 finished with value: 36.69284439086914 and parameters: {'hidden_dim': 94, 'learning_rate': 0.01170771029447579, 'batch_size': 128, 'num_epochs': 84, 'alpha': 1.0559777302089826, 'beta': 0.4269654301582726}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:38,786] Trial 45 finished with value: 48.868072509765625 and parameters: {'hidden_dim': 224, 'learning_rate': 0.007938250039684605, 'batch_size': 128, 'num_epochs': 65, 'alpha': 0.606192486440215, 'beta': 9.401119035242235}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:39,033] Trial 46 finished with value: 100.09027862548828 and parameters: {'hidden_dim': 204, 'learning_rate': 0.0017584980105030384, 'batch_size': 128, 'num_epochs': 39, 'alpha': 1.4630976090301955, 'beta': 0.9927654009117424}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:39,354] Trial 47 finished with value: 39.94120407104492 and parameters: {'hidden_dim': 37, 'learning_rate': 0.013304509050272103, 'batch_size': 32, 'num_epochs': 95, 'alpha': 0.4065776525240965, 'beta': 0.13446767403022494}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:39,853] Trial 48 finished with value: 99.10847473144531 and parameters: {'hidden_dim': 256, 'learning_rate': 0.0051313514093888745, 'batch_size': 128, 'num_epochs': 89, 'alpha': 2.5115827519453764, 'beta': 0.19312464984336347}. Best is trial 22 with value: 24.931629180908203.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:69: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:72: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_6972\\884279414.py:73: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
      "[I 2024-07-26 08:36:40,275] Trial 49 finished with value: 32.96991729736328 and parameters: {'hidden_dim': 192, 'learning_rate': 0.024559585099291223, 'batch_size': 128, 'num_epochs': 77, 'alpha': 1.7787140169671771, 'beta': 0.6360687233770314}. Best is trial 22 with value: 24.931629180908203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         RMSE R-squared Training Time Inference Time  \\\n",
      "Linear Regression    0.211628  0.999992      0.001003            0.0   \n",
      "Ridge Regression     0.905454  0.999858      0.000999            0.0   \n",
      "Lasso Regression     0.383952  0.999974           0.0            0.0   \n",
      "ElasticNet           4.404609  0.996642      0.001995            0.0   \n",
      "Decision Tree       36.807797    0.7655      0.000998            0.0   \n",
      "Random Forest       37.345571  0.758598      0.391953       0.023935   \n",
      "Gradient Boosting   32.551013  0.816603      0.060839       0.000996   \n",
      "XGBoost             47.556448  0.608545      0.066822       0.000997   \n",
      "LightGBM           105.104923 -0.912095      0.005985       0.000997   \n",
      "CatBoost             61.82527    0.3384       0.16257       0.001988   \n",
      "MLP                  7.094997  0.991287      0.594412       0.000998   \n",
      "DNN                109.746297 -1.084697      0.063799            0.0   \n",
      "DCN                   4.72361  0.996138      0.418879            0.0   \n",
      "Wide & Deep          1.155051  0.999769       0.31017       0.000997   \n",
      "XGBoost + NN        27.221821  0.871738      0.215427       0.000995   \n",
      "LightGBM + NN      101.505875 -0.783387      0.035905            0.0   \n",
      "CatBoost + NN        35.45834  0.782379      0.506645       0.000997   \n",
      "FT Transformer      43.537186  0.671917      1.026257       0.000997   \n",
      "AutoInt             11.983161  0.975145      0.570478       0.000996   \n",
      "NAS                 16.521265  0.952756       0.41489            0.0   \n",
      "KAN                  2.629192  0.998803      0.262269       0.000997   \n",
      "VIME                43.168236  0.677454      0.275263       0.000997   \n",
      "\n",
      "                  Computation Time  \\\n",
      "Linear Regression         0.055115   \n",
      "Ridge Regression          0.050876   \n",
      "Lasso Regression          0.048733   \n",
      "ElasticNet                0.120685   \n",
      "Decision Tree             0.049859   \n",
      "Random Forest             6.074769   \n",
      "Gradient Boosting         1.556828   \n",
      "XGBoost                    0.67719   \n",
      "LightGBM                  0.290224   \n",
      "CatBoost                  2.900246   \n",
      "MLP                      41.936884   \n",
      "DNN                       8.531194   \n",
      "DCN                      25.516252   \n",
      "Wide & Deep              16.102487   \n",
      "XGBoost + NN             22.702605   \n",
      "LightGBM + NN            18.373859   \n",
      "CatBoost + NN            81.689478   \n",
      "FT Transformer           98.291804   \n",
      "AutoInt                  34.191591   \n",
      "NAS                      18.283068   \n",
      "KAN                      17.044498   \n",
      "VIME                     20.055836   \n",
      "\n",
      "                                                     Best Parameters  \n",
      "Linear Regression                                                 {}  \n",
      "Ridge Regression                                      {'alpha': 0.1}  \n",
      "Lasso Regression                                      {'alpha': 0.1}  \n",
      "ElasticNet                           {'alpha': 0.1, 'l1_ratio': 0.8}  \n",
      "Decision Tree                                       {'max_depth': 5}  \n",
      "Random Forest                  {'max_depth': 7, 'n_estimators': 200}  \n",
      "Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 100}  \n",
      "XGBoost                  {'learning_rate': 0.1, 'n_estimators': 200}  \n",
      "LightGBM                 {'learning_rate': 0.01, 'n_estimators': 50}  \n",
      "CatBoost                   {'iterations': 200, 'learning_rate': 0.1}  \n",
      "MLP                {'activation': 'relu', 'alpha': 0.001, 'hidden...  \n",
      "DNN                {'batch_size': 32, 'hidden_dims': [50], 'learn...  \n",
      "DCN                {'cross_layers': 2, 'deep_layer_0': 148, 'deep...  \n",
      "Wide & Deep        {'wide_dim': 113, 'n_deep_layers': 3, 'deep_di...  \n",
      "XGBoost + NN       {'xgb_n_estimators': 55, 'xgb_max_depth': 9, '...  \n",
      "LightGBM + NN      {'lgb_n_estimators': 57, 'lgb_max_depth': 10, ...  \n",
      "CatBoost + NN      {'cat_iterations': 511, 'cat_depth': 7, 'cat_l...  \n",
      "FT Transformer     {'num_tokens': 236, 'heads': 6, 'dim': 120, 'd...  \n",
      "AutoInt            {'num_heads': 6, 'embedding_dim': 252, 'num_la...  \n",
      "NAS                {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1'...  \n",
      "KAN                {'inner_dim': 60, 'outer_dim': 46, 'learning_r...  \n",
      "VIME               {'hidden_dim': 228, 'learning_rate': 0.0124134...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim: 228\n",
      "learning_rate: 0.012413492732038789\n",
      "batch_size: 128\n",
      "num_epochs: 55\n",
      "alpha: 1.2113826761359088\n",
      "beta: 2.455177837276202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is your initial DataFrame\n",
    "# Split data\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1).to(device)\n",
    "\n",
    "class VIME(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(VIME, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mask_predictor = nn.Linear(hidden_dim, input_dim)\n",
    "        self.feature_predictor = nn.Linear(hidden_dim, input_dim)\n",
    "        self.predictor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.bernoulli(torch.ones_like(x) * 0.8)\n",
    "        x_masked = x * mask\n",
    "        h = self.encoder(x_masked)\n",
    "        mask_pred = torch.sigmoid(self.mask_predictor(h))\n",
    "        feature_pred = self.feature_predictor(h)\n",
    "        y_pred = self.predictor(h)\n",
    "        return y_pred, mask_pred, feature_pred\n",
    "\n",
    "def vime_loss(y_true, y_pred, mask_true, mask_pred, feature_true, feature_pred, alpha=1.0, beta=1.0):\n",
    "    prediction_loss = nn.MSELoss()(y_pred, y_true)\n",
    "    mask_loss = nn.BCELoss()(mask_pred, mask_true)\n",
    "    feature_loss = nn.MSELoss()(feature_pred, feature_true)\n",
    "    return prediction_loss + alpha * mask_loss + beta * feature_loss\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 32, 256)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "    alpha = trial.suggest_loguniform('alpha', 0.1, 10.0)\n",
    "    beta = trial.suggest_loguniform('beta', 0.1, 10.0)\n",
    "\n",
    "    # Create the VIME model\n",
    "    model = VIME(X_train.shape[1], hidden_dim).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            mask = torch.bernoulli(torch.ones_like(batch_X) * 0.8)\n",
    "            y_pred, mask_pred, feature_pred = model(batch_X, mask)\n",
    "            loss = vime_loss(batch_y, y_pred, mask, mask_pred, batch_X, feature_pred, alpha, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred, _, _ = model(X_test_tensor)\n",
    "        mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final VIME model with the best hyperparameters\n",
    "best_model = VIME(X_train.shape[1], best_params['hidden_dim']).to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        mask = torch.bernoulli(torch.ones_like(batch_X) * 0.8)\n",
    "        y_pred, mask_pred, feature_pred = best_model(batch_X, mask)\n",
    "        loss = vime_loss(batch_y, y_pred, mask, mask_pred, batch_X, feature_pred, best_params['alpha'], best_params['beta'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    y_pred, _, _ = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['VIME'] = [rmse, r2, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
