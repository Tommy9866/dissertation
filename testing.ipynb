{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Y\n",
      "0    0.821209  -0.249454   1.697793  -1.175892   0.064107  2\n",
      "1   -2.119163  -0.375135  -1.737217  -1.923423  -0.123764  0\n",
      "2    0.131719  -1.151343   1.912180  -0.953912   0.132853  2\n",
      "3   -1.847853   1.828564  -1.739899   0.305378   0.821996  1\n",
      "4   -1.410634  -1.016693  -0.528842  -1.621271   0.010901  0\n",
      "5    1.654569  -1.862979   1.088969  -1.594433  -1.249609  2\n",
      "6   -1.576386  -0.779648  -1.571243   0.570999   0.315930  1\n",
      "7   -1.394641   0.872065  -1.586451   0.404680   1.675884  1\n",
      "8   -0.836367  -0.753035   0.692772  -3.515997  -1.324955  2\n",
      "9   -0.206201   0.731854  -0.506389   1.166799  -0.879734  1\n",
      "10  -1.090315  -0.440489  -0.858923   1.146621   1.331481  1\n",
      "11  -0.363141  -0.782933  -0.193642  -0.583964   1.762350  0\n",
      "12  -0.671268  -0.183820  -1.067632   0.890163   0.417564  1\n",
      "13  -0.654774  -0.371809  -0.952730  -0.801694  -0.409717  0\n",
      "14   2.057538  -0.469427   1.393917   0.439180  -0.236643  2\n",
      "15   0.406995  -1.343292  -1.560592   0.664346  -0.253849  0\n",
      "16  -1.221993  -1.123622  -0.264153  -1.434934  -1.442184  0\n",
      "17  -1.748125   0.721678  -0.990665   1.056137  -0.779286  1\n",
      "18  -0.172296   0.951517  -1.961191   0.049750   0.411611  0\n",
      "19   1.386634   0.063169   0.110517  -2.775118   0.662665  2\n",
      "20  -2.110191   0.604303  -2.775689  -1.462076   0.689652  0\n",
      "21  -1.073897   0.173168  -0.832708   1.196660   1.368321  1\n",
      "22   0.147849   0.369789  -0.597347   1.257165  -0.399021  1\n",
      "23  -0.817644   0.030955   0.385469  -1.214479  -0.061252  0\n",
      "24  -2.363669  -0.453114  -2.780846  -0.491164  -0.552681  1\n",
      "25   1.090194  -0.539472   1.339898  -0.967071  -0.596968  2\n",
      "26  -1.744575   0.295481   0.016315  -1.892052  -0.265813  0\n",
      "27   0.882834  -0.583897   1.631098  -1.115529   0.273762  2\n",
      "28   1.074751   0.101809   0.435536  -1.879055  -0.265064  2\n",
      "29  -1.374816  -0.873424  -1.654124   0.556464   0.784465  1\n",
      "30  -1.184661  -0.546774  -2.623031  -0.656097  -1.445241  0\n",
      "31   0.133826  -0.685389  -0.557055  -0.121560   0.479588  0\n",
      "32   0.166516   0.007179  -0.280118   1.348369   0.847730  1\n",
      "33   0.973582   0.898626   0.982722  -1.117958  -1.648785  2\n",
      "34  -1.421413   0.308421  -1.501647   0.291967   0.091347  1\n",
      "35   0.505172   0.527751   0.705457  -0.131658  -0.207345  2\n",
      "36   0.696975  -0.106235   0.270059  -1.215861  -1.432638  2\n",
      "37  -1.332312   0.072884  -4.481069  -0.066588   1.472432  0\n",
      "38  -1.186781   0.557808  -0.609765  -1.193924  -0.150583  0\n",
      "39  -0.728826   0.498852   2.552225   0.309026  -1.379981  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=40, n_features=5, n_informative=3, n_redundant=0, n_clusters_per_class=1, n_classes=3)\n",
    "\n",
    "# Convert the data to a DataFrame for easier handling\n",
    "df_features = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(5)])\n",
    "df_target = pd.DataFrame(y, columns=['Y'])\n",
    "\n",
    "# Concatenate features and target into one DataFrame\n",
    "df = pd.concat([df_features, df_target], axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Y_column = df['Y'].copy()\n",
    "df.drop('Y', axis=1, inplace=True)\n",
    "\n",
    "# Identify categorical data (change this based on your actual data)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Standardize only the continuous (non-categorical) columns\n",
    "continuous_cols = df.columns.difference(categorical_cols)  # Gets the difference, i.e., continuous cols\n",
    "df[continuous_cols] = (df[continuous_cols] - df[continuous_cols].mean()) / df[continuous_cols].std()\n",
    "\n",
    "# Filter out outliers in continuous data (|z-score| > 5)\n",
    "mask = (np.abs(df[continuous_cols]) < 5).all(axis=1)\n",
    "df = df[mask]\n",
    "\n",
    "# Reattach the target variable 'Y' to the DataFrame\n",
    "df['Y'] = Y_column[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for column in categorical_cols:\n",
    "    # Ensure the column is of type object (string) or category\n",
    "    if df[column].dtype == 'object' or df[column].dtype.name == 'category':\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        \n",
    "df['Y'], unique = pd.factorize(df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:45:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.268511\n",
      "[LightGBM] [Info] Start training from score -0.980829\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.268511\n",
      "[LightGBM] [Info] Start training from score -0.980829\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary of models and their reduced hyperparameter grids\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000), {\n",
    "        'C': [0.01, 0.1, 1],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 4]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 4]\n",
    "    }),\n",
    "    'LightGBM': (LGBMClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 50]\n",
    "    }),\n",
    "    'CatBoost': (CatBoostClassifier(verbose=0), {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'depth': [4, 6]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Measure training time for best parameters\n",
    "    best_param_train_start = time.time()\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    best_param_train_time = time.time() - best_param_train_start\n",
    "\n",
    "    # Measure inference time for best parameters\n",
    "    inference_start_time = time.time()\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "    \n",
    "    # Calculate total computation time\n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    if len(np.unique(y)) == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "    else:  # Multiclass classification\n",
    "        auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr', average='macro')\n",
    "\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'AUC Score': auc,\n",
    "        'Training Time (Best Params)': best_param_train_time,\n",
    "        'Inference Time (Best Params)': inference_time,\n",
    "        'Computation Time (Total)': computation_time,\n",
    "        'Best Parameters': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "result = pd.DataFrame(results).T\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "activation: relu\n",
      "alpha: 0.0001\n",
      "hidden_layer_sizes: (100, 50)\n",
      "learning_rate: constant\n",
      "solver: sgd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the MLP model and its hyperparameter grid\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with StratifiedKFold\n",
    "start_time = time.time()\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(mlp, param_grid=param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Measure training time for best parameters\n",
    "best_param_train_start = time.time()\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - best_param_train_start\n",
    "\n",
    "# Measure inference time for best parameters\n",
    "inference_start_time = time.time()\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled)[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, best_model.predict_proba(X_test_scaled), multi_class='ovr', average='macro')\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['MLP'] = [accuracy, auc, training_time, inference_time, computation_time, grid_search.best_params_]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:46:16,958] A new study created in memory with name: no-name-7560ba0e-7c0e-41f7-a212-760c6068857a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:17,319] Trial 0 finished with value: 0.875 and parameters: {'hidden_dim_0': 188, 'hidden_dim_1': 168, 'hidden_dim_2': 143, 'learning_rate': 0.0025503930381602113, 'batch_size': 64, 'num_epochs': 65}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:17,609] Trial 1 finished with value: 0.875 and parameters: {'hidden_dim_0': 211, 'hidden_dim_1': 167, 'hidden_dim_2': 102, 'learning_rate': 0.0047179236236254045, 'batch_size': 64, 'num_epochs': 60}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:17,689] Trial 2 finished with value: 0.75 and parameters: {'hidden_dim_0': 238, 'hidden_dim_1': 213, 'hidden_dim_2': 132, 'learning_rate': 0.001214410015281299, 'batch_size': 256, 'num_epochs': 11}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:17,792] Trial 3 finished with value: 0.5 and parameters: {'hidden_dim_0': 147, 'hidden_dim_1': 41, 'hidden_dim_2': 143, 'learning_rate': 0.0001411001758517163, 'batch_size': 256, 'num_epochs': 26}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:18,111] Trial 4 finished with value: 0.875 and parameters: {'hidden_dim_0': 103, 'hidden_dim_1': 89, 'hidden_dim_2': 154, 'learning_rate': 0.01119825948728646, 'batch_size': 32, 'num_epochs': 66}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:18,169] Trial 5 finished with value: 0.625 and parameters: {'hidden_dim_0': 129, 'hidden_dim_1': 70, 'hidden_dim_2': 167, 'learning_rate': 0.09459623672709794, 'batch_size': 256, 'num_epochs': 12}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:18,311] Trial 6 finished with value: 0.75 and parameters: {'hidden_dim_0': 56, 'hidden_dim_1': 196, 'hidden_dim_2': 40, 'learning_rate': 0.00040457722632986136, 'batch_size': 128, 'num_epochs': 34}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:18,675] Trial 7 finished with value: 0.75 and parameters: {'hidden_dim_0': 143, 'hidden_dim_1': 245, 'hidden_dim_2': 202, 'learning_rate': 0.01295261721878711, 'batch_size': 64, 'num_epochs': 63}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:19,152] Trial 8 finished with value: 0.75 and parameters: {'hidden_dim_0': 244, 'hidden_dim_1': 157, 'hidden_dim_2': 94, 'learning_rate': 0.0005774159676143936, 'batch_size': 32, 'num_epochs': 94}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:19,286] Trial 9 finished with value: 0.75 and parameters: {'hidden_dim_0': 78, 'hidden_dim_1': 154, 'hidden_dim_2': 42, 'learning_rate': 0.010993549510365155, 'batch_size': 32, 'num_epochs': 28}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:19,776] Trial 10 finished with value: 0.875 and parameters: {'hidden_dim_0': 189, 'hidden_dim_1': 114, 'hidden_dim_2': 249, 'learning_rate': 0.07846905962869255, 'batch_size': 64, 'num_epochs': 88}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:20,069] Trial 11 finished with value: 0.875 and parameters: {'hidden_dim_0': 199, 'hidden_dim_1': 187, 'hidden_dim_2': 95, 'learning_rate': 0.0027499031853174547, 'batch_size': 64, 'num_epochs': 47}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:20,497] Trial 12 finished with value: 0.75 and parameters: {'hidden_dim_0': 192, 'hidden_dim_1': 127, 'hidden_dim_2': 90, 'learning_rate': 0.0036306208631671625, 'batch_size': 64, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:20,842] Trial 13 finished with value: 0.75 and parameters: {'hidden_dim_0': 217, 'hidden_dim_1': 174, 'hidden_dim_2': 185, 'learning_rate': 0.00428082455841582, 'batch_size': 64, 'num_epochs': 50}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:21,314] Trial 14 finished with value: 0.625 and parameters: {'hidden_dim_0': 169, 'hidden_dim_1': 233, 'hidden_dim_2': 121, 'learning_rate': 0.03222348231024843, 'batch_size': 128, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:21,700] Trial 15 finished with value: 0.75 and parameters: {'hidden_dim_0': 225, 'hidden_dim_1': 132, 'hidden_dim_2': 212, 'learning_rate': 0.0019635035128021004, 'batch_size': 64, 'num_epochs': 60}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:21,972] Trial 16 finished with value: 0.75 and parameters: {'hidden_dim_0': 168, 'hidden_dim_1': 209, 'hidden_dim_2': 69, 'learning_rate': 0.0009736731368931642, 'batch_size': 64, 'num_epochs': 43}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:22,370] Trial 17 finished with value: 0.75 and parameters: {'hidden_dim_0': 254, 'hidden_dim_1': 104, 'hidden_dim_2': 112, 'learning_rate': 0.0062974036453582564, 'batch_size': 64, 'num_epochs': 74}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:22,863] Trial 18 finished with value: 0.75 and parameters: {'hidden_dim_0': 211, 'hidden_dim_1': 164, 'hidden_dim_2': 70, 'learning_rate': 0.02597185355860373, 'batch_size': 128, 'num_epochs': 85}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:23,466] Trial 19 finished with value: 0.75 and parameters: {'hidden_dim_0': 170, 'hidden_dim_1': 141, 'hidden_dim_2': 169, 'learning_rate': 0.00024075132917949682, 'batch_size': 64, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:23,918] Trial 20 finished with value: 0.875 and parameters: {'hidden_dim_0': 126, 'hidden_dim_1': 227, 'hidden_dim_2': 243, 'learning_rate': 0.001524813670714186, 'batch_size': 64, 'num_epochs': 69}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:24,227] Trial 21 finished with value: 0.875 and parameters: {'hidden_dim_0': 105, 'hidden_dim_1': 87, 'hidden_dim_2': 153, 'learning_rate': 0.009313680004962714, 'batch_size': 32, 'num_epochs': 55}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:24,519] Trial 22 finished with value: 0.875 and parameters: {'hidden_dim_0': 93, 'hidden_dim_1': 57, 'hidden_dim_2': 148, 'learning_rate': 0.005470492240522751, 'batch_size': 32, 'num_epochs': 65}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:24,817] Trial 23 finished with value: 1.0 and parameters: {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidden_dim_2': 117, 'learning_rate': 0.020371689757134166, 'batch_size': 32, 'num_epochs': 55}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:25,154] Trial 24 finished with value: 0.75 and parameters: {'hidden_dim_0': 48, 'hidden_dim_1': 186, 'hidden_dim_2': 113, 'learning_rate': 0.021715875353744765, 'batch_size': 32, 'num_epochs': 54}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:25,388] Trial 25 finished with value: 0.75 and parameters: {'hidden_dim_0': 34, 'hidden_dim_1': 115, 'hidden_dim_2': 71, 'learning_rate': 0.039572686810302196, 'batch_size': 32, 'num_epochs': 44}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:25,662] Trial 26 finished with value: 0.75 and parameters: {'hidden_dim_0': 180, 'hidden_dim_1': 144, 'hidden_dim_2': 127, 'learning_rate': 0.002554089663803773, 'batch_size': 256, 'num_epochs': 40}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:26,283] Trial 27 finished with value: 0.875 and parameters: {'hidden_dim_0': 152, 'hidden_dim_1': 173, 'hidden_dim_2': 86, 'learning_rate': 0.016762698303183256, 'batch_size': 128, 'num_epochs': 59}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:30,618] Trial 28 finished with value: 0.625 and parameters: {'hidden_dim_0': 227, 'hidden_dim_1': 92, 'hidden_dim_2': 106, 'learning_rate': 0.051404072087392796, 'batch_size': 64, 'num_epochs': 72}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:31,698] Trial 29 finished with value: 0.75 and parameters: {'hidden_dim_0': 204, 'hidden_dim_1': 207, 'hidden_dim_2': 126, 'learning_rate': 0.000939645901863735, 'batch_size': 256, 'num_epochs': 82}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:32,004] Trial 30 finished with value: 0.875 and parameters: {'hidden_dim_0': 76, 'hidden_dim_1': 71, 'hidden_dim_2': 138, 'learning_rate': 0.006308630243833289, 'batch_size': 32, 'num_epochs': 53}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:32,979] Trial 31 finished with value: 0.875 and parameters: {'hidden_dim_0': 110, 'hidden_dim_1': 33, 'hidden_dim_2': 162, 'learning_rate': 0.00792206116848665, 'batch_size': 32, 'num_epochs': 66}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:33,438] Trial 32 finished with value: 0.75 and parameters: {'hidden_dim_0': 66, 'hidden_dim_1': 86, 'hidden_dim_2': 187, 'learning_rate': 0.015412426378974995, 'batch_size': 32, 'num_epochs': 59}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:33,766] Trial 33 finished with value: 0.875 and parameters: {'hidden_dim_0': 32, 'hidden_dim_1': 52, 'hidden_dim_2': 138, 'learning_rate': 0.004056108774132999, 'batch_size': 32, 'num_epochs': 68}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:33,982] Trial 34 finished with value: 0.75 and parameters: {'hidden_dim_0': 132, 'hidden_dim_1': 74, 'hidden_dim_2': 153, 'learning_rate': 0.018681228248868538, 'batch_size': 256, 'num_epochs': 34}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:34,309] Trial 35 finished with value: 0.875 and parameters: {'hidden_dim_0': 96, 'hidden_dim_1': 126, 'hidden_dim_2': 176, 'learning_rate': 0.002108257295604268, 'batch_size': 32, 'num_epochs': 61}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:34,616] Trial 36 finished with value: 1.0 and parameters: {'hidden_dim_0': 156, 'hidden_dim_1': 105, 'hidden_dim_2': 132, 'learning_rate': 0.010226391753626751, 'batch_size': 64, 'num_epochs': 50}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:34,763] Trial 37 finished with value: 0.75 and parameters: {'hidden_dim_0': 157, 'hidden_dim_1': 112, 'hidden_dim_2': 107, 'learning_rate': 0.001279045172658697, 'batch_size': 64, 'num_epochs': 20}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:35,462] Trial 38 finished with value: 0.75 and parameters: {'hidden_dim_0': 138, 'hidden_dim_1': 154, 'hidden_dim_2': 53, 'learning_rate': 0.008996402702289961, 'batch_size': 64, 'num_epochs': 49}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:35,761] Trial 39 finished with value: 0.75 and parameters: {'hidden_dim_0': 182, 'hidden_dim_1': 194, 'hidden_dim_2': 134, 'learning_rate': 0.0005568259955365571, 'batch_size': 64, 'num_epochs': 39}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:36,050] Trial 40 finished with value: 0.625 and parameters: {'hidden_dim_0': 160, 'hidden_dim_1': 99, 'hidden_dim_2': 100, 'learning_rate': 0.058950916688937205, 'batch_size': 64, 'num_epochs': 34}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:36,390] Trial 41 finished with value: 0.875 and parameters: {'hidden_dim_0': 124, 'hidden_dim_1': 60, 'hidden_dim_2': 160, 'learning_rate': 0.011130245009161816, 'batch_size': 32, 'num_epochs': 56}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:36,828] Trial 42 finished with value: 0.875 and parameters: {'hidden_dim_0': 235, 'hidden_dim_1': 79, 'hidden_dim_2': 81, 'learning_rate': 0.01386409354078316, 'batch_size': 64, 'num_epochs': 65}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:37,134] Trial 43 finished with value: 1.0 and parameters: {'hidden_dim_0': 195, 'hidden_dim_1': 174, 'hidden_dim_2': 122, 'learning_rate': 0.005058299570813077, 'batch_size': 128, 'num_epochs': 51}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:37,464] Trial 44 finished with value: 0.75 and parameters: {'hidden_dim_0': 196, 'hidden_dim_1': 171, 'hidden_dim_2': 119, 'learning_rate': 0.0001053551247023995, 'batch_size': 128, 'num_epochs': 50}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:37,784] Trial 45 finished with value: 0.875 and parameters: {'hidden_dim_0': 211, 'hidden_dim_1': 186, 'hidden_dim_2': 141, 'learning_rate': 0.004987048445293745, 'batch_size': 128, 'num_epochs': 47}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:38,077] Trial 46 finished with value: 0.75 and parameters: {'hidden_dim_0': 183, 'hidden_dim_1': 156, 'hidden_dim_2': 123, 'learning_rate': 0.002982040048310103, 'batch_size': 128, 'num_epochs': 53}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:38,328] Trial 47 finished with value: 0.75 and parameters: {'hidden_dim_0': 146, 'hidden_dim_1': 140, 'hidden_dim_2': 103, 'learning_rate': 0.007141381829633715, 'batch_size': 128, 'num_epochs': 41}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:38,776] Trial 48 finished with value: 0.875 and parameters: {'hidden_dim_0': 202, 'hidden_dim_1': 179, 'hidden_dim_2': 114, 'learning_rate': 0.003835738479156414, 'batch_size': 64, 'num_epochs': 71}. Best is trial 23 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1406562829.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:38,971] Trial 49 finished with value: 0.75 and parameters: {'hidden_dim_0': 224, 'hidden_dim_1': 148, 'hidden_dim_2': 131, 'learning_rate': 0.0020042795618854525, 'batch_size': 256, 'num_epochs': 26}. Best is trial 23 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim_0: 41\n",
      "hidden_dim_1: 93\n",
      "hidden_dim_2: 117\n",
      "learning_rate: 0.020371689757134166\n",
      "batch_size: 32\n",
      "num_epochs: 55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_dims = [trial.suggest_int(f'hidden_dim_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = DNN(input_dim, hidden_dims, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = DNN(input_dim, [best_params[f'hidden_dim_{i}'] for i in range(3)], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DNN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:46:39,311] A new study created in memory with name: no-name-3900da39-de46-45da-b04d-8140173eabe5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:39,632] Trial 0 finished with value: 0.875 and parameters: {'cross_layers': 4, 'hidden_layer_0': 184, 'hidden_layer_1': 169, 'hidden_layer_2': 100, 'learning_rate': 0.0003024897105741653, 'batch_size': 128, 'num_epochs': 36}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:39,943] Trial 1 finished with value: 0.5 and parameters: {'cross_layers': 1, 'hidden_layer_0': 235, 'hidden_layer_1': 47, 'hidden_layer_2': 73, 'learning_rate': 0.08362348124405034, 'batch_size': 128, 'num_epochs': 58}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:40,342] Trial 2 finished with value: 0.625 and parameters: {'cross_layers': 4, 'hidden_layer_0': 57, 'hidden_layer_1': 141, 'hidden_layer_2': 243, 'learning_rate': 0.00010340647121060388, 'batch_size': 256, 'num_epochs': 46}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:40,842] Trial 3 finished with value: 0.875 and parameters: {'cross_layers': 5, 'hidden_layer_0': 38, 'hidden_layer_1': 231, 'hidden_layer_2': 241, 'learning_rate': 0.005892111448098555, 'batch_size': 256, 'num_epochs': 39}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:41,515] Trial 4 finished with value: 0.875 and parameters: {'cross_layers': 2, 'hidden_layer_0': 126, 'hidden_layer_1': 224, 'hidden_layer_2': 236, 'learning_rate': 0.004616795411593872, 'batch_size': 32, 'num_epochs': 86}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:41,884] Trial 5 finished with value: 0.625 and parameters: {'cross_layers': 4, 'hidden_layer_0': 60, 'hidden_layer_1': 127, 'hidden_layer_2': 212, 'learning_rate': 0.03122991683890232, 'batch_size': 128, 'num_epochs': 36}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:42,153] Trial 6 finished with value: 0.625 and parameters: {'cross_layers': 2, 'hidden_layer_0': 256, 'hidden_layer_1': 235, 'hidden_layer_2': 83, 'learning_rate': 0.00040552078697090905, 'batch_size': 256, 'num_epochs': 20}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:42,453] Trial 7 finished with value: 0.75 and parameters: {'cross_layers': 1, 'hidden_layer_0': 249, 'hidden_layer_1': 91, 'hidden_layer_2': 163, 'learning_rate': 0.0006227750526583782, 'batch_size': 128, 'num_epochs': 31}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:42,889] Trial 8 finished with value: 0.875 and parameters: {'cross_layers': 3, 'hidden_layer_0': 53, 'hidden_layer_1': 231, 'hidden_layer_2': 113, 'learning_rate': 0.012856103943972257, 'batch_size': 256, 'num_epochs': 39}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:43,550] Trial 9 finished with value: 0.75 and parameters: {'cross_layers': 5, 'hidden_layer_0': 175, 'hidden_layer_1': 252, 'hidden_layer_2': 251, 'learning_rate': 0.012778624388792174, 'batch_size': 32, 'num_epochs': 56}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:44,222] Trial 10 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 161, 'hidden_layer_1': 174, 'hidden_layer_2': 37, 'learning_rate': 0.0008707578092231305, 'batch_size': 64, 'num_epochs': 76}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:44,390] Trial 11 finished with value: 0.75 and parameters: {'cross_layers': 5, 'hidden_layer_0': 111, 'hidden_layer_1': 185, 'hidden_layer_2': 156, 'learning_rate': 0.002243563034650112, 'batch_size': 64, 'num_epochs': 10}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:45,074] Trial 12 finished with value: 0.5 and parameters: {'cross_layers': 5, 'hidden_layer_0': 198, 'hidden_layer_1': 197, 'hidden_layer_2': 184, 'learning_rate': 0.0001032125284235966, 'batch_size': 128, 'num_epochs': 71}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:45,908] Trial 13 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 100, 'hidden_layer_1': 166, 'hidden_layer_2': 117, 'learning_rate': 0.0029115227114030456, 'batch_size': 256, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:46,137] Trial 14 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 205, 'hidden_layer_1': 113, 'hidden_layer_2': 123, 'learning_rate': 0.00032473518263364864, 'batch_size': 128, 'num_epochs': 24}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:46,566] Trial 15 finished with value: 0.75 and parameters: {'cross_layers': 5, 'hidden_layer_0': 32, 'hidden_layer_1': 206, 'hidden_layer_2': 198, 'learning_rate': 0.0015111740873082944, 'batch_size': 256, 'num_epochs': 47}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:46,714] Trial 16 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 140, 'hidden_layer_1': 157, 'hidden_layer_2': 69, 'learning_rate': 0.006246983661364105, 'batch_size': 32, 'num_epochs': 11}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:47,236] Trial 17 finished with value: 0.625 and parameters: {'cross_layers': 5, 'hidden_layer_0': 91, 'hidden_layer_1': 76, 'hidden_layer_2': 135, 'learning_rate': 0.0002458833379982921, 'batch_size': 64, 'num_epochs': 67}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:47,629] Trial 18 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 190, 'hidden_layer_1': 215, 'hidden_layer_2': 92, 'learning_rate': 0.001223070783391054, 'batch_size': 128, 'num_epochs': 46}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:47,999] Trial 19 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 219, 'hidden_layer_1': 255, 'hidden_layer_2': 52, 'learning_rate': 0.019484612161988706, 'batch_size': 256, 'num_epochs': 30}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:48,228] Trial 20 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 159, 'hidden_layer_1': 190, 'hidden_layer_2': 171, 'learning_rate': 0.00620804030716667, 'batch_size': 128, 'num_epochs': 21}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:48,913] Trial 21 finished with value: 0.875 and parameters: {'cross_layers': 2, 'hidden_layer_0': 124, 'hidden_layer_1': 230, 'hidden_layer_2': 226, 'learning_rate': 0.005120952259380891, 'batch_size': 32, 'num_epochs': 93}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:49,546] Trial 22 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 84, 'hidden_layer_1': 217, 'hidden_layer_2': 227, 'learning_rate': 0.0044969451854298724, 'batch_size': 32, 'num_epochs': 87}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:50,036] Trial 23 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 136, 'hidden_layer_1': 241, 'hidden_layer_2': 254, 'learning_rate': 0.009841909204398668, 'batch_size': 32, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:50,698] Trial 24 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 176, 'hidden_layer_1': 154, 'hidden_layer_2': 199, 'learning_rate': 0.002196538945828169, 'batch_size': 32, 'num_epochs': 82}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:50,997] Trial 25 finished with value: 0.75 and parameters: {'cross_layers': 1, 'hidden_layer_0': 122, 'hidden_layer_1': 214, 'hidden_layer_2': 229, 'learning_rate': 0.03349825867322429, 'batch_size': 32, 'num_epochs': 38}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:51,451] Trial 26 finished with value: 0.625 and parameters: {'cross_layers': 5, 'hidden_layer_0': 77, 'hidden_layer_1': 175, 'hidden_layer_2': 99, 'learning_rate': 0.00347828966191968, 'batch_size': 256, 'num_epochs': 50}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:51,905] Trial 27 finished with value: 0.625 and parameters: {'cross_layers': 2, 'hidden_layer_0': 37, 'hidden_layer_1': 198, 'hidden_layer_2': 184, 'learning_rate': 0.0001958651706769927, 'batch_size': 64, 'num_epochs': 64}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:52,177] Trial 28 finished with value: 0.875 and parameters: {'cross_layers': 4, 'hidden_layer_0': 158, 'hidden_layer_1': 140, 'hidden_layer_2': 146, 'learning_rate': 0.0005833303130764577, 'batch_size': 32, 'num_epochs': 28}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:52,586] Trial 29 finished with value: 0.75 and parameters: {'cross_layers': 1, 'hidden_layer_0': 225, 'hidden_layer_1': 224, 'hidden_layer_2': 207, 'learning_rate': 0.049991697824872984, 'batch_size': 128, 'num_epochs': 54}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:52,980] Trial 30 finished with value: 0.875 and parameters: {'cross_layers': 5, 'hidden_layer_0': 71, 'hidden_layer_1': 107, 'hidden_layer_2': 243, 'learning_rate': 0.008262379659523878, 'batch_size': 256, 'num_epochs': 39}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:53,378] Trial 31 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 50, 'hidden_layer_1': 242, 'hidden_layer_2': 110, 'learning_rate': 0.016588761618644032, 'batch_size': 256, 'num_epochs': 41}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:53,792] Trial 32 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 49, 'hidden_layer_1': 41, 'hidden_layer_2': 137, 'learning_rate': 0.09054319567642591, 'batch_size': 256, 'num_epochs': 42}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:54,121] Trial 33 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 61, 'hidden_layer_1': 230, 'hidden_layer_2': 75, 'learning_rate': 0.019064948282848968, 'batch_size': 256, 'num_epochs': 33}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:54,544] Trial 34 finished with value: 0.625 and parameters: {'cross_layers': 3, 'hidden_layer_0': 42, 'hidden_layer_1': 58, 'hidden_layer_2': 101, 'learning_rate': 0.009511770776690527, 'batch_size': 256, 'num_epochs': 54}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:54,712] Trial 35 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 67, 'hidden_layer_1': 206, 'hidden_layer_2': 123, 'learning_rate': 0.028241507237419142, 'batch_size': 128, 'num_epochs': 18}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:55,100] Trial 36 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 101, 'hidden_layer_1': 242, 'hidden_layer_2': 239, 'learning_rate': 0.051161196821349, 'batch_size': 256, 'num_epochs': 34}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:55,367] Trial 37 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 180, 'hidden_layer_1': 181, 'hidden_layer_2': 66, 'learning_rate': 0.001571811151222937, 'batch_size': 128, 'num_epochs': 25}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:55,736] Trial 38 finished with value: 0.875 and parameters: {'cross_layers': 1, 'hidden_layer_0': 244, 'hidden_layer_1': 252, 'hidden_layer_2': 218, 'learning_rate': 0.0035325371750498068, 'batch_size': 256, 'num_epochs': 45}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:56,150] Trial 39 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 55, 'hidden_layer_1': 143, 'hidden_layer_2': 81, 'learning_rate': 0.002201543022117994, 'batch_size': 64, 'num_epochs': 51}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:56,891] Trial 40 finished with value: 0.75 and parameters: {'cross_layers': 5, 'hidden_layer_0': 148, 'hidden_layer_1': 200, 'hidden_layer_2': 164, 'learning_rate': 0.0001648514781693983, 'batch_size': 128, 'num_epochs': 78}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:57,782] Trial 41 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 124, 'hidden_layer_1': 227, 'hidden_layer_2': 234, 'learning_rate': 0.005538387796401345, 'batch_size': 32, 'num_epochs': 97}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:58,514] Trial 42 finished with value: 0.875 and parameters: {'cross_layers': 2, 'hidden_layer_0': 115, 'hidden_layer_1': 234, 'hidden_layer_2': 219, 'learning_rate': 0.013069850527752586, 'batch_size': 32, 'num_epochs': 94}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:46:59,348] Trial 43 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 103, 'hidden_layer_1': 221, 'hidden_layer_2': 247, 'learning_rate': 0.00477888671896725, 'batch_size': 32, 'num_epochs': 89}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:00,284] Trial 44 finished with value: 0.875 and parameters: {'cross_layers': 2, 'hidden_layer_0': 130, 'hidden_layer_1': 244, 'hidden_layer_2': 210, 'learning_rate': 0.007220274153156957, 'batch_size': 32, 'num_epochs': 86}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:00,903] Trial 45 finished with value: 0.75 and parameters: {'cross_layers': 1, 'hidden_layer_0': 148, 'hidden_layer_1': 208, 'hidden_layer_2': 180, 'learning_rate': 0.013673530454424264, 'batch_size': 32, 'num_epochs': 92}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:01,403] Trial 46 finished with value: 0.75 and parameters: {'cross_layers': 2, 'hidden_layer_0': 87, 'hidden_layer_1': 189, 'hidden_layer_2': 256, 'learning_rate': 0.0006048088655791755, 'batch_size': 256, 'num_epochs': 73}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:01,733] Trial 47 finished with value: 0.75 and parameters: {'cross_layers': 4, 'hidden_layer_0': 206, 'hidden_layer_1': 128, 'hidden_layer_2': 196, 'learning_rate': 0.002711364522957453, 'batch_size': 32, 'num_epochs': 35}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:02,233] Trial 48 finished with value: 0.75 and parameters: {'cross_layers': 3, 'hidden_layer_0': 184, 'hidden_layer_1': 166, 'hidden_layer_2': 93, 'learning_rate': 0.0009336738971129497, 'batch_size': 64, 'num_epochs': 59}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1122408048.py:77: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:03,143] Trial 49 finished with value: 0.875 and parameters: {'cross_layers': 5, 'hidden_layer_0': 109, 'hidden_layer_1': 235, 'hidden_layer_2': 112, 'learning_rate': 0.025463529548282564, 'batch_size': 128, 'num_epochs': 98}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "cross_layers: 4\n",
      "hidden_layer_0: 184\n",
      "hidden_layer_1: 169\n",
      "hidden_layer_2: 100\n",
      "learning_rate: 0.0003024897105741653\n",
      "batch_size: 128\n",
      "num_epochs: 36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x0, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x0 = x0.unsqueeze(2)\n",
    "        interaction = torch.matmul(x0, torch.matmul(x.transpose(1, 2), self.weight))\n",
    "        return x0.squeeze(2) + interaction.squeeze(2) + self.bias.T\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, input_dim, cross_layers, hidden_layers, output_dim):\n",
    "        super(DCN, self).__init__()\n",
    "        self.cross_layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(cross_layers)])\n",
    "        \n",
    "        deep_layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "        self.deep_net = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        self.final_layer = nn.Linear(input_dim + hidden_layers[-1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cross_out = x\n",
    "        for layer in self.cross_layers:\n",
    "            cross_out = layer(x, cross_out)\n",
    "        deep_out = self.deep_net(x)\n",
    "        concat_out = torch.cat([cross_out, deep_out], dim=1)\n",
    "        return self.final_layer(concat_out)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    cross_layers = trial.suggest_int('cross_layers', 1, 5)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = DCN(input_dim, cross_layers, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = DCN(input_dim, best_params['cross_layers'], \n",
    "                 [best_params[f'hidden_layer_{i}'] for i in range(3)], \n",
    "                 output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['DCN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:47:03,528] A new study created in memory with name: no-name-e4818d4b-c71f-436d-bd5b-0e49d9597479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:03,937] Trial 0 finished with value: 0.875 and parameters: {'hidden_layer_0': 56, 'hidden_layer_1': 227, 'hidden_layer_2': 118, 'learning_rate': 0.0041430798801775125, 'batch_size': 128, 'num_epochs': 84}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:03,997] Trial 1 finished with value: 0.625 and parameters: {'hidden_layer_0': 190, 'hidden_layer_1': 197, 'hidden_layer_2': 167, 'learning_rate': 0.05923346005014561, 'batch_size': 32, 'num_epochs': 10}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:04,352] Trial 2 finished with value: 0.75 and parameters: {'hidden_layer_0': 123, 'hidden_layer_1': 108, 'hidden_layer_2': 148, 'learning_rate': 0.0011046238994844699, 'batch_size': 32, 'num_epochs': 78}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:04,632] Trial 3 finished with value: 0.875 and parameters: {'hidden_layer_0': 141, 'hidden_layer_1': 33, 'hidden_layer_2': 256, 'learning_rate': 0.004804514455657034, 'batch_size': 128, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:04,949] Trial 4 finished with value: 0.75 and parameters: {'hidden_layer_0': 164, 'hidden_layer_1': 80, 'hidden_layer_2': 78, 'learning_rate': 0.02697705264473068, 'batch_size': 128, 'num_epochs': 67}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:05,356] Trial 5 finished with value: 0.75 and parameters: {'hidden_layer_0': 60, 'hidden_layer_1': 66, 'hidden_layer_2': 60, 'learning_rate': 0.00034903426602142225, 'batch_size': 128, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:06,148] Trial 6 finished with value: 0.625 and parameters: {'hidden_layer_0': 88, 'hidden_layer_1': 249, 'hidden_layer_2': 190, 'learning_rate': 0.0005324555655325558, 'batch_size': 256, 'num_epochs': 74}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:07,011] Trial 7 finished with value: 0.75 and parameters: {'hidden_layer_0': 127, 'hidden_layer_1': 248, 'hidden_layer_2': 190, 'learning_rate': 0.0005426274981909569, 'batch_size': 128, 'num_epochs': 82}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:07,254] Trial 8 finished with value: 0.875 and parameters: {'hidden_layer_0': 241, 'hidden_layer_1': 59, 'hidden_layer_2': 120, 'learning_rate': 0.025511031085127513, 'batch_size': 64, 'num_epochs': 49}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:07,670] Trial 9 finished with value: 0.5 and parameters: {'hidden_layer_0': 96, 'hidden_layer_1': 116, 'hidden_layer_2': 69, 'learning_rate': 0.000133237275664105, 'batch_size': 64, 'num_epochs': 87}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:07,918] Trial 10 finished with value: 0.75 and parameters: {'hidden_layer_0': 43, 'hidden_layer_1': 179, 'hidden_layer_2': 110, 'learning_rate': 0.0047494343745508975, 'batch_size': 256, 'num_epochs': 42}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:08,239] Trial 11 finished with value: 0.875 and parameters: {'hidden_layer_0': 192, 'hidden_layer_1': 33, 'hidden_layer_2': 252, 'learning_rate': 0.0059246507236104685, 'batch_size': 128, 'num_epochs': 57}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:08,466] Trial 12 finished with value: 0.75 and parameters: {'hidden_layer_0': 33, 'hidden_layer_1': 170, 'hidden_layer_2': 245, 'learning_rate': 0.002326578337604907, 'batch_size': 128, 'num_epochs': 32}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:09,936] Trial 13 finished with value: 0.875 and parameters: {'hidden_layer_0': 85, 'hidden_layer_1': 209, 'hidden_layer_2': 33, 'learning_rate': 0.01374668505916491, 'batch_size': 128, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:10,315] Trial 14 finished with value: 0.75 and parameters: {'hidden_layer_0': 245, 'hidden_layer_1': 142, 'hidden_layer_2': 111, 'learning_rate': 0.0018609290736481454, 'batch_size': 128, 'num_epochs': 66}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:10,732] Trial 15 finished with value: 0.875 and parameters: {'hidden_layer_0': 167, 'hidden_layer_1': 222, 'hidden_layer_2': 215, 'learning_rate': 0.00910459864760245, 'batch_size': 128, 'num_epochs': 35}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:11,143] Trial 16 finished with value: 0.75 and parameters: {'hidden_layer_0': 111, 'hidden_layer_1': 147, 'hidden_layer_2': 139, 'learning_rate': 0.0029344081735255398, 'batch_size': 64, 'num_epochs': 59}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:11,660] Trial 17 finished with value: 0.875 and parameters: {'hidden_layer_0': 65, 'hidden_layer_1': 114, 'hidden_layer_2': 219, 'learning_rate': 0.09615490686120659, 'batch_size': 256, 'num_epochs': 90}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:11,813] Trial 18 finished with value: 0.875 and parameters: {'hidden_layer_0': 217, 'hidden_layer_1': 40, 'hidden_layer_2': 90, 'learning_rate': 0.013054272264795917, 'batch_size': 32, 'num_epochs': 19}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:12,241] Trial 19 finished with value: 0.75 and parameters: {'hidden_layer_0': 144, 'hidden_layer_1': 86, 'hidden_layer_2': 169, 'learning_rate': 0.0008946759744047693, 'batch_size': 128, 'num_epochs': 71}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:12,821] Trial 20 finished with value: 0.875 and parameters: {'hidden_layer_0': 68, 'hidden_layer_1': 149, 'hidden_layer_2': 137, 'learning_rate': 0.00471371188875187, 'batch_size': 128, 'num_epochs': 90}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:13,091] Trial 21 finished with value: 0.75 and parameters: {'hidden_layer_0': 252, 'hidden_layer_1': 53, 'hidden_layer_2': 109, 'learning_rate': 0.026368093218526397, 'batch_size': 64, 'num_epochs': 45}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:13,403] Trial 22 finished with value: 0.75 and parameters: {'hidden_layer_0': 223, 'hidden_layer_1': 60, 'hidden_layer_2': 125, 'learning_rate': 0.019497931436306716, 'batch_size': 64, 'num_epochs': 47}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:13,822] Trial 23 finished with value: 0.75 and parameters: {'hidden_layer_0': 154, 'hidden_layer_1': 87, 'hidden_layer_2': 161, 'learning_rate': 0.04463994863691511, 'batch_size': 64, 'num_epochs': 62}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:14,138] Trial 24 finished with value: 0.75 and parameters: {'hidden_layer_0': 194, 'hidden_layer_1': 32, 'hidden_layer_2': 101, 'learning_rate': 0.007065668884588797, 'batch_size': 64, 'num_epochs': 53}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:14,377] Trial 25 finished with value: 0.75 and parameters: {'hidden_layer_0': 226, 'hidden_layer_1': 228, 'hidden_layer_2': 52, 'learning_rate': 0.0015169343848637847, 'batch_size': 64, 'num_epochs': 31}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:14,682] Trial 26 finished with value: 0.75 and parameters: {'hidden_layer_0': 177, 'hidden_layer_1': 54, 'hidden_layer_2': 129, 'learning_rate': 0.003559297413464274, 'batch_size': 32, 'num_epochs': 51}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:14,896] Trial 27 finished with value: 0.75 and parameters: {'hidden_layer_0': 129, 'hidden_layer_1': 100, 'hidden_layer_2': 185, 'learning_rate': 0.010362928132069172, 'batch_size': 256, 'num_epochs': 22}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:15,160] Trial 28 finished with value: 0.875 and parameters: {'hidden_layer_0': 104, 'hidden_layer_1': 134, 'hidden_layer_2': 93, 'learning_rate': 0.04857291601062401, 'batch_size': 128, 'num_epochs': 38}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:15,712] Trial 29 finished with value: 1.0 and parameters: {'hidden_layer_0': 205, 'hidden_layer_1': 188, 'hidden_layer_2': 155, 'learning_rate': 0.07621259981949924, 'batch_size': 32, 'num_epochs': 81}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:16,234] Trial 30 finished with value: 0.625 and parameters: {'hidden_layer_0': 203, 'hidden_layer_1': 187, 'hidden_layer_2': 223, 'learning_rate': 0.07465037887017784, 'batch_size': 32, 'num_epochs': 81}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:16,717] Trial 31 finished with value: 0.75 and parameters: {'hidden_layer_0': 239, 'hidden_layer_1': 205, 'hidden_layer_2': 166, 'learning_rate': 0.033928480303991496, 'batch_size': 32, 'num_epochs': 73}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:17,316] Trial 32 finished with value: 0.875 and parameters: {'hidden_layer_0': 205, 'hidden_layer_1': 229, 'hidden_layer_2': 151, 'learning_rate': 0.06722683062726609, 'batch_size': 32, 'num_epochs': 95}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:17,893] Trial 33 finished with value: 0.75 and parameters: {'hidden_layer_0': 235, 'hidden_layer_1': 196, 'hidden_layer_2': 123, 'learning_rate': 0.018893766903788804, 'batch_size': 32, 'num_epochs': 79}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:18,335] Trial 34 finished with value: 0.75 and parameters: {'hidden_layer_0': 180, 'hidden_layer_1': 166, 'hidden_layer_2': 82, 'learning_rate': 0.0348887270319399, 'batch_size': 32, 'num_epochs': 67}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:18,925] Trial 35 finished with value: 1.0 and parameters: {'hidden_layer_0': 140, 'hidden_layer_1': 71, 'hidden_layer_2': 235, 'learning_rate': 0.09781902405777142, 'batch_size': 128, 'num_epochs': 87}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:19,513] Trial 36 finished with value: 0.75 and parameters: {'hidden_layer_0': 139, 'hidden_layer_1': 77, 'hidden_layer_2': 207, 'learning_rate': 0.09899174321144112, 'batch_size': 128, 'num_epochs': 86}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:20,030] Trial 37 finished with value: 0.75 and parameters: {'hidden_layer_0': 158, 'hidden_layer_1': 238, 'hidden_layer_2': 238, 'learning_rate': 0.00022773685818950162, 'batch_size': 128, 'num_epochs': 75}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:20,660] Trial 38 finished with value: 0.75 and parameters: {'hidden_layer_0': 114, 'hidden_layer_1': 216, 'hidden_layer_2': 256, 'learning_rate': 0.0009472541441498101, 'batch_size': 128, 'num_epochs': 94}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:21,302] Trial 39 finished with value: 0.75 and parameters: {'hidden_layer_0': 128, 'hidden_layer_1': 254, 'hidden_layer_2': 234, 'learning_rate': 0.05512752453557098, 'batch_size': 128, 'num_epochs': 82}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:21,721] Trial 40 finished with value: 0.75 and parameters: {'hidden_layer_0': 86, 'hidden_layer_1': 45, 'hidden_layer_2': 199, 'learning_rate': 0.0005263988492448798, 'batch_size': 128, 'num_epochs': 77}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:22,261] Trial 41 finished with value: 0.75 and parameters: {'hidden_layer_0': 255, 'hidden_layer_1': 77, 'hidden_layer_2': 179, 'learning_rate': 0.021802817588493253, 'batch_size': 256, 'num_epochs': 85}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:22,619] Trial 42 finished with value: 0.625 and parameters: {'hidden_layer_0': 176, 'hidden_layer_1': 66, 'hidden_layer_2': 151, 'learning_rate': 0.039895105341059126, 'batch_size': 64, 'num_epochs': 61}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:23,154] Trial 43 finished with value: 0.875 and parameters: {'hidden_layer_0': 52, 'hidden_layer_1': 102, 'hidden_layer_2': 230, 'learning_rate': 0.06715612813105773, 'batch_size': 128, 'num_epochs': 94}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:23,685] Trial 44 finished with value: 0.625 and parameters: {'hidden_layer_0': 143, 'hidden_layer_1': 127, 'hidden_layer_2': 246, 'learning_rate': 0.003934473333426409, 'batch_size': 128, 'num_epochs': 70}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:24,029] Trial 45 finished with value: 0.625 and parameters: {'hidden_layer_0': 153, 'hidden_layer_1': 66, 'hidden_layer_2': 118, 'learning_rate': 0.014594175299992702, 'batch_size': 128, 'num_epochs': 48}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:24,403] Trial 46 finished with value: 0.75 and parameters: {'hidden_layer_0': 204, 'hidden_layer_1': 45, 'hidden_layer_2': 205, 'learning_rate': 0.005859883446808405, 'batch_size': 32, 'num_epochs': 63}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:24,732] Trial 47 finished with value: 1.0 and parameters: {'hidden_layer_0': 72, 'hidden_layer_1': 158, 'hidden_layer_2': 63, 'learning_rate': 0.008378908807673386, 'batch_size': 64, 'num_epochs': 56}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:25,096] Trial 48 finished with value: 0.75 and parameters: {'hidden_layer_0': 51, 'hidden_layer_1': 164, 'hidden_layer_2': 55, 'learning_rate': 0.0026567642209369743, 'batch_size': 256, 'num_epochs': 56}. Best is trial 29 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3180404897.py:61: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:26,053] Trial 49 finished with value: 0.875 and parameters: {'hidden_layer_0': 76, 'hidden_layer_1': 157, 'hidden_layer_2': 33, 'learning_rate': 0.007714627358375351, 'batch_size': 128, 'num_epochs': 90}. Best is trial 29 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_layer_0: 205\n",
      "hidden_layer_1: 188\n",
      "hidden_layer_2: 155\n",
      "learning_rate: 0.07621259981949924\n",
      "batch_size: 32\n",
      "num_epochs: 81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class WideAndDeepNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(WideAndDeepNetwork, self).__init__()\n",
    "        \n",
    "        # Wide part\n",
    "        self.wide = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        # Deep part\n",
    "        deep_layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                deep_layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                deep_layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "        deep_layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.deep = nn.Sequential(*deep_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        wide_out = self.wide(x)\n",
    "        deep_out = self.deep(x)\n",
    "        return wide_out + deep_out\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = WideAndDeepNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = WideAndDeepNetwork(input_dim, \n",
    "                                [best_params[f'hidden_layer_{i}'] for i in range(3)], \n",
    "                                output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['Wide_and_Deep'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:47:26,601] A new study created in memory with name: no-name-14cde900-b97d-4cbb-adbc-5872cc99c59a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:27,143] Trial 0 finished with value: 0.875 and parameters: {'n_estimators': 204, 'max_depth': 7, 'xgb_learning_rate': 0.0010371340988060615, 'subsample': 0.6175172180456624, 'colsample_bytree': 0.7451200216773843, 'hidden_layer_0': 127, 'hidden_layer_1': 88, 'hidden_layer_2': 92, 'nn_learning_rate': 0.0041130999314882115, 'batch_size': 256, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:28,255] Trial 1 finished with value: 0.875 and parameters: {'n_estimators': 197, 'max_depth': 4, 'xgb_learning_rate': 0.0009457318477613953, 'subsample': 0.8155502476893431, 'colsample_bytree': 0.6456736098230677, 'hidden_layer_0': 115, 'hidden_layer_1': 51, 'hidden_layer_2': 172, 'nn_learning_rate': 0.00020823560423936948, 'batch_size': 128, 'num_epochs': 46}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:28,935] Trial 2 finished with value: 0.875 and parameters: {'n_estimators': 152, 'max_depth': 9, 'xgb_learning_rate': 0.0007887519645363873, 'subsample': 0.6185150183245769, 'colsample_bytree': 0.9692125221658303, 'hidden_layer_0': 93, 'hidden_layer_1': 241, 'hidden_layer_2': 137, 'nn_learning_rate': 0.01760138887344757, 'batch_size': 256, 'num_epochs': 81}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:29,429] Trial 3 finished with value: 0.875 and parameters: {'n_estimators': 55, 'max_depth': 3, 'xgb_learning_rate': 0.017382168418318838, 'subsample': 0.6505130388862379, 'colsample_bytree': 0.8855587778271687, 'hidden_layer_0': 217, 'hidden_layer_1': 254, 'hidden_layer_2': 49, 'nn_learning_rate': 0.005443707479760451, 'batch_size': 64, 'num_epochs': 70}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:30,001] Trial 4 finished with value: 0.875 and parameters: {'n_estimators': 58, 'max_depth': 6, 'xgb_learning_rate': 0.0011949886885771267, 'subsample': 0.5198378695288195, 'colsample_bytree': 0.6526616112066406, 'hidden_layer_0': 43, 'hidden_layer_1': 73, 'hidden_layer_2': 164, 'nn_learning_rate': 0.0008610161516613324, 'batch_size': 32, 'num_epochs': 76}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:30,477] Trial 5 finished with value: 0.875 and parameters: {'n_estimators': 165, 'max_depth': 9, 'xgb_learning_rate': 0.024056562026430306, 'subsample': 0.8953645865511564, 'colsample_bytree': 0.9766643457706181, 'hidden_layer_0': 142, 'hidden_layer_1': 137, 'hidden_layer_2': 74, 'nn_learning_rate': 0.00022147392042935957, 'batch_size': 64, 'num_epochs': 67}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:30,852] Trial 6 finished with value: 0.875 and parameters: {'n_estimators': 91, 'max_depth': 9, 'xgb_learning_rate': 0.023422303945727075, 'subsample': 0.5546020306925215, 'colsample_bytree': 0.9315137514635033, 'hidden_layer_0': 189, 'hidden_layer_1': 200, 'hidden_layer_2': 94, 'nn_learning_rate': 0.0007966867353126284, 'batch_size': 256, 'num_epochs': 56}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:31,184] Trial 7 finished with value: 0.875 and parameters: {'n_estimators': 295, 'max_depth': 5, 'xgb_learning_rate': 0.0002387821253360141, 'subsample': 0.8408448290678456, 'colsample_bytree': 0.6952925329105604, 'hidden_layer_0': 158, 'hidden_layer_1': 176, 'hidden_layer_2': 116, 'nn_learning_rate': 0.00015329062044338863, 'batch_size': 128, 'num_epochs': 23}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:31,596] Trial 8 finished with value: 0.5 and parameters: {'n_estimators': 89, 'max_depth': 5, 'xgb_learning_rate': 0.01009086595757549, 'subsample': 0.804506988649693, 'colsample_bytree': 0.8169251858566859, 'hidden_layer_0': 90, 'hidden_layer_1': 250, 'hidden_layer_2': 252, 'nn_learning_rate': 0.0419489274743473, 'batch_size': 256, 'num_epochs': 57}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:31,987] Trial 9 finished with value: 0.75 and parameters: {'n_estimators': 132, 'max_depth': 7, 'xgb_learning_rate': 0.08545990368581333, 'subsample': 0.8694440194637278, 'colsample_bytree': 0.5639232232319177, 'hidden_layer_0': 202, 'hidden_layer_1': 82, 'hidden_layer_2': 231, 'nn_learning_rate': 0.03249292338877331, 'batch_size': 32, 'num_epochs': 49}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:32,835] Trial 10 finished with value: 0.875 and parameters: {'n_estimators': 231, 'max_depth': 7, 'xgb_learning_rate': 0.00012619718914663391, 'subsample': 0.9851482678202259, 'colsample_bytree': 0.7854013329216777, 'hidden_layer_0': 253, 'hidden_layer_1': 120, 'hidden_layer_2': 38, 'nn_learning_rate': 0.006030353295223327, 'batch_size': 256, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:33,237] Trial 11 finished with value: 0.875 and parameters: {'n_estimators': 224, 'max_depth': 3, 'xgb_learning_rate': 0.0022146733325659196, 'subsample': 0.7151178373737048, 'colsample_bytree': 0.5693590462589806, 'hidden_layer_0': 108, 'hidden_layer_1': 35, 'hidden_layer_2': 174, 'nn_learning_rate': 0.0012955350743508105, 'batch_size': 128, 'num_epochs': 33}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:33,662] Trial 12 finished with value: 0.875 and parameters: {'n_estimators': 216, 'max_depth': 5, 'xgb_learning_rate': 0.0004579848727274042, 'subsample': 0.7200783375421509, 'colsample_bytree': 0.6719929634970008, 'hidden_layer_0': 128, 'hidden_layer_1': 35, 'hidden_layer_2': 214, 'nn_learning_rate': 0.002551291586788078, 'batch_size': 128, 'num_epochs': 39}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:33,939] Trial 13 finished with value: 0.875 and parameters: {'n_estimators': 198, 'max_depth': 8, 'xgb_learning_rate': 0.004095364630820879, 'subsample': 0.6403090498855917, 'colsample_bytree': 0.5034455827445778, 'hidden_layer_0': 64, 'hidden_layer_1': 93, 'hidden_layer_2': 190, 'nn_learning_rate': 0.00026306042339361793, 'batch_size': 128, 'num_epochs': 13}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:34,768] Trial 14 finished with value: 0.875 and parameters: {'n_estimators': 262, 'max_depth': 4, 'xgb_learning_rate': 0.0034468143102874592, 'subsample': 0.7708774386266689, 'colsample_bytree': 0.7432313866691277, 'hidden_layer_0': 167, 'hidden_layer_1': 68, 'hidden_layer_2': 132, 'nn_learning_rate': 0.011181607332780862, 'batch_size': 256, 'num_epochs': 92}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:35,233] Trial 15 finished with value: 0.5 and parameters: {'n_estimators': 188, 'max_depth': 6, 'xgb_learning_rate': 0.000453443200147377, 'subsample': 0.9581722802750906, 'colsample_bytree': 0.6167271063965232, 'hidden_layer_0': 128, 'hidden_layer_1': 109, 'hidden_layer_2': 99, 'nn_learning_rate': 0.08512100375132153, 'batch_size': 128, 'num_epochs': 40}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:35,861] Trial 16 finished with value: 0.875 and parameters: {'n_estimators': 260, 'max_depth': 10, 'xgb_learning_rate': 0.001730716066719304, 'subsample': 0.5821441989346028, 'colsample_bytree': 0.8420152101286846, 'hidden_layer_0': 64, 'hidden_layer_1': 51, 'hidden_layer_2': 153, 'nn_learning_rate': 0.002389112180984712, 'batch_size': 64, 'num_epochs': 89}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:36,389] Trial 17 finished with value: 0.875 and parameters: {'n_estimators': 121, 'max_depth': 4, 'xgb_learning_rate': 0.0007003667020616359, 'subsample': 0.6834185074984177, 'colsample_bytree': 0.7394231517337441, 'hidden_layer_0': 114, 'hidden_layer_1': 150, 'hidden_layer_2': 194, 'nn_learning_rate': 0.00042685805977543337, 'batch_size': 32, 'num_epochs': 63}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:36,829] Trial 18 finished with value: 0.875 and parameters: {'n_estimators': 184, 'max_depth': 8, 'xgb_learning_rate': 0.005948073798538281, 'subsample': 0.9219190359102869, 'colsample_bytree': 0.7105886568599842, 'hidden_layer_0': 79, 'hidden_layer_1': 95, 'hidden_layer_2': 72, 'nn_learning_rate': 0.00010105086225147418, 'batch_size': 256, 'num_epochs': 49}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:37,516] Trial 19 finished with value: 0.875 and parameters: {'n_estimators': 251, 'max_depth': 4, 'xgb_learning_rate': 0.00011386980029355806, 'subsample': 0.7820648558754569, 'colsample_bytree': 0.6328510724909149, 'hidden_layer_0': 172, 'hidden_layer_1': 51, 'hidden_layer_2': 116, 'nn_learning_rate': 0.0060607944992348015, 'batch_size': 128, 'num_epochs': 76}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:42,982] Trial 20 finished with value: 0.875 and parameters: {'n_estimators': 287, 'max_depth': 6, 'xgb_learning_rate': 0.0003003819327949977, 'subsample': 0.8253948040345411, 'colsample_bytree': 0.7791466213662107, 'hidden_layer_0': 139, 'hidden_layer_1': 113, 'hidden_layer_2': 80, 'nn_learning_rate': 0.00048764598972981886, 'batch_size': 256, 'num_epochs': 27}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:48,369] Trial 21 finished with value: 0.875 and parameters: {'n_estimators': 161, 'max_depth': 10, 'xgb_learning_rate': 0.0009308362103613554, 'subsample': 0.6040285366976375, 'colsample_bytree': 0.8801196270845857, 'hidden_layer_0': 99, 'hidden_layer_1': 210, 'hidden_layer_2': 137, 'nn_learning_rate': 0.012652645672363915, 'batch_size': 256, 'num_epochs': 82}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:50,113] Trial 22 finished with value: 0.375 and parameters: {'n_estimators': 142, 'max_depth': 8, 'xgb_learning_rate': 0.0014245024350827546, 'subsample': 0.5012893023392403, 'colsample_bytree': 0.9802639627860084, 'hidden_layer_0': 38, 'hidden_layer_1': 228, 'hidden_layer_2': 121, 'nn_learning_rate': 0.019212956095122476, 'batch_size': 256, 'num_epochs': 84}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:51,629] Trial 23 finished with value: 0.875 and parameters: {'n_estimators': 155, 'max_depth': 9, 'xgb_learning_rate': 0.0006546811204816436, 'subsample': 0.6232874329011968, 'colsample_bytree': 0.5915900881188758, 'hidden_layer_0': 115, 'hidden_layer_1': 165, 'hidden_layer_2': 154, 'nn_learning_rate': 0.0018998164698205778, 'batch_size': 256, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:52,483] Trial 24 finished with value: 0.875 and parameters: {'n_estimators': 216, 'max_depth': 7, 'xgb_learning_rate': 0.00020841114559366512, 'subsample': 0.7414435342070291, 'colsample_bytree': 0.7031284050945827, 'hidden_layer_0': 78, 'hidden_layer_1': 56, 'hidden_layer_2': 188, 'nn_learning_rate': 0.005147620747894406, 'batch_size': 256, 'num_epochs': 74}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:53,461] Trial 25 finished with value: 0.625 and parameters: {'n_estimators': 199, 'max_depth': 8, 'xgb_learning_rate': 0.002440486182098824, 'subsample': 0.665486610969682, 'colsample_bytree': 0.5216056076741865, 'hidden_layer_0': 94, 'hidden_layer_1': 131, 'hidden_layer_2': 98, 'nn_learning_rate': 0.02352995367673913, 'batch_size': 128, 'num_epochs': 62}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:54,099] Trial 26 finished with value: 0.375 and parameters: {'n_estimators': 113, 'max_depth': 9, 'xgb_learning_rate': 0.0004163227265780702, 'subsample': 0.6896033659912556, 'colsample_bytree': 0.7833990753818206, 'hidden_layer_0': 62, 'hidden_layer_1': 156, 'hidden_layer_2': 140, 'nn_learning_rate': 0.09335511932742337, 'batch_size': 64, 'num_epochs': 47}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:54,956] Trial 27 finished with value: 0.875 and parameters: {'n_estimators': 172, 'max_depth': 10, 'xgb_learning_rate': 0.001202814673185599, 'subsample': 0.5673528766219667, 'colsample_bytree': 0.8407017952162642, 'hidden_layer_0': 150, 'hidden_layer_1': 188, 'hidden_layer_2': 172, 'nn_learning_rate': 0.009833181513968154, 'batch_size': 32, 'num_epochs': 85}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:55,751] Trial 28 finished with value: 0.875 and parameters: {'n_estimators': 148, 'max_depth': 7, 'xgb_learning_rate': 0.005923821000698846, 'subsample': 0.6044425303820952, 'colsample_bytree': 0.9263960835077827, 'hidden_layer_0': 120, 'hidden_layer_1': 95, 'hidden_layer_2': 60, 'nn_learning_rate': 0.0037877029617687185, 'batch_size': 256, 'num_epochs': 79}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:56,603] Trial 29 finished with value: 0.25 and parameters: {'n_estimators': 240, 'max_depth': 3, 'xgb_learning_rate': 0.0007487333638178348, 'subsample': 0.6548032766134504, 'colsample_bytree': 0.8937893674838073, 'hidden_layer_0': 236, 'hidden_layer_1': 251, 'hidden_layer_2': 213, 'nn_learning_rate': 0.04413225134165149, 'batch_size': 64, 'num_epochs': 68}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:57,315] Trial 30 finished with value: 0.875 and parameters: {'n_estimators': 203, 'max_depth': 5, 'xgb_learning_rate': 0.0003252332280045083, 'subsample': 0.5412602815370839, 'colsample_bytree': 0.6613478389813969, 'hidden_layer_0': 184, 'hidden_layer_1': 66, 'hidden_layer_2': 124, 'nn_learning_rate': 0.0014368140379653572, 'batch_size': 128, 'num_epochs': 92}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:57,808] Trial 31 finished with value: 0.875 and parameters: {'n_estimators': 52, 'max_depth': 3, 'xgb_learning_rate': 0.06505721257586408, 'subsample': 0.6168004294587555, 'colsample_bytree': 0.9376884063865374, 'hidden_layer_0': 213, 'hidden_layer_1': 229, 'hidden_layer_2': 36, 'nn_learning_rate': 0.0038056900788673753, 'batch_size': 64, 'num_epochs': 70}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:58,376] Trial 32 finished with value: 0.75 and parameters: {'n_estimators': 77, 'max_depth': 4, 'xgb_learning_rate': 0.015540302896099544, 'subsample': 0.525314062993893, 'colsample_bytree': 0.8798510197964524, 'hidden_layer_0': 233, 'hidden_layer_1': 228, 'hidden_layer_2': 54, 'nn_learning_rate': 0.008758129917918482, 'batch_size': 64, 'num_epochs': 72}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:58,845] Trial 33 finished with value: 0.875 and parameters: {'n_estimators': 67, 'max_depth': 3, 'xgb_learning_rate': 0.039514587627316966, 'subsample': 0.5794753062395354, 'colsample_bytree': 0.9930019149037179, 'hidden_layer_0': 132, 'hidden_layer_1': 239, 'hidden_layer_2': 55, 'nn_learning_rate': 0.01491493475645688, 'batch_size': 64, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:47:59,732] Trial 34 finished with value: 0.875 and parameters: {'n_estimators': 179, 'max_depth': 6, 'xgb_learning_rate': 0.0010584107396494056, 'subsample': 0.7131399617086549, 'colsample_bytree': 0.9444746026900122, 'hidden_layer_0': 150, 'hidden_layer_1': 210, 'hidden_layer_2': 83, 'nn_learning_rate': 0.0010177163801783128, 'batch_size': 64, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:47:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:00,199] Trial 35 finished with value: 0.875 and parameters: {'n_estimators': 103, 'max_depth': 3, 'xgb_learning_rate': 0.010693869313211982, 'subsample': 0.7528958634505922, 'colsample_bytree': 0.9032053004027009, 'hidden_layer_0': 81, 'hidden_layer_1': 253, 'hidden_layer_2': 106, 'nn_learning_rate': 0.007199474481134559, 'batch_size': 32, 'num_epochs': 54}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:00,682] Trial 36 finished with value: 0.875 and parameters: {'n_estimators': 127, 'max_depth': 4, 'xgb_learning_rate': 0.00200489167651283, 'subsample': 0.8842091228893025, 'colsample_bytree': 0.9601449220028665, 'hidden_layer_0': 103, 'hidden_layer_1': 82, 'hidden_layer_2': 163, 'nn_learning_rate': 0.000696328356689417, 'batch_size': 256, 'num_epochs': 66}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:01,118] Trial 37 finished with value: 0.875 and parameters: {'n_estimators': 91, 'max_depth': 5, 'xgb_learning_rate': 0.00019766037575733782, 'subsample': 0.8233210575901732, 'colsample_bytree': 0.8315860511024485, 'hidden_layer_0': 161, 'hidden_layer_1': 187, 'hidden_layer_2': 64, 'nn_learning_rate': 0.004283846523035001, 'batch_size': 256, 'num_epochs': 55}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:01,713] Trial 38 finished with value: 0.875 and parameters: {'n_estimators': 209, 'max_depth': 9, 'xgb_learning_rate': 0.022580575147159304, 'subsample': 0.6770142497421814, 'colsample_bytree': 0.8053923674926599, 'hidden_layer_0': 54, 'hidden_layer_1': 130, 'hidden_layer_2': 86, 'nn_learning_rate': 0.026801498365919142, 'batch_size': 64, 'num_epochs': 88}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:02,214] Trial 39 finished with value: 0.375 and parameters: {'n_estimators': 235, 'max_depth': 6, 'xgb_learning_rate': 0.00296786806669555, 'subsample': 0.6441311041814686, 'colsample_bytree': 0.6815500434194874, 'hidden_layer_0': 197, 'hidden_layer_1': 238, 'hidden_layer_2': 110, 'nn_learning_rate': 0.05276636330235722, 'batch_size': 128, 'num_epochs': 40}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:02,808] Trial 40 finished with value: 0.875 and parameters: {'n_estimators': 167, 'max_depth': 7, 'xgb_learning_rate': 0.005379084696853597, 'subsample': 0.5553833183626957, 'colsample_bytree': 0.7208784956029336, 'hidden_layer_0': 176, 'hidden_layer_1': 44, 'hidden_layer_2': 45, 'nn_learning_rate': 0.015991990479880296, 'batch_size': 32, 'num_epochs': 79}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:03,381] Trial 41 finished with value: 0.875 and parameters: {'n_estimators': 53, 'max_depth': 5, 'xgb_learning_rate': 0.0005627998102853162, 'subsample': 0.5074567108429289, 'colsample_bytree': 0.6537829218621242, 'hidden_layer_0': 44, 'hidden_layer_1': 74, 'hidden_layer_2': 173, 'nn_learning_rate': 0.00023465525631395138, 'batch_size': 32, 'num_epochs': 95}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:03,922] Trial 42 finished with value: 0.875 and parameters: {'n_estimators': 76, 'max_depth': 9, 'xgb_learning_rate': 0.001471295759790325, 'subsample': 0.5302683617063311, 'colsample_bytree': 0.6193785194263339, 'hidden_layer_0': 89, 'hidden_layer_1': 82, 'hidden_layer_2': 151, 'nn_learning_rate': 0.00015085466786948014, 'batch_size': 32, 'num_epochs': 72}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:04,432] Trial 43 finished with value: 0.875 and parameters: {'n_estimators': 103, 'max_depth': 4, 'xgb_learning_rate': 0.0008782481968560514, 'subsample': 0.8676497673924474, 'colsample_bytree': 0.5875701355656534, 'hidden_layer_0': 32, 'hidden_layer_1': 65, 'hidden_layer_2': 203, 'nn_learning_rate': 0.00032814341870780034, 'batch_size': 32, 'num_epochs': 82}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:04,778] Trial 44 finished with value: 0.875 and parameters: {'n_estimators': 64, 'max_depth': 5, 'xgb_learning_rate': 0.0011986660918711428, 'subsample': 0.5885540298573435, 'colsample_bytree': 0.5477549201930058, 'hidden_layer_0': 144, 'hidden_layer_1': 36, 'hidden_layer_2': 250, 'nn_learning_rate': 0.0027746565810172147, 'batch_size': 128, 'num_epochs': 45}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:05,361] Trial 45 finished with value: 0.875 and parameters: {'n_estimators': 224, 'max_depth': 7, 'xgb_learning_rate': 0.0017979129579033552, 'subsample': 0.7062947410422491, 'colsample_bytree': 0.6413183487366214, 'hidden_layer_0': 123, 'hidden_layer_1': 105, 'hidden_layer_2': 163, 'nn_learning_rate': 0.0007404057805753005, 'batch_size': 256, 'num_epochs': 66}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:06,357] Trial 46 finished with value: 0.875 and parameters: {'n_estimators': 192, 'max_depth': 8, 'xgb_learning_rate': 0.0005284043035430836, 'subsample': 0.7931538546860912, 'colsample_bytree': 0.7417689984333741, 'hidden_layer_0': 136, 'hidden_layer_1': 208, 'hidden_layer_2': 181, 'nn_learning_rate': 0.0018682430762718598, 'batch_size': 32, 'num_epochs': 15}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:06,779] Trial 47 finished with value: 0.875 and parameters: {'n_estimators': 89, 'max_depth': 3, 'xgb_learning_rate': 0.0025899623652282816, 'subsample': 0.630900967507515, 'colsample_bytree': 0.8612702990669554, 'hidden_layer_0': 107, 'hidden_layer_1': 59, 'hidden_layer_2': 146, 'nn_learning_rate': 0.0005019633781792613, 'batch_size': 128, 'num_epochs': 58}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:07,405] Trial 48 finished with value: 0.875 and parameters: {'n_estimators': 141, 'max_depth': 6, 'xgb_learning_rate': 0.004025950519361405, 'subsample': 0.7385892432786251, 'colsample_bytree': 0.7572568155183774, 'hidden_layer_0': 72, 'hidden_layer_1': 243, 'hidden_layer_2': 163, 'nn_learning_rate': 0.00010593715498361437, 'batch_size': 256, 'num_epochs': 51}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\13981151.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:07,790] Trial 49 finished with value: 0.875 and parameters: {'n_estimators': 279, 'max_depth': 4, 'xgb_learning_rate': 0.000394616717741283, 'subsample': 0.9281786130701115, 'colsample_bytree': 0.6830617136723737, 'hidden_layer_0': 53, 'hidden_layer_1': 81, 'hidden_layer_2': 132, 'nn_learning_rate': 0.0001742494949250936, 'batch_size': 256, 'num_epochs': 30}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:48:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "XGBoost + NN           0.875  0.888889                    0.364206   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "XGBoost + NN                            0.000997                41.681399   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN         {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_estimators: 204\n",
      "max_depth: 7\n",
      "xgb_learning_rate: 0.0010371340988060615\n",
      "subsample: 0.6175172180456624\n",
      "colsample_bytree: 0.7451200216773843\n",
      "hidden_layer_0: 127\n",
      "hidden_layer_1: 88\n",
      "hidden_layer_2: 92\n",
      "nn_learning_rate: 0.0041130999314882115\n",
      "batch_size: 256\n",
      "num_epochs: 77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for XGBoost\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-4, 1e-1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train XGBoost model\n",
    "    xgb_model = XGBClassifier(**xgb_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using XGBoost\n",
    "    X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "    X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final XGBoost model with the best hyperparameters\n",
    "xgb_best_params = {\n",
    "    'n_estimators': best_params['n_estimators'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'learning_rate': best_params['xgb_learning_rate'],\n",
    "    'subsample': best_params['subsample'],\n",
    "    'colsample_bytree': best_params['colsample_bytree']\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using XGBoost\n",
    "X_train_transformed = xgb_model.apply(X_train_scaled)\n",
    "X_test_transformed = xgb_model.apply(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['XGBoost + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:08,387] A new study created in memory with name: no-name-643bb24c-82bd-4848-b6be-2c32307b6582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:08,639] Trial 0 finished with value: 0.375 and parameters: {'n_estimators': 135, 'max_depth': 6, 'lgb_learning_rate': 0.004344106798222012, 'num_leaves': 87, 'subsample': 0.627133286041994, 'colsample_bytree': 0.678357634226493, 'hidden_layer_0': 102, 'hidden_layer_1': 177, 'hidden_layer_2': 180, 'nn_learning_rate': 0.035613521983106015, 'batch_size': 128, 'num_epochs': 45}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:08,703] Trial 1 finished with value: 0.25 and parameters: {'n_estimators': 282, 'max_depth': 3, 'lgb_learning_rate': 0.0008018380661104405, 'num_leaves': 27, 'subsample': 0.910903275217358, 'colsample_bytree': 0.7231024885289279, 'hidden_layer_0': 123, 'hidden_layer_1': 38, 'hidden_layer_2': 248, 'nn_learning_rate': 0.006985564980418307, 'batch_size': 64, 'num_epochs': 10}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:09,011] Trial 2 finished with value: 0.25 and parameters: {'n_estimators': 293, 'max_depth': 6, 'lgb_learning_rate': 0.0177700758435059, 'num_leaves': 31, 'subsample': 0.7018695501799963, 'colsample_bytree': 0.6981637430902841, 'hidden_layer_0': 152, 'hidden_layer_1': 244, 'hidden_layer_2': 233, 'nn_learning_rate': 0.007261364537486744, 'batch_size': 64, 'num_epochs': 51}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:09,422] Trial 3 finished with value: 0.25 and parameters: {'n_estimators': 152, 'max_depth': 9, 'lgb_learning_rate': 0.00238015967610395, 'num_leaves': 42, 'subsample': 0.9489529461407711, 'colsample_bytree': 0.7700332527321114, 'hidden_layer_0': 179, 'hidden_layer_1': 59, 'hidden_layer_2': 181, 'nn_learning_rate': 0.0002920280837952565, 'batch_size': 256, 'num_epochs': 76}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:09,635] Trial 4 finished with value: 0.25 and parameters: {'n_estimators': 156, 'max_depth': 9, 'lgb_learning_rate': 0.001961888163322222, 'num_leaves': 82, 'subsample': 0.5804702300187159, 'colsample_bytree': 0.6964346630985988, 'hidden_layer_0': 229, 'hidden_layer_1': 210, 'hidden_layer_2': 145, 'nn_learning_rate': 0.014180753814713726, 'batch_size': 256, 'num_epochs': 36}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:09,842] Trial 5 finished with value: 0.375 and parameters: {'n_estimators': 249, 'max_depth': 10, 'lgb_learning_rate': 0.002367778417159049, 'num_leaves': 90, 'subsample': 0.5884822282084095, 'colsample_bytree': 0.7017338008742362, 'hidden_layer_0': 63, 'hidden_layer_1': 37, 'hidden_layer_2': 221, 'nn_learning_rate': 0.07588812610256784, 'batch_size': 256, 'num_epochs': 31}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:10,377] Trial 6 finished with value: 0.375 and parameters: {'n_estimators': 218, 'max_depth': 4, 'lgb_learning_rate': 0.04282815445966122, 'num_leaves': 96, 'subsample': 0.5183653187149606, 'colsample_bytree': 0.8270552101547505, 'hidden_layer_0': 146, 'hidden_layer_1': 159, 'hidden_layer_2': 167, 'nn_learning_rate': 0.002326052319484496, 'batch_size': 256, 'num_epochs': 96}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:10,522] Trial 7 finished with value: 0.375 and parameters: {'n_estimators': 209, 'max_depth': 5, 'lgb_learning_rate': 0.00045632340763066004, 'num_leaves': 68, 'subsample': 0.5367865414493158, 'colsample_bytree': 0.7978401108060664, 'hidden_layer_0': 234, 'hidden_layer_1': 188, 'hidden_layer_2': 132, 'nn_learning_rate': 0.0002505476925759534, 'batch_size': 128, 'num_epochs': 25}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:10,727] Trial 8 finished with value: 0.375 and parameters: {'n_estimators': 52, 'max_depth': 3, 'lgb_learning_rate': 0.03603541675597387, 'num_leaves': 70, 'subsample': 0.5975970688301913, 'colsample_bytree': 0.7130265047251779, 'hidden_layer_0': 130, 'hidden_layer_1': 140, 'hidden_layer_2': 193, 'nn_learning_rate': 0.00046447336244987716, 'batch_size': 128, 'num_epochs': 38}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:10,994] Trial 9 finished with value: 0.375 and parameters: {'n_estimators': 256, 'max_depth': 9, 'lgb_learning_rate': 0.0005482527019910094, 'num_leaves': 96, 'subsample': 0.5256009648916088, 'colsample_bytree': 0.9713035113771805, 'hidden_layer_0': 203, 'hidden_layer_1': 137, 'hidden_layer_2': 247, 'nn_learning_rate': 0.0010384555266015557, 'batch_size': 32, 'num_epochs': 42}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:11,375] Trial 10 finished with value: 0.25 and parameters: {'n_estimators': 95, 'max_depth': 7, 'lgb_learning_rate': 0.00010007883783423802, 'num_leaves': 51, 'subsample': 0.7728659838818752, 'colsample_bytree': 0.5128506987562584, 'hidden_layer_0': 32, 'hidden_layer_1': 97, 'hidden_layer_2': 73, 'nn_learning_rate': 0.09372371688506563, 'batch_size': 128, 'num_epochs': 68}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:11,585] Trial 11 finished with value: 0.375 and parameters: {'n_estimators': 127, 'max_depth': 7, 'lgb_learning_rate': 0.007173957459639556, 'num_leaves': 82, 'subsample': 0.6800614040408063, 'colsample_bytree': 0.5852411397718272, 'hidden_layer_0': 66, 'hidden_layer_1': 98, 'hidden_layer_2': 207, 'nn_learning_rate': 0.0808821429728747, 'batch_size': 32, 'num_epochs': 22}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:11,958] Trial 12 finished with value: 0.25 and parameters: {'n_estimators': 205, 'max_depth': 10, 'lgb_learning_rate': 0.007375551736030586, 'num_leaves': 84, 'subsample': 0.6487233058281614, 'colsample_bytree': 0.6217810508935546, 'hidden_layer_0': 80, 'hidden_layer_1': 88, 'hidden_layer_2': 103, 'nn_learning_rate': 0.029165665687270748, 'batch_size': 256, 'num_epochs': 61}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:12,298] Trial 13 finished with value: 0.25 and parameters: {'n_estimators': 102, 'max_depth': 6, 'lgb_learning_rate': 0.0069973479433327705, 'num_leaves': 99, 'subsample': 0.7846070490509167, 'colsample_bytree': 0.8852422958695563, 'hidden_layer_0': 90, 'hidden_layer_1': 183, 'hidden_layer_2': 40, 'nn_learning_rate': 0.03208166245568093, 'batch_size': 128, 'num_epochs': 50}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:12,548] Trial 14 finished with value: 0.375 and parameters: {'n_estimators': 238, 'max_depth': 8, 'lgb_learning_rate': 0.0011242032746784252, 'num_leaves': 67, 'subsample': 0.8485218077810216, 'colsample_bytree': 0.6224805854020403, 'hidden_layer_0': 32, 'hidden_layer_1': 241, 'hidden_layer_2': 209, 'nn_learning_rate': 0.034601170956689, 'batch_size': 128, 'num_epochs': 27}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:13,033] Trial 15 finished with value: 0.25 and parameters: {'n_estimators': 181, 'max_depth': 5, 'lgb_learning_rate': 0.00017256247355594112, 'num_leaves': 87, 'subsample': 0.6348558741520534, 'colsample_bytree': 0.6413250387156253, 'hidden_layer_0': 112, 'hidden_layer_1': 120, 'hidden_layer_2': 155, 'nn_learning_rate': 0.012224772258098211, 'batch_size': 256, 'num_epochs': 84}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:13,222] Trial 16 finished with value: 0.375 and parameters: {'n_estimators': 177, 'max_depth': 10, 'lgb_learning_rate': 0.004443500948286934, 'num_leaves': 56, 'subsample': 0.7266110656589441, 'colsample_bytree': 0.5423412262260742, 'hidden_layer_0': 73, 'hidden_layer_1': 175, 'hidden_layer_2': 115, 'nn_learning_rate': 0.0027547684074587745, 'batch_size': 64, 'num_epochs': 13}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:13,663] Trial 17 finished with value: 0.25 and parameters: {'n_estimators': 253, 'max_depth': 8, 'lgb_learning_rate': 0.013773222895002905, 'num_leaves': 75, 'subsample': 0.5993610945891761, 'colsample_bytree': 0.8657673039509162, 'hidden_layer_0': 103, 'hidden_layer_1': 214, 'hidden_layer_2': 223, 'nn_learning_rate': 0.05905866980041131, 'batch_size': 32, 'num_epochs': 59}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:13,965] Trial 18 finished with value: 0.25 and parameters: {'n_estimators': 68, 'max_depth': 5, 'lgb_learning_rate': 0.0016348641122665728, 'num_leaves': 89, 'subsample': 0.647447960839574, 'colsample_bytree': 0.6596803184938218, 'hidden_layer_0': 62, 'hidden_layer_1': 74, 'hidden_layer_2': 191, 'nn_learning_rate': 0.018051811251142417, 'batch_size': 256, 'num_epochs': 44}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:14,229] Trial 19 finished with value: 0.375 and parameters: {'n_estimators': 126, 'max_depth': 8, 'lgb_learning_rate': 0.09623445755228234, 'num_leaves': 76, 'subsample': 0.5601377385802258, 'colsample_bytree': 0.5701265132024069, 'hidden_layer_0': 51, 'hidden_layer_1': 120, 'hidden_layer_2': 171, 'nn_learning_rate': 0.005214117881031498, 'batch_size': 128, 'num_epochs': 32}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:14,439] Trial 20 finished with value: 0.375 and parameters: {'n_estimators': 148, 'max_depth': 7, 'lgb_learning_rate': 0.00040685396267511263, 'num_leaves': 61, 'subsample': 0.8274949808552886, 'colsample_bytree': 0.7499437763657953, 'hidden_layer_0': 165, 'hidden_layer_1': 37, 'hidden_layer_2': 227, 'nn_learning_rate': 0.04934587842896551, 'batch_size': 128, 'num_epochs': 21}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:15,096] Trial 21 finished with value: 0.375 and parameters: {'n_estimators': 218, 'max_depth': 4, 'lgb_learning_rate': 0.08100348534347629, 'num_leaves': 92, 'subsample': 0.5113265994474822, 'colsample_bytree': 0.8306041706714573, 'hidden_layer_0': 139, 'hidden_layer_1': 162, 'hidden_layer_2': 169, 'nn_learning_rate': 0.0023980118111536016, 'batch_size': 256, 'num_epochs': 98}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:16,337] Trial 22 finished with value: 0.375 and parameters: {'n_estimators': 233, 'max_depth': 4, 'lgb_learning_rate': 0.026264400508879036, 'num_leaves': 99, 'subsample': 0.6032177781665184, 'colsample_bytree': 0.9023133981917331, 'hidden_layer_0': 99, 'hidden_layer_1': 158, 'hidden_layer_2': 156, 'nn_learning_rate': 0.000985211637536405, 'batch_size': 256, 'num_epochs': 100}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:16,926] Trial 23 finished with value: 0.375 and parameters: {'n_estimators': 266, 'max_depth': 4, 'lgb_learning_rate': 0.0037874425990602045, 'num_leaves': 92, 'subsample': 0.5530022979578257, 'colsample_bytree': 0.806616985822967, 'hidden_layer_0': 188, 'hidden_layer_1': 213, 'hidden_layer_2': 202, 'nn_learning_rate': 0.0013489152142408846, 'batch_size': 256, 'num_epochs': 80}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:17,485] Trial 24 finished with value: 0.375 and parameters: {'n_estimators': 198, 'max_depth': 6, 'lgb_learning_rate': 0.05627674682876316, 'num_leaves': 78, 'subsample': 0.5081371731998021, 'colsample_bytree': 0.9285771213132018, 'hidden_layer_0': 150, 'hidden_layer_1': 121, 'hidden_layer_2': 133, 'nn_learning_rate': 0.0037463722583148083, 'batch_size': 256, 'num_epochs': 89}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:17,924] Trial 25 finished with value: 0.375 and parameters: {'n_estimators': 230, 'max_depth': 5, 'lgb_learning_rate': 0.012872367172915575, 'num_leaves': 93, 'subsample': 0.6823161550928606, 'colsample_bytree': 0.7542547127199513, 'hidden_layer_0': 121, 'hidden_layer_1': 192, 'hidden_layer_2': 176, 'nn_learning_rate': 0.022292990306568634, 'batch_size': 256, 'num_epochs': 69}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:18,315] Trial 26 finished with value: 0.375 and parameters: {'n_estimators': 189, 'max_depth': 3, 'lgb_learning_rate': 0.003826991189587347, 'num_leaves': 99, 'subsample': 0.6223541681959035, 'colsample_bytree': 0.8405257595228213, 'hidden_layer_0': 54, 'hidden_layer_1': 160, 'hidden_layer_2': 219, 'nn_learning_rate': 0.010560736767986188, 'batch_size': 64, 'num_epochs': 47}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:18,707] Trial 27 finished with value: 0.25 and parameters: {'n_estimators': 275, 'max_depth': 4, 'lgb_learning_rate': 0.03146212803629289, 'num_leaves': 87, 'subsample': 0.5634001113993131, 'colsample_bytree': 0.6620829990544586, 'hidden_layer_0': 89, 'hidden_layer_1': 150, 'hidden_layer_2': 89, 'nn_learning_rate': 0.00010650994726848133, 'batch_size': 32, 'num_epochs': 56}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:19,171] Trial 28 finished with value: 0.25 and parameters: {'n_estimators': 299, 'max_depth': 6, 'lgb_learning_rate': 0.0012151031840681894, 'num_leaves': 72, 'subsample': 0.7312862626169214, 'colsample_bytree': 0.7786584615292467, 'hidden_layer_0': 109, 'hidden_layer_1': 171, 'hidden_layer_2': 191, 'nn_learning_rate': 0.038400102710106285, 'batch_size': 256, 'num_epochs': 65}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:19,355] Trial 29 finished with value: 0.375 and parameters: {'n_estimators': 244, 'max_depth': 7, 'lgb_learning_rate': 0.0007368192932976072, 'num_leaves': 26, 'subsample': 0.6690871714654332, 'colsample_bytree': 0.7233253880240917, 'hidden_layer_0': 128, 'hidden_layer_1': 54, 'hidden_layer_2': 244, 'nn_learning_rate': 0.007608384456651124, 'batch_size': 64, 'num_epochs': 12}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:19,682] Trial 30 finished with value: 0.375 and parameters: {'n_estimators': 162, 'max_depth': 3, 'lgb_learning_rate': 0.00021216953666813447, 'num_leaves': 46, 'subsample': 0.5008683340969677, 'colsample_bytree': 0.6809161467810023, 'hidden_layer_0': 158, 'hidden_layer_1': 199, 'hidden_layer_2': 256, 'nn_learning_rate': 0.002006023395398832, 'batch_size': 128, 'num_epochs': 32}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:19,961] Trial 31 finished with value: 0.375 and parameters: {'n_estimators': 214, 'max_depth': 5, 'lgb_learning_rate': 0.000343179411863547, 'num_leaves': 66, 'subsample': 0.5463551817097777, 'colsample_bytree': 0.7977677387987683, 'hidden_layer_0': 239, 'hidden_layer_1': 227, 'hidden_layer_2': 136, 'nn_learning_rate': 0.00018125854729103007, 'batch_size': 128, 'num_epochs': 21}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:20,238] Trial 32 finished with value: 0.375 and parameters: {'n_estimators': 219, 'max_depth': 5, 'lgb_learning_rate': 0.0010062931152423045, 'num_leaves': 35, 'subsample': 0.5321661074259103, 'colsample_bytree': 0.734297981760575, 'hidden_layer_0': 246, 'hidden_layer_1': 199, 'hidden_layer_2': 119, 'nn_learning_rate': 0.00037597730437771194, 'batch_size': 128, 'num_epochs': 27}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:20,647] Trial 33 finished with value: 0.375 and parameters: {'n_estimators': 281, 'max_depth': 6, 'lgb_learning_rate': 0.0022739476740252446, 'num_leaves': 80, 'subsample': 0.5754572323182072, 'colsample_bytree': 0.8292215951426952, 'hidden_layer_0': 211, 'hidden_layer_1': 185, 'hidden_layer_2': 154, 'nn_learning_rate': 0.0006860353154845884, 'batch_size': 128, 'num_epochs': 39}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:21,053] Trial 34 finished with value: 0.25 and parameters: {'n_estimators': 133, 'max_depth': 4, 'lgb_learning_rate': 0.010820769092394555, 'num_leaves': 60, 'subsample': 0.6166606748195517, 'colsample_bytree': 0.7869344533241622, 'hidden_layer_0': 220, 'hidden_layer_1': 132, 'hidden_layer_2': 165, 'nn_learning_rate': 0.00010488415639743368, 'batch_size': 128, 'num_epochs': 52}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:21,368] Trial 35 finished with value: 0.375 and parameters: {'n_estimators': 197, 'max_depth': 5, 'lgb_learning_rate': 0.05013273737699614, 'num_leaves': 84, 'subsample': 0.5799185685913761, 'colsample_bytree': 0.7029464761883252, 'hidden_layer_0': 256, 'hidden_layer_1': 168, 'hidden_layer_2': 125, 'nn_learning_rate': 0.00019978203959116072, 'batch_size': 256, 'num_epochs': 33}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:21,647] Trial 36 finished with value: 0.25 and parameters: {'n_estimators': 165, 'max_depth': 6, 'lgb_learning_rate': 0.018836368910118435, 'num_leaves': 94, 'subsample': 0.9297435284203068, 'colsample_bytree': 0.7542459059767115, 'hidden_layer_0': 173, 'hidden_layer_1': 252, 'hidden_layer_2': 182, 'nn_learning_rate': 0.004286876758570333, 'batch_size': 64, 'num_epochs': 26}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:21,871] Trial 37 finished with value: 0.25 and parameters: {'n_estimators': 263, 'max_depth': 9, 'lgb_learning_rate': 0.0006255045316460203, 'num_leaves': 89, 'subsample': 0.5386684312286147, 'colsample_bytree': 0.6859197197988874, 'hidden_layer_0': 193, 'hidden_layer_1': 150, 'hidden_layer_2': 145, 'nn_learning_rate': 0.0005939751603566032, 'batch_size': 128, 'num_epochs': 16}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:22,988] Trial 38 finished with value: 0.25 and parameters: {'n_estimators': 113, 'max_depth': 3, 'lgb_learning_rate': 0.0002734230942658305, 'num_leaves': 71, 'subsample': 0.578430591951557, 'colsample_bytree': 0.862748204526536, 'hidden_layer_0': 136, 'hidden_layer_1': 222, 'hidden_layer_2': 106, 'nn_learning_rate': 0.06609061470582059, 'batch_size': 256, 'num_epochs': 41}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:23,271] Trial 39 finished with value: 0.375 and parameters: {'n_estimators': 148, 'max_depth': 4, 'lgb_learning_rate': 0.00012076837938316001, 'num_leaves': 36, 'subsample': 0.5317332021081886, 'colsample_bytree': 0.9809981267194415, 'hidden_layer_0': 46, 'hidden_layer_1': 53, 'hidden_layer_2': 240, 'nn_learning_rate': 0.0015526126560255018, 'batch_size': 32, 'num_epochs': 36}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:23,848] Trial 40 finished with value: 0.375 and parameters: {'n_estimators': 208, 'max_depth': 7, 'lgb_learning_rate': 0.0029107482286498316, 'num_leaves': 96, 'subsample': 0.995846217098215, 'colsample_bytree': 0.9448711857392207, 'hidden_layer_0': 83, 'hidden_layer_1': 181, 'hidden_layer_2': 213, 'nn_learning_rate': 0.09112920604470012, 'batch_size': 128, 'num_epochs': 76}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:24,227] Trial 41 finished with value: 0.375 and parameters: {'n_estimators': 95, 'max_depth': 3, 'lgb_learning_rate': 0.033588902532500964, 'num_leaves': 67, 'subsample': 0.6025954597777767, 'colsample_bytree': 0.710703339424055, 'hidden_layer_0': 122, 'hidden_layer_1': 138, 'hidden_layer_2': 194, 'nn_learning_rate': 0.00042296477768859425, 'batch_size': 128, 'num_epochs': 48}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:24,587] Trial 42 finished with value: 0.25 and parameters: {'n_estimators': 56, 'max_depth': 3, 'lgb_learning_rate': 0.02007526191476051, 'num_leaves': 51, 'subsample': 0.6607437692380673, 'colsample_bytree': 0.5969696251649694, 'hidden_layer_0': 95, 'hidden_layer_1': 109, 'hidden_layer_2': 181, 'nn_learning_rate': 0.0007700353197788623, 'batch_size': 128, 'num_epochs': 37}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:24,912] Trial 43 finished with value: 0.25 and parameters: {'n_estimators': 73, 'max_depth': 4, 'lgb_learning_rate': 0.006120527814637326, 'num_leaves': 83, 'subsample': 0.7033946988672379, 'colsample_bytree': 0.8097961969534013, 'hidden_layer_0': 76, 'hidden_layer_1': 86, 'hidden_layer_2': 207, 'nn_learning_rate': 0.00019955550449302057, 'batch_size': 128, 'num_epochs': 43}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:25,163] Trial 44 finished with value: 0.25 and parameters: {'n_estimators': 225, 'max_depth': 10, 'lgb_learning_rate': 0.05345931173303852, 'num_leaves': 72, 'subsample': 0.5965127058571011, 'colsample_bytree': 0.6692052215874396, 'hidden_layer_0': 111, 'hidden_layer_1': 69, 'hidden_layer_2': 197, 'nn_learning_rate': 0.0002886067105674396, 'batch_size': 128, 'num_epochs': 28}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:25,416] Trial 45 finished with value: 0.25 and parameters: {'n_estimators': 245, 'max_depth': 5, 'lgb_learning_rate': 0.0004464498944941391, 'num_leaves': 59, 'subsample': 0.6298482280940679, 'colsample_bytree': 0.6418510113846818, 'hidden_layer_0': 134, 'hidden_layer_1': 205, 'hidden_layer_2': 234, 'nn_learning_rate': 0.00048174942636793777, 'batch_size': 256, 'num_epochs': 17}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:25,790] Trial 46 finished with value: 0.25 and parameters: {'n_estimators': 78, 'max_depth': 6, 'lgb_learning_rate': 0.009510801583720377, 'num_leaves': 64, 'subsample': 0.5211628185977476, 'colsample_bytree': 0.7730863366001575, 'hidden_layer_0': 169, 'hidden_layer_1': 145, 'hidden_layer_2': 45, 'nn_learning_rate': 0.00029318280039266936, 'batch_size': 128, 'num_epochs': 52}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:26,683] Trial 47 finished with value: 0.25 and parameters: {'n_estimators': 179, 'max_depth': 9, 'lgb_learning_rate': 0.0016778100197626022, 'num_leaves': 54, 'subsample': 0.6945726339776863, 'colsample_bytree': 0.7340095710074102, 'hidden_layer_0': 146, 'hidden_layer_1': 192, 'hidden_layer_2': 164, 'nn_learning_rate': 0.022579975826739977, 'batch_size': 32, 'num_epochs': 45}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:27,634] Trial 48 finished with value: 0.375 and parameters: {'n_estimators': 51, 'max_depth': 3, 'lgb_learning_rate': 0.04350613731361228, 'num_leaves': 87, 'subsample': 0.647892252543757, 'colsample_bytree': 0.6212991141149389, 'hidden_layer_0': 68, 'hidden_layer_1': 233, 'hidden_layer_2': 185, 'nn_learning_rate': 0.007821446893751976, 'batch_size': 256, 'num_epochs': 94}. Best is trial 0 with value: 0.375.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1256629833.py:76: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:27,904] Trial 49 finished with value: 0.375 and parameters: {'n_estimators': 113, 'max_depth': 4, 'lgb_learning_rate': 0.005865934273537366, 'num_leaves': 20, 'subsample': 0.8588704621929674, 'colsample_bytree': 0.7078553378402648, 'hidden_layer_0': 40, 'hidden_layer_1': 177, 'hidden_layer_2': 158, 'nn_learning_rate': 0.05089237501304061, 'batch_size': 128, 'num_epochs': 30}. Best is trial 0 with value: 0.375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 32, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score -1.163151\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Info] Start training from score -1.067841\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "XGBoost + NN           0.875  0.888889                    0.364206   \n",
      "LightGBM + NN          0.375       0.5                    0.253997   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "XGBoost + NN                            0.000997                41.681399   \n",
      "LightGBM + NN                           0.000989                19.848618   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN         {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_estimators: 135\n",
      "max_depth: 6\n",
      "lgb_learning_rate: 0.004344106798222012\n",
      "num_leaves: 87\n",
      "subsample: 0.627133286041994\n",
      "colsample_bytree: 0.678357634226493\n",
      "hidden_layer_0: 102\n",
      "hidden_layer_1: 177\n",
      "hidden_layer_2: 180\n",
      "nn_learning_rate: 0.035613521983106015\n",
      "batch_size: 128\n",
      "num_epochs: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for LightGBM\n",
    "    lgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-4, 1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model\n",
    "    lgb_model = LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using LightGBM\n",
    "    X_train_transformed = lgb_model.predict_proba(X_train_scaled)\n",
    "    X_test_transformed = lgb_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final LightGBM model with the best hyperparameters\n",
    "lgb_best_params = {\n",
    "    'n_estimators': best_params['n_estimators'],\n",
    "    'max_depth': best_params['max_depth'],\n",
    "    'learning_rate': best_params['lgb_learning_rate'],\n",
    "    'num_leaves': best_params['num_leaves'],\n",
    "    'subsample': best_params['subsample'],\n",
    "    'colsample_bytree': best_params['colsample_bytree']\n",
    "}\n",
    "lgb_model = LGBMClassifier(**lgb_best_params)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using LightGBM\n",
    "X_train_transformed = lgb_model.predict_proba(X_train_scaled)\n",
    "X_test_transformed = lgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['LightGBM + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:48:28,289] A new study created in memory with name: no-name-c0bbadbb-c617-46f0-8765-0f91504a61c6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:34,396] Trial 0 finished with value: 0.625 and parameters: {'iterations': 153, 'depth': 9, 'catboost_learning_rate': 0.01773245000576407, 'l2_leaf_reg': 0.003396809801524944, 'border_count': 108, 'hidden_layer_0': 133, 'hidden_layer_1': 104, 'hidden_layer_2': 81, 'nn_learning_rate': 0.03713133597993742, 'batch_size': 64, 'num_epochs': 100}. Best is trial 0 with value: 0.625.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:35,377] Trial 1 finished with value: 0.75 and parameters: {'iterations': 85, 'depth': 8, 'catboost_learning_rate': 0.0521242008728483, 'l2_leaf_reg': 0.0030351544709401293, 'border_count': 142, 'hidden_layer_0': 241, 'hidden_layer_1': 126, 'hidden_layer_2': 85, 'nn_learning_rate': 0.07661611352522985, 'batch_size': 128, 'num_epochs': 61}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:36,242] Trial 2 finished with value: 0.75 and parameters: {'iterations': 210, 'depth': 4, 'catboost_learning_rate': 0.0003310047488189256, 'l2_leaf_reg': 0.019411876826133316, 'border_count': 76, 'hidden_layer_0': 162, 'hidden_layer_1': 59, 'hidden_layer_2': 66, 'nn_learning_rate': 0.008780438117782417, 'batch_size': 32, 'num_epochs': 26}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:36,580] Trial 3 finished with value: 0.5 and parameters: {'iterations': 146, 'depth': 4, 'catboost_learning_rate': 0.00115407341611321, 'l2_leaf_reg': 0.03581870177944969, 'border_count': 199, 'hidden_layer_0': 86, 'hidden_layer_1': 248, 'hidden_layer_2': 84, 'nn_learning_rate': 0.0007621541491149387, 'batch_size': 32, 'num_epochs': 20}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:37,398] Trial 4 finished with value: 0.375 and parameters: {'iterations': 123, 'depth': 6, 'catboost_learning_rate': 0.00010647325398303662, 'l2_leaf_reg': 0.00011900429220192917, 'border_count': 154, 'hidden_layer_0': 78, 'hidden_layer_1': 174, 'hidden_layer_2': 147, 'nn_learning_rate': 0.07947365683823539, 'batch_size': 256, 'num_epochs': 65}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:38,397] Trial 5 finished with value: 0.75 and parameters: {'iterations': 225, 'depth': 6, 'catboost_learning_rate': 0.0015012277829853362, 'l2_leaf_reg': 0.005461493251157666, 'border_count': 176, 'hidden_layer_0': 209, 'hidden_layer_1': 117, 'hidden_layer_2': 222, 'nn_learning_rate': 0.0011757849385282332, 'batch_size': 128, 'num_epochs': 78}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:39,466] Trial 6 finished with value: 0.625 and parameters: {'iterations': 143, 'depth': 8, 'catboost_learning_rate': 0.003158224169815032, 'l2_leaf_reg': 0.08461490844714313, 'border_count': 35, 'hidden_layer_0': 198, 'hidden_layer_1': 144, 'hidden_layer_2': 133, 'nn_learning_rate': 0.06931546753436778, 'batch_size': 64, 'num_epochs': 51}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:40,786] Trial 7 finished with value: 0.5 and parameters: {'iterations': 146, 'depth': 10, 'catboost_learning_rate': 0.00012141858800096862, 'l2_leaf_reg': 0.07219029523753391, 'border_count': 129, 'hidden_layer_0': 227, 'hidden_layer_1': 169, 'hidden_layer_2': 128, 'nn_learning_rate': 0.0002987888429564813, 'batch_size': 64, 'num_epochs': 73}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:41,411] Trial 8 finished with value: 0.375 and parameters: {'iterations': 145, 'depth': 6, 'catboost_learning_rate': 0.00035885342871511074, 'l2_leaf_reg': 0.0007307581117860691, 'border_count': 162, 'hidden_layer_0': 75, 'hidden_layer_1': 120, 'hidden_layer_2': 66, 'nn_learning_rate': 0.0840872107643306, 'batch_size': 256, 'num_epochs': 89}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:42,479] Trial 9 finished with value: 0.75 and parameters: {'iterations': 136, 'depth': 9, 'catboost_learning_rate': 0.011967299943532161, 'l2_leaf_reg': 0.0002425736093490558, 'border_count': 253, 'hidden_layer_0': 107, 'hidden_layer_1': 155, 'hidden_layer_2': 191, 'nn_learning_rate': 0.0009835766888340155, 'batch_size': 64, 'num_epochs': 82}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:43,032] Trial 10 finished with value: 0.75 and parameters: {'iterations': 53, 'depth': 8, 'catboost_learning_rate': 0.07508379731056206, 'l2_leaf_reg': 0.0009395204230015661, 'border_count': 219, 'hidden_layer_0': 244, 'hidden_layer_1': 221, 'hidden_layer_2': 32, 'nn_learning_rate': 0.011117931412533743, 'batch_size': 128, 'num_epochs': 51}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:43,720] Trial 11 finished with value: 0.75 and parameters: {'iterations': 300, 'depth': 3, 'catboost_learning_rate': 0.06848258608554081, 'l2_leaf_reg': 0.01365697854762466, 'border_count': 74, 'hidden_layer_0': 171, 'hidden_layer_1': 41, 'hidden_layer_2': 32, 'nn_learning_rate': 0.011131968851017148, 'batch_size': 32, 'num_epochs': 28}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:44,247] Trial 12 finished with value: 0.75 and parameters: {'iterations': 213, 'depth': 4, 'catboost_learning_rate': 0.010654279050178572, 'l2_leaf_reg': 0.013014722991628088, 'border_count': 86, 'hidden_layer_0': 33, 'hidden_layer_1': 51, 'hidden_layer_2': 106, 'nn_learning_rate': 0.009529045219621599, 'batch_size': 128, 'num_epochs': 37}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:44,659] Trial 13 finished with value: 0.25 and parameters: {'iterations': 65, 'depth': 7, 'catboost_learning_rate': 0.00045876107260889846, 'l2_leaf_reg': 0.0010100812251156686, 'border_count': 39, 'hidden_layer_0': 164, 'hidden_layer_1': 79, 'hidden_layer_2': 58, 'nn_learning_rate': 0.0035502973267690766, 'batch_size': 32, 'num_epochs': 39}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:45,142] Trial 14 finished with value: 0.75 and parameters: {'iterations': 201, 'depth': 5, 'catboost_learning_rate': 0.027357261126634762, 'l2_leaf_reg': 0.00855028906109666, 'border_count': 120, 'hidden_layer_0': 256, 'hidden_layer_1': 75, 'hidden_layer_2': 165, 'nn_learning_rate': 0.026318736152535438, 'batch_size': 32, 'num_epochs': 14}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:45,632] Trial 15 finished with value: 0.75 and parameters: {'iterations': 261, 'depth': 3, 'catboost_learning_rate': 0.0036572288080578743, 'l2_leaf_reg': 0.028895699164296802, 'border_count': 74, 'hidden_layer_0': 183, 'hidden_layer_1': 82, 'hidden_layer_2': 104, 'nn_learning_rate': 0.0024328837924661104, 'batch_size': 128, 'num_epochs': 62}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:46,003] Trial 16 finished with value: 0.25 and parameters: {'iterations': 90, 'depth': 7, 'catboost_learning_rate': 0.000356577293782595, 'l2_leaf_reg': 0.00206880133470581, 'border_count': 100, 'hidden_layer_0': 138, 'hidden_layer_1': 32, 'hidden_layer_2': 102, 'nn_learning_rate': 0.00012001212925744016, 'batch_size': 128, 'num_epochs': 41}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:46,237] Trial 17 finished with value: 0.75 and parameters: {'iterations': 180, 'depth': 4, 'catboost_learning_rate': 0.006076213044611063, 'l2_leaf_reg': 0.0025802906869718347, 'border_count': 62, 'hidden_layer_0': 218, 'hidden_layer_1': 200, 'hidden_layer_2': 54, 'nn_learning_rate': 0.030525823698612096, 'batch_size': 32, 'num_epochs': 10}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:46,528] Trial 18 finished with value: 0.75 and parameters: {'iterations': 106, 'depth': 5, 'catboost_learning_rate': 0.0011520759669696732, 'l2_leaf_reg': 0.016984603046990992, 'border_count': 137, 'hidden_layer_0': 115, 'hidden_layer_1': 99, 'hidden_layer_2': 239, 'nn_learning_rate': 0.004828652979483877, 'batch_size': 256, 'num_epochs': 28}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:47,617] Trial 19 finished with value: 0.75 and parameters: {'iterations': 243, 'depth': 8, 'catboost_learning_rate': 0.03826568530546307, 'l2_leaf_reg': 0.004435111064462489, 'border_count': 185, 'hidden_layer_0': 157, 'hidden_layer_1': 127, 'hidden_layer_2': 165, 'nn_learning_rate': 0.02081322200017155, 'batch_size': 128, 'num_epochs': 57}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:48,617] Trial 20 finished with value: 0.75 and parameters: {'iterations': 185, 'depth': 10, 'catboost_learning_rate': 0.00018740028426555898, 'l2_leaf_reg': 0.032033288354458817, 'border_count': 211, 'hidden_layer_0': 190, 'hidden_layer_1': 54, 'hidden_layer_2': 78, 'nn_learning_rate': 0.006242052348980412, 'batch_size': 32, 'num_epochs': 46}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:49,278] Trial 21 finished with value: 0.75 and parameters: {'iterations': 231, 'depth': 5, 'catboost_learning_rate': 0.0012341198467767536, 'l2_leaf_reg': 0.0059268018264112335, 'border_count': 171, 'hidden_layer_0': 219, 'hidden_layer_1': 125, 'hidden_layer_2': 219, 'nn_learning_rate': 0.0011748327646173907, 'batch_size': 128, 'num_epochs': 71}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:50,096] Trial 22 finished with value: 0.75 and parameters: {'iterations': 276, 'depth': 6, 'catboost_learning_rate': 0.0018166463101327182, 'l2_leaf_reg': 0.001637151749462902, 'border_count': 234, 'hidden_layer_0': 202, 'hidden_layer_1': 102, 'hidden_layer_2': 244, 'nn_learning_rate': 0.002234771729201057, 'batch_size': 128, 'num_epochs': 84}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:52,829] Trial 23 finished with value: 0.75 and parameters: {'iterations': 214, 'depth': 7, 'catboost_learning_rate': 0.0004762007414172079, 'l2_leaf_reg': 0.007906426828497645, 'border_count': 183, 'hidden_layer_0': 231, 'hidden_layer_1': 66, 'hidden_layer_2': 203, 'nn_learning_rate': 0.00048164093459611114, 'batch_size': 128, 'num_epochs': 74}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:54,713] Trial 24 finished with value: 0.75 and parameters: {'iterations': 239, 'depth': 9, 'catboost_learning_rate': 0.0006861432186307456, 'l2_leaf_reg': 0.00037401461034902527, 'border_count': 145, 'hidden_layer_0': 209, 'hidden_layer_1': 149, 'hidden_layer_2': 117, 'nn_learning_rate': 0.0014851437944379028, 'batch_size': 128, 'num_epochs': 62}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:55,414] Trial 25 finished with value: 0.75 and parameters: {'iterations': 173, 'depth': 5, 'catboost_learning_rate': 0.002423380826483131, 'l2_leaf_reg': 0.0039810714718434195, 'border_count': 110, 'hidden_layer_0': 242, 'hidden_layer_1': 192, 'hidden_layer_2': 164, 'nn_learning_rate': 0.0004396872792693022, 'batch_size': 128, 'num_epochs': 90}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:56,174] Trial 26 finished with value: 0.75 and parameters: {'iterations': 261, 'depth': 8, 'catboost_learning_rate': 0.005661866774291643, 'l2_leaf_reg': 0.008563480727734411, 'border_count': 174, 'hidden_layer_0': 184, 'hidden_layer_1': 92, 'hidden_layer_2': 183, 'nn_learning_rate': 0.0001905278662848331, 'batch_size': 32, 'num_epochs': 31}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:56,896] Trial 27 finished with value: 0.5 and parameters: {'iterations': 80, 'depth': 6, 'catboost_learning_rate': 0.00019610046483255976, 'l2_leaf_reg': 0.02068684799603138, 'border_count': 196, 'hidden_layer_0': 255, 'hidden_layer_1': 135, 'hidden_layer_2': 224, 'nn_learning_rate': 0.016042754769093538, 'batch_size': 256, 'num_epochs': 77}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:57,402] Trial 28 finished with value: 0.75 and parameters: {'iterations': 196, 'depth': 3, 'catboost_learning_rate': 0.0007189277663883943, 'l2_leaf_reg': 0.05469788219129917, 'border_count': 49, 'hidden_layer_0': 153, 'hidden_layer_1': 112, 'hidden_layer_2': 47, 'nn_learning_rate': 0.005330004848632665, 'batch_size': 128, 'num_epochs': 68}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:58,293] Trial 29 finished with value: 0.625 and parameters: {'iterations': 163, 'depth': 7, 'catboost_learning_rate': 0.0068007335380400645, 'l2_leaf_reg': 0.0014268880981589602, 'border_count': 98, 'hidden_layer_0': 171, 'hidden_layer_1': 63, 'hidden_layer_2': 86, 'nn_learning_rate': 0.048604109300155955, 'batch_size': 64, 'num_epochs': 99}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:58,801] Trial 30 finished with value: 0.75 and parameters: {'iterations': 221, 'depth': 4, 'catboost_learning_rate': 0.021791210485396968, 'l2_leaf_reg': 0.00308989590843153, 'border_count': 117, 'hidden_layer_0': 121, 'hidden_layer_1': 110, 'hidden_layer_2': 253, 'nn_learning_rate': 0.048348015116819645, 'batch_size': 32, 'num_epochs': 54}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:48:59,707] Trial 31 finished with value: 0.75 and parameters: {'iterations': 115, 'depth': 9, 'catboost_learning_rate': 0.0416597996870187, 'l2_leaf_reg': 0.00016274209971536472, 'border_count': 226, 'hidden_layer_0': 102, 'hidden_layer_1': 161, 'hidden_layer_2': 207, 'nn_learning_rate': 0.000992768747708989, 'batch_size': 64, 'num_epochs': 80}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:00,700] Trial 32 finished with value: 0.75 and parameters: {'iterations': 98, 'depth': 9, 'catboost_learning_rate': 0.011373269679123523, 'l2_leaf_reg': 0.0004077476463556702, 'border_count': 254, 'hidden_layer_0': 57, 'hidden_layer_1': 156, 'hidden_layer_2': 190, 'nn_learning_rate': 0.000639637675259496, 'batch_size': 64, 'num_epochs': 96}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:05,354] Trial 33 finished with value: 0.75 and parameters: {'iterations': 197, 'depth': 10, 'catboost_learning_rate': 0.0408665701964169, 'l2_leaf_reg': 0.000171378131056832, 'border_count': 249, 'hidden_layer_0': 132, 'hidden_layer_1': 184, 'hidden_layer_2': 184, 'nn_learning_rate': 0.0018252800849706042, 'batch_size': 64, 'num_epochs': 86}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:06,076] Trial 34 finished with value: 0.75 and parameters: {'iterations': 131, 'depth': 8, 'catboost_learning_rate': 0.011682109310124523, 'l2_leaf_reg': 0.000390658929550256, 'border_count': 201, 'hidden_layer_0': 102, 'hidden_layer_1': 140, 'hidden_layer_2': 144, 'nn_learning_rate': 0.003273677409844758, 'batch_size': 64, 'num_epochs': 80}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:06,910] Trial 35 finished with value: 0.75 and parameters: {'iterations': 164, 'depth': 9, 'catboost_learning_rate': 0.018050089675416867, 'l2_leaf_reg': 0.044881205857802156, 'border_count': 150, 'hidden_layer_0': 97, 'hidden_layer_1': 232, 'hidden_layer_2': 83, 'nn_learning_rate': 0.0003101662374922996, 'batch_size': 64, 'num_epochs': 66}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:07,728] Trial 36 finished with value: 0.75 and parameters: {'iterations': 80, 'depth': 9, 'catboost_learning_rate': 0.09823768568683651, 'l2_leaf_reg': 0.0005868341659371997, 'border_count': 131, 'hidden_layer_0': 236, 'hidden_layer_1': 176, 'hidden_layer_2': 224, 'nn_learning_rate': 0.0008365803169843731, 'batch_size': 256, 'num_epochs': 93}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:08,473] Trial 37 finished with value: 0.75 and parameters: {'iterations': 129, 'depth': 8, 'catboost_learning_rate': 0.003908917645368271, 'l2_leaf_reg': 0.005042933929649887, 'border_count': 157, 'hidden_layer_0': 212, 'hidden_layer_1': 209, 'hidden_layer_2': 72, 'nn_learning_rate': 0.007699066646572124, 'batch_size': 64, 'num_epochs': 46}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:09,103] Trial 38 finished with value: 0.625 and parameters: {'iterations': 118, 'depth': 10, 'catboost_learning_rate': 0.0022380355471692454, 'l2_leaf_reg': 0.0996940815138528, 'border_count': 241, 'hidden_layer_0': 126, 'hidden_layer_1': 91, 'hidden_layer_2': 93, 'nn_learning_rate': 0.09111053224077371, 'batch_size': 32, 'num_epochs': 18}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:09,563] Trial 39 finished with value: 0.5 and parameters: {'iterations': 50, 'depth': 6, 'catboost_learning_rate': 0.00020258358945527256, 'l2_leaf_reg': 0.00010591466005517058, 'border_count': 213, 'hidden_layer_0': 152, 'hidden_layer_1': 165, 'hidden_layer_2': 127, 'nn_learning_rate': 0.0013394692782779706, 'batch_size': 128, 'num_epochs': 59}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:10,276] Trial 40 finished with value: 0.625 and parameters: {'iterations': 155, 'depth': 8, 'catboost_learning_rate': 0.0012180092275318635, 'l2_leaf_reg': 0.021906157169546435, 'border_count': 84, 'hidden_layer_0': 71, 'hidden_layer_1': 143, 'hidden_layer_2': 150, 'nn_learning_rate': 0.01642077584870357, 'batch_size': 128, 'num_epochs': 70}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:10,765] Trial 41 finished with value: 0.75 and parameters: {'iterations': 69, 'depth': 7, 'catboost_learning_rate': 0.08024339708875916, 'l2_leaf_reg': 0.001139441527818538, 'border_count': 229, 'hidden_layer_0': 244, 'hidden_layer_1': 256, 'hidden_layer_2': 38, 'nn_learning_rate': 0.04500276511152998, 'batch_size': 128, 'num_epochs': 51}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:11,107] Trial 42 finished with value: 0.75 and parameters: {'iterations': 54, 'depth': 8, 'catboost_learning_rate': 0.05421380259686242, 'l2_leaf_reg': 0.0007028346010039341, 'border_count': 214, 'hidden_layer_0': 227, 'hidden_layer_1': 238, 'hidden_layer_2': 63, 'nn_learning_rate': 0.011562004969617585, 'batch_size': 128, 'num_epochs': 23}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:11,682] Trial 43 finished with value: 0.75 and parameters: {'iterations': 64, 'depth': 9, 'catboost_learning_rate': 0.029175257387176808, 'l2_leaf_reg': 0.00025436687610179166, 'border_count': 220, 'hidden_layer_0': 247, 'hidden_layer_1': 219, 'hidden_layer_2': 59, 'nn_learning_rate': 0.00381963136684616, 'batch_size': 128, 'num_epochs': 35}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:12,188] Trial 44 finished with value: 0.75 and parameters: {'iterations': 86, 'depth': 8, 'catboost_learning_rate': 0.05796442990523998, 'l2_leaf_reg': 0.011470165661512917, 'border_count': 241, 'hidden_layer_0': 199, 'hidden_layer_1': 120, 'hidden_layer_2': 44, 'nn_learning_rate': 0.0024632335595845454, 'batch_size': 32, 'num_epochs': 45}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:12,928] Trial 45 finished with value: 0.75 and parameters: {'iterations': 140, 'depth': 7, 'catboost_learning_rate': 0.014086976988950511, 'l2_leaf_reg': 0.0020953456976649, 'border_count': 198, 'hidden_layer_0': 174, 'hidden_layer_1': 177, 'hidden_layer_2': 36, 'nn_learning_rate': 0.0006297471319382771, 'batch_size': 128, 'num_epochs': 84}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:13,455] Trial 46 finished with value: 0.75 and parameters: {'iterations': 105, 'depth': 5, 'catboost_learning_rate': 0.00835292995618935, 'l2_leaf_reg': 0.006587903946086731, 'border_count': 167, 'hidden_layer_0': 224, 'hidden_layer_1': 130, 'hidden_layer_2': 50, 'nn_learning_rate': 0.008355866422165585, 'batch_size': 256, 'num_epochs': 77}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:13,979] Trial 47 finished with value: 0.75 and parameters: {'iterations': 209, 'depth': 4, 'catboost_learning_rate': 0.027550502657109757, 'l2_leaf_reg': 0.0010487308340600074, 'border_count': 61, 'hidden_layer_0': 256, 'hidden_layer_1': 151, 'hidden_layer_2': 72, 'nn_learning_rate': 0.06634654260499341, 'batch_size': 32, 'num_epochs': 65}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:14,288] Trial 48 finished with value: 0.375 and parameters: {'iterations': 71, 'depth': 6, 'catboost_learning_rate': 0.00010975002213351968, 'l2_leaf_reg': 0.0029571774270618343, 'border_count': 179, 'hidden_layer_0': 141, 'hidden_layer_1': 32, 'hidden_layer_2': 209, 'nn_learning_rate': 0.02831769403579642, 'batch_size': 128, 'num_epochs': 24}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3359856436.py:75: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:16,448] Trial 49 finished with value: 0.75 and parameters: {'iterations': 226, 'depth': 10, 'catboost_learning_rate': 0.0002847920073429777, 'l2_leaf_reg': 0.010863977681844493, 'border_count': 242, 'hidden_layer_0': 238, 'hidden_layer_1': 213, 'hidden_layer_2': 232, 'nn_learning_rate': 0.011792744384769469, 'batch_size': 64, 'num_epochs': 50}. Best is trial 1 with value: 0.75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "XGBoost + NN           0.875  0.888889                    0.364206   \n",
      "LightGBM + NN          0.375       0.5                    0.253997   \n",
      "CatBoost + NN           0.75  0.755556                    0.277767   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "XGBoost + NN                            0.000997                41.681399   \n",
      "LightGBM + NN                           0.000989                19.848618   \n",
      "CatBoost + NN                           0.000998                 48.66849   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN         {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "iterations: 85\n",
      "depth: 8\n",
      "catboost_learning_rate: 0.0521242008728483\n",
      "l2_leaf_reg: 0.0030351544709401293\n",
      "border_count: 142\n",
      "hidden_layer_0: 241\n",
      "hidden_layer_1: 126\n",
      "hidden_layer_2: 85\n",
      "nn_learning_rate: 0.07661611352522985\n",
      "batch_size: 128\n",
      "num_epochs: 61\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for CatBoost\n",
    "    catboost_params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('catboost_learning_rate', 1e-4, 1e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 1e-1),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255)\n",
    "    }\n",
    "\n",
    "    # Train CatBoost model\n",
    "    catboost_model = CatBoostClassifier(**catboost_params, verbose=0)\n",
    "    catboost_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Extract features using CatBoost\n",
    "    X_train_transformed = catboost_model.predict_proba(X_train_scaled)\n",
    "    X_test_transformed = catboost_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final CatBoost model with the best hyperparameters\n",
    "catboost_best_params = {\n",
    "    'iterations': best_params['iterations'],\n",
    "    'depth': best_params['depth'],\n",
    "    'learning_rate': best_params['catboost_learning_rate'],\n",
    "    'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "    'border_count': best_params['border_count']\n",
    "}\n",
    "catboost_model = CatBoostClassifier(**catboost_best_params, verbose=0)\n",
    "catboost_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract features using CatBoost\n",
    "X_train_transformed = catboost_model.predict_proba(X_train_scaled)\n",
    "X_test_transformed = catboost_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['CatBoost + NN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:49:17,041] A new study created in memory with name: no-name-f2c415c6-28ee-431f-8031-586e30dec0b2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:17,308] Trial 0 finished with value: 0.625 and parameters: {'num_heads': 1, 'embedding_dim': 60, 'num_layers': 3, 'hidden_layer_0': 59, 'hidden_layer_1': 248, 'hidden_layer_2': 107, 'nn_learning_rate': 0.00014720027560375466, 'batch_size': 32, 'num_epochs': 17}. Best is trial 0 with value: 0.625.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:17,858] Trial 1 finished with value: 0.875 and parameters: {'num_heads': 3, 'embedding_dim': 39, 'num_layers': 1, 'hidden_layer_0': 176, 'hidden_layer_1': 147, 'hidden_layer_2': 66, 'nn_learning_rate': 0.04480925362642859, 'batch_size': 256, 'num_epochs': 97}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:18,646] Trial 2 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 160, 'hidden_layer_1': 255, 'hidden_layer_2': 172, 'nn_learning_rate': 0.006204886951343538, 'batch_size': 32, 'num_epochs': 88}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:19,148] Trial 3 finished with value: 0.875 and parameters: {'num_heads': 6, 'embedding_dim': 6, 'num_layers': 3, 'hidden_layer_0': 236, 'hidden_layer_1': 37, 'hidden_layer_2': 169, 'nn_learning_rate': 0.011194508786349998, 'batch_size': 32, 'num_epochs': 96}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:19,610] Trial 4 finished with value: 0.875 and parameters: {'num_heads': 2, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 176, 'hidden_layer_1': 192, 'hidden_layer_2': 79, 'nn_learning_rate': 0.019886186800920372, 'batch_size': 32, 'num_epochs': 79}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:19,802] Trial 5 finished with value: 0.625 and parameters: {'num_heads': 4, 'embedding_dim': 20, 'num_layers': 2, 'hidden_layer_0': 215, 'hidden_layer_1': 74, 'hidden_layer_2': 216, 'nn_learning_rate': 0.0006581262771069041, 'batch_size': 32, 'num_epochs': 25}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:20,176] Trial 6 finished with value: 0.75 and parameters: {'num_heads': 3, 'embedding_dim': 33, 'num_layers': 1, 'hidden_layer_0': 172, 'hidden_layer_1': 245, 'hidden_layer_2': 252, 'nn_learning_rate': 0.005557820426002593, 'batch_size': 256, 'num_epochs': 47}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:20,331] Trial 7 finished with value: 0.875 and parameters: {'num_heads': 6, 'embedding_dim': 18, 'num_layers': 3, 'hidden_layer_0': 190, 'hidden_layer_1': 149, 'hidden_layer_2': 40, 'nn_learning_rate': 0.02364646730920074, 'batch_size': 64, 'num_epochs': 17}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:20,859] Trial 8 finished with value: 0.875 and parameters: {'num_heads': 8, 'embedding_dim': 32, 'num_layers': 2, 'hidden_layer_0': 106, 'hidden_layer_1': 197, 'hidden_layer_2': 217, 'nn_learning_rate': 0.00842007497867443, 'batch_size': 128, 'num_epochs': 70}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:21,099] Trial 9 finished with value: 0.75 and parameters: {'num_heads': 1, 'embedding_dim': 55, 'num_layers': 1, 'hidden_layer_0': 114, 'hidden_layer_1': 112, 'hidden_layer_2': 192, 'nn_learning_rate': 0.001036523072943866, 'batch_size': 256, 'num_epochs': 40}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:21,541] Trial 10 finished with value: 0.75 and parameters: {'num_heads': 4, 'embedding_dim': 64, 'num_layers': 1, 'hidden_layer_0': 253, 'hidden_layer_1': 143, 'hidden_layer_2': 32, 'nn_learning_rate': 0.08759648952683392, 'batch_size': 256, 'num_epochs': 62}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:22,106] Trial 11 finished with value: 0.5 and parameters: {'num_heads': 6, 'embedding_dim': 6, 'num_layers': 3, 'hidden_layer_0': 256, 'hidden_layer_1': 33, 'hidden_layer_2': 129, 'nn_learning_rate': 0.08808189239005719, 'batch_size': 128, 'num_epochs': 99}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:22,652] Trial 12 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 5, 'num_layers': 1, 'hidden_layer_0': 211, 'hidden_layer_1': 32, 'hidden_layer_2': 157, 'nn_learning_rate': 0.027536479277163732, 'batch_size': 64, 'num_epochs': 98}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:23,264] Trial 13 finished with value: 0.75 and parameters: {'num_heads': 3, 'embedding_dim': 24, 'num_layers': 3, 'hidden_layer_0': 129, 'hidden_layer_1': 92, 'hidden_layer_2': 93, 'nn_learning_rate': 0.0014421931318814665, 'batch_size': 256, 'num_epochs': 83}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:23,794] Trial 14 finished with value: 0.625 and parameters: {'num_heads': 5, 'embedding_dim': 40, 'num_layers': 2, 'hidden_layer_0': 230, 'hidden_layer_1': 164, 'hidden_layer_2': 137, 'nn_learning_rate': 0.03917075800857624, 'batch_size': 32, 'num_epochs': 71}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:24,367] Trial 15 finished with value: 0.625 and parameters: {'num_heads': 8, 'embedding_dim': 48, 'num_layers': 3, 'hidden_layer_0': 74, 'hidden_layer_1': 68, 'hidden_layer_2': 65, 'nn_learning_rate': 0.010811123063843199, 'batch_size': 256, 'num_epochs': 92}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:24,854] Trial 16 finished with value: 0.875 and parameters: {'num_heads': 3, 'embedding_dim': 15, 'num_layers': 1, 'hidden_layer_0': 147, 'hidden_layer_1': 124, 'hidden_layer_2': 118, 'nn_learning_rate': 0.0030103283634895237, 'batch_size': 64, 'num_epochs': 77}. Best is trial 1 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:25,321] Trial 17 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 24, 'num_layers': 2, 'hidden_layer_0': 198, 'hidden_layer_1': 181, 'hidden_layer_2': 175, 'nn_learning_rate': 0.03688655483588923, 'batch_size': 128, 'num_epochs': 56}. Best is trial 1 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:25,889] Trial 18 finished with value: 1.0 and parameters: {'num_heads': 2, 'embedding_dim': 12, 'num_layers': 2, 'hidden_layer_0': 231, 'hidden_layer_1': 111, 'hidden_layer_2': 65, 'nn_learning_rate': 0.00282538200280168, 'batch_size': 256, 'num_epochs': 90}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:26,436] Trial 19 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 30, 'num_layers': 1, 'hidden_layer_0': 199, 'hidden_layer_1': 118, 'hidden_layer_2': 63, 'nn_learning_rate': 0.00032730708209926314, 'batch_size': 256, 'num_epochs': 87}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:26,742] Trial 20 finished with value: 0.5 and parameters: {'num_heads': 2, 'embedding_dim': 12, 'num_layers': 2, 'hidden_layer_0': 224, 'hidden_layer_1': 221, 'hidden_layer_2': 57, 'nn_learning_rate': 0.0025210923097938537, 'batch_size': 256, 'num_epochs': 36}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:27,279] Trial 21 finished with value: 0.875 and parameters: {'num_heads': 4, 'embedding_dim': 8, 'num_layers': 3, 'hidden_layer_0': 232, 'hidden_layer_1': 66, 'hidden_layer_2': 95, 'nn_learning_rate': 0.004046186431469917, 'batch_size': 256, 'num_epochs': 98}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:27,937] Trial 22 finished with value: 0.5 and parameters: {'num_heads': 3, 'embedding_dim': 9, 'num_layers': 3, 'hidden_layer_0': 248, 'hidden_layer_1': 99, 'hidden_layer_2': 152, 'nn_learning_rate': 0.013107363674722946, 'batch_size': 32, 'num_epochs': 91}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:28,437] Trial 23 finished with value: 0.5 and parameters: {'num_heads': 2, 'embedding_dim': 2, 'num_layers': 2, 'hidden_layer_0': 182, 'hidden_layer_1': 141, 'hidden_layer_2': 79, 'nn_learning_rate': 0.06373037611308183, 'batch_size': 256, 'num_epochs': 74}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:28,899] Trial 24 finished with value: 0.875 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 1, 'hidden_layer_0': 233, 'hidden_layer_1': 48, 'hidden_layer_2': 200, 'nn_learning_rate': 0.015019396399474612, 'batch_size': 256, 'num_epochs': 63}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:29,434] Trial 25 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 25, 'num_layers': 2, 'hidden_layer_0': 147, 'hidden_layer_1': 86, 'hidden_layer_2': 49, 'nn_learning_rate': 0.0015797108424029126, 'batch_size': 32, 'num_epochs': 93}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:29,968] Trial 26 finished with value: 0.5 and parameters: {'num_heads': 1, 'embedding_dim': 2, 'num_layers': 2, 'hidden_layer_0': 207, 'hidden_layer_1': 169, 'hidden_layer_2': 114, 'nn_learning_rate': 0.046671287308613085, 'batch_size': 128, 'num_epochs': 83}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:30,745] Trial 27 finished with value: 1.0 and parameters: {'num_heads': 7, 'embedding_dim': 14, 'num_layers': 3, 'hidden_layer_0': 240, 'hidden_layer_1': 129, 'hidden_layer_2': 80, 'nn_learning_rate': 0.009421148949368586, 'batch_size': 64, 'num_epochs': 100}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:31,236] Trial 28 finished with value: 0.875 and parameters: {'num_heads': 7, 'embedding_dim': 14, 'num_layers': 1, 'hidden_layer_0': 167, 'hidden_layer_1': 137, 'hidden_layer_2': 79, 'nn_learning_rate': 0.00048543183603786674, 'batch_size': 64, 'num_epochs': 83}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:31,793] Trial 29 finished with value: 0.75 and parameters: {'num_heads': 1, 'embedding_dim': 28, 'num_layers': 3, 'hidden_layer_0': 62, 'hidden_layer_1': 105, 'hidden_layer_2': 101, 'nn_learning_rate': 0.0021479561165035498, 'batch_size': 64, 'num_epochs': 99}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:32,197] Trial 30 finished with value: 0.5 and parameters: {'num_heads': 3, 'embedding_dim': 36, 'num_layers': 2, 'hidden_layer_0': 43, 'hidden_layer_1': 122, 'hidden_layer_2': 71, 'nn_learning_rate': 0.00011664841758138296, 'batch_size': 64, 'num_epochs': 68}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:32,865] Trial 31 finished with value: 0.625 and parameters: {'num_heads': 7, 'embedding_dim': 14, 'num_layers': 3, 'hidden_layer_0': 244, 'hidden_layer_1': 152, 'hidden_layer_2': 48, 'nn_learning_rate': 0.00750525941606651, 'batch_size': 64, 'num_epochs': 93}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:33,565] Trial 32 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 18, 'num_layers': 3, 'hidden_layer_0': 239, 'hidden_layer_1': 131, 'hidden_layer_2': 167, 'nn_learning_rate': 0.005381152758270259, 'batch_size': 32, 'num_epochs': 86}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:34,219] Trial 33 finished with value: 0.875 and parameters: {'num_heads': 7, 'embedding_dim': 49, 'num_layers': 3, 'hidden_layer_0': 223, 'hidden_layer_1': 51, 'hidden_layer_2': 91, 'nn_learning_rate': 0.01894042954149547, 'batch_size': 32, 'num_epochs': 100}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:34,826] Trial 34 finished with value: 0.875 and parameters: {'num_heads': 8, 'embedding_dim': 40, 'num_layers': 3, 'hidden_layer_0': 216, 'hidden_layer_1': 81, 'hidden_layer_2': 130, 'nn_learning_rate': 0.0046110327817702424, 'batch_size': 256, 'num_epochs': 92}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:35,454] Trial 35 finished with value: 1.0 and parameters: {'num_heads': 2, 'embedding_dim': 10, 'num_layers': 3, 'hidden_layer_0': 184, 'hidden_layer_1': 206, 'hidden_layer_2': 110, 'nn_learning_rate': 0.00948973812547069, 'batch_size': 32, 'num_epochs': 79}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:36,070] Trial 36 finished with value: 1.0 and parameters: {'num_heads': 2, 'embedding_dim': 10, 'num_layers': 2, 'hidden_layer_0': 160, 'hidden_layer_1': 213, 'hidden_layer_2': 113, 'nn_learning_rate': 0.008441279493108142, 'batch_size': 32, 'num_epochs': 79}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:36,654] Trial 37 finished with value: 0.625 and parameters: {'num_heads': 2, 'embedding_dim': 12, 'num_layers': 2, 'hidden_layer_0': 156, 'hidden_layer_1': 221, 'hidden_layer_2': 111, 'nn_learning_rate': 0.008423131072479476, 'batch_size': 32, 'num_epochs': 79}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:37,114] Trial 38 finished with value: 0.875 and parameters: {'num_heads': 1, 'embedding_dim': 21, 'num_layers': 2, 'hidden_layer_0': 128, 'hidden_layer_1': 214, 'hidden_layer_2': 83, 'nn_learning_rate': 0.0034448443003156745, 'batch_size': 32, 'num_epochs': 64}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:37,566] Trial 39 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 10, 'num_layers': 2, 'hidden_layer_0': 186, 'hidden_layer_1': 233, 'hidden_layer_2': 123, 'nn_learning_rate': 0.006806144597268373, 'batch_size': 32, 'num_epochs': 55}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:38,140] Trial 40 finished with value: 0.75 and parameters: {'num_heads': 4, 'embedding_dim': 20, 'num_layers': 2, 'hidden_layer_0': 162, 'hidden_layer_1': 202, 'hidden_layer_2': 108, 'nn_learning_rate': 0.010586409049217968, 'batch_size': 32, 'num_epochs': 76}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:38,766] Trial 41 finished with value: 0.75 and parameters: {'num_heads': 3, 'embedding_dim': 6, 'num_layers': 2, 'hidden_layer_0': 190, 'hidden_layer_1': 251, 'hidden_layer_2': 72, 'nn_learning_rate': 0.017367353782065692, 'batch_size': 64, 'num_epochs': 88}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:39,299] Trial 42 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 16, 'num_layers': 3, 'hidden_layer_0': 171, 'hidden_layer_1': 179, 'hidden_layer_2': 49, 'nn_learning_rate': 0.002010848752973225, 'batch_size': 32, 'num_epochs': 81}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:39,891] Trial 43 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 36, 'num_layers': 1, 'hidden_layer_0': 133, 'hidden_layer_1': 157, 'hidden_layer_2': 88, 'nn_learning_rate': 0.026503565583616155, 'batch_size': 256, 'num_epochs': 95}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:40,850] Trial 44 finished with value: 0.25 and parameters: {'num_heads': 1, 'embedding_dim': 1, 'num_layers': 3, 'hidden_layer_0': 199, 'hidden_layer_1': 238, 'hidden_layer_2': 101, 'nn_learning_rate': 0.0009800170560690304, 'batch_size': 128, 'num_epochs': 11}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:41,412] Trial 45 finished with value: 0.75 and parameters: {'num_heads': 3, 'embedding_dim': 9, 'num_layers': 1, 'hidden_layer_0': 108, 'hidden_layer_1': 205, 'hidden_layer_2': 59, 'nn_learning_rate': 0.00635536679657024, 'batch_size': 256, 'num_epochs': 89}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:42,192] Trial 46 finished with value: 0.875 and parameters: {'num_heads': 4, 'embedding_dim': 52, 'num_layers': 2, 'hidden_layer_0': 178, 'hidden_layer_1': 187, 'hidden_layer_2': 34, 'nn_learning_rate': 0.00980428897644707, 'batch_size': 32, 'num_epochs': 87}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:42,831] Trial 47 finished with value: 0.875 and parameters: {'num_heads': 1, 'embedding_dim': 45, 'num_layers': 3, 'hidden_layer_0': 159, 'hidden_layer_1': 109, 'hidden_layer_2': 252, 'nn_learning_rate': 0.02383620133912525, 'batch_size': 64, 'num_epochs': 73}. Best is trial 18 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:43,522] Trial 48 finished with value: 0.625 and parameters: {'num_heads': 3, 'embedding_dim': 12, 'num_layers': 2, 'hidden_layer_0': 143, 'hidden_layer_1': 175, 'hidden_layer_2': 71, 'nn_learning_rate': 0.05931977960136398, 'batch_size': 256, 'num_epochs': 95}. Best is trial 18 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2123632112.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:49:44,400] Trial 49 finished with value: 0.875 and parameters: {'num_heads': 2, 'embedding_dim': 4, 'num_layers': 3, 'hidden_layer_0': 218, 'hidden_layer_1': 191, 'hidden_layer_2': 140, 'nn_learning_rate': 0.013225575417089274, 'batch_size': 32, 'num_epochs': 84}. Best is trial 18 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "XGBoost + NN           0.875  0.888889                    0.364206   \n",
      "LightGBM + NN          0.375       0.5                    0.253997   \n",
      "CatBoost + NN           0.75  0.755556                    0.277767   \n",
      "AutoInt                0.625  0.927778                    6.090192   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "XGBoost + NN                            0.000997                41.681399   \n",
      "LightGBM + NN                           0.000989                19.848618   \n",
      "CatBoost + NN                           0.000998                 48.66849   \n",
      "AutoInt                                 0.067931                33.747949   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN         {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt              {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_heads: 2\n",
      "embedding_dim: 12\n",
      "num_layers: 2\n",
      "hidden_layer_0: 231\n",
      "hidden_layer_1: 111\n",
      "hidden_layer_2: 65\n",
      "nn_learning_rate: 0.00282538200280168\n",
      "batch_size: 256\n",
      "num_epochs: 90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class AutoInt(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "        super(AutoInt, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x, _ = attn_layer(x, x, x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for AutoInt\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    \n",
    "    # Train AutoInt model\n",
    "    autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "        autoint_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = autoint_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Extract features using AutoInt\n",
    "    autoint_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final AutoInt model with the best hyperparameters\n",
    "embedding_dim = best_params['embedding_dim']\n",
    "num_heads = best_params['num_heads']\n",
    "num_layers = best_params['num_layers']\n",
    "autoint_model = AutoInt(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "optimizer = optim.Adam(autoint_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  # Fixed number of epochs for AutoInt\n",
    "    autoint_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoint_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract features using AutoInt\n",
    "autoint_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_transformed = autoint_model.embedding(X_train_tensor).cpu().numpy()\n",
    "    X_test_transformed = autoint_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['AutoInt'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:49:51,798] A new study created in memory with name: no-name-14fa1cd7-5559-4abd-8519-080704ddae98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:00,084] Trial 0 finished with value: 0.875 and parameters: {'num_heads': 5, 'embedding_dim': 50, 'num_layers': 2, 'hidden_layer_0': 89, 'hidden_layer_1': 188, 'hidden_layer_2': 116, 'nn_learning_rate': 0.019592167641062218, 'batch_size': 64, 'num_epochs': 51}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:04,942] Trial 1 finished with value: 0.875 and parameters: {'num_heads': 5, 'embedding_dim': 20, 'num_layers': 3, 'hidden_layer_0': 39, 'hidden_layer_1': 216, 'hidden_layer_2': 42, 'nn_learning_rate': 0.008545646708488109, 'batch_size': 64, 'num_epochs': 95}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:05,895] Trial 2 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 48, 'num_layers': 3, 'hidden_layer_0': 252, 'hidden_layer_1': 205, 'hidden_layer_2': 171, 'nn_learning_rate': 0.00013332322206932102, 'batch_size': 32, 'num_epochs': 18}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:06,946] Trial 3 finished with value: 0.5 and parameters: {'num_heads': 4, 'embedding_dim': 60, 'num_layers': 1, 'hidden_layer_0': 42, 'hidden_layer_1': 150, 'hidden_layer_2': 106, 'nn_learning_rate': 0.00010523937033630965, 'batch_size': 32, 'num_epochs': 92}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:07,249] Trial 4 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 30, 'num_layers': 2, 'hidden_layer_0': 55, 'hidden_layer_1': 210, 'hidden_layer_2': 65, 'nn_learning_rate': 0.010027178597191419, 'batch_size': 128, 'num_epochs': 17}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:07,968] Trial 5 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 179, 'hidden_layer_1': 106, 'hidden_layer_2': 52, 'nn_learning_rate': 0.07986959694553712, 'batch_size': 256, 'num_epochs': 63}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:08,703] Trial 6 finished with value: 0.75 and parameters: {'num_heads': 1, 'embedding_dim': 28, 'num_layers': 3, 'hidden_layer_0': 221, 'hidden_layer_1': 88, 'hidden_layer_2': 171, 'nn_learning_rate': 0.028010162088812473, 'batch_size': 32, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:09,403] Trial 7 finished with value: 0.75 and parameters: {'num_heads': 2, 'embedding_dim': 32, 'num_layers': 3, 'hidden_layer_0': 201, 'hidden_layer_1': 248, 'hidden_layer_2': 91, 'nn_learning_rate': 0.07990367879298468, 'batch_size': 64, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:10,696] Trial 8 finished with value: 0.625 and parameters: {'num_heads': 1, 'embedding_dim': 53, 'num_layers': 3, 'hidden_layer_0': 243, 'hidden_layer_1': 172, 'hidden_layer_2': 135, 'nn_learning_rate': 0.0021103621674512336, 'batch_size': 256, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:11,032] Trial 9 finished with value: 0.625 and parameters: {'num_heads': 2, 'embedding_dim': 32, 'num_layers': 1, 'hidden_layer_0': 197, 'hidden_layer_1': 79, 'hidden_layer_2': 78, 'nn_learning_rate': 0.001175835777035706, 'batch_size': 256, 'num_epochs': 17}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:11,519] Trial 10 finished with value: 0.625 and parameters: {'num_heads': 8, 'embedding_dim': 8, 'num_layers': 2, 'hidden_layer_0': 107, 'hidden_layer_1': 32, 'hidden_layer_2': 233, 'nn_learning_rate': 0.0006021575511961219, 'batch_size': 64, 'num_epochs': 38}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:12,070] Trial 11 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 15, 'num_layers': 2, 'hidden_layer_0': 96, 'hidden_layer_1': 254, 'hidden_layer_2': 33, 'nn_learning_rate': 0.008516661559168763, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:12,976] Trial 12 finished with value: 0.875 and parameters: {'num_heads': 4, 'embedding_dim': 20, 'num_layers': 1, 'hidden_layer_0': 92, 'hidden_layer_1': 200, 'hidden_layer_2': 129, 'nn_learning_rate': 0.008325840668472018, 'batch_size': 64, 'num_epochs': 98}. Best is trial 0 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:13,743] Trial 13 finished with value: 1.0 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 128, 'hidden_layer_1': 168, 'hidden_layer_2': 236, 'nn_learning_rate': 0.026099443553451257, 'batch_size': 64, 'num_epochs': 45}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:14,307] Trial 14 finished with value: 0.75 and parameters: {'num_heads': 8, 'embedding_dim': 64, 'num_layers': 2, 'hidden_layer_0': 141, 'hidden_layer_1': 134, 'hidden_layer_2': 252, 'nn_learning_rate': 0.023707085674313854, 'batch_size': 128, 'num_epochs': 43}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:14,897] Trial 15 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 141, 'hidden_layer_1': 169, 'hidden_layer_2': 204, 'nn_learning_rate': 0.033450491684089874, 'batch_size': 64, 'num_epochs': 48}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:15,262] Trial 16 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 42, 'num_layers': 1, 'hidden_layer_0': 121, 'hidden_layer_1': 124, 'hidden_layer_2': 173, 'nn_learning_rate': 0.0036612724523990272, 'batch_size': 64, 'num_epochs': 30}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:16,074] Trial 17 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 72, 'hidden_layer_1': 168, 'hidden_layer_2': 203, 'nn_learning_rate': 0.039442924049518366, 'batch_size': 64, 'num_epochs': 79}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:16,650] Trial 18 finished with value: 0.875 and parameters: {'num_heads': 7, 'embedding_dim': 56, 'num_layers': 2, 'hidden_layer_0': 163, 'hidden_layer_1': 180, 'hidden_layer_2': 107, 'nn_learning_rate': 0.019291737879547022, 'batch_size': 128, 'num_epochs': 52}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:16,957] Trial 19 finished with value: 0.875 and parameters: {'num_heads': 4, 'embedding_dim': 40, 'num_layers': 1, 'hidden_layer_0': 75, 'hidden_layer_1': 234, 'hidden_layer_2': 151, 'nn_learning_rate': 0.004094013802363632, 'batch_size': 64, 'num_epochs': 29}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:17,646] Trial 20 finished with value: 0.875 and parameters: {'num_heads': 3, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 122, 'hidden_layer_1': 154, 'hidden_layer_2': 209, 'nn_learning_rate': 0.09573622794036094, 'batch_size': 64, 'num_epochs': 73}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:18,335] Trial 21 finished with value: 0.875 and parameters: {'num_heads': 5, 'embedding_dim': 20, 'num_layers': 3, 'hidden_layer_0': 35, 'hidden_layer_1': 200, 'hidden_layer_2': 36, 'nn_learning_rate': 0.013148538055630722, 'batch_size': 64, 'num_epochs': 88}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:18,988] Trial 22 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 5, 'num_layers': 3, 'hidden_layer_0': 71, 'hidden_layer_1': 228, 'hidden_layer_2': 116, 'nn_learning_rate': 0.0067918424257474245, 'batch_size': 64, 'num_epochs': 56}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:19,557] Trial 23 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 24, 'num_layers': 2, 'hidden_layer_0': 57, 'hidden_layer_1': 224, 'hidden_layer_2': 77, 'nn_learning_rate': 0.048191318662740935, 'batch_size': 64, 'num_epochs': 66}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:20,057] Trial 24 finished with value: 0.75 and parameters: {'num_heads': 3, 'embedding_dim': 36, 'num_layers': 3, 'hidden_layer_0': 116, 'hidden_layer_1': 187, 'hidden_layer_2': 152, 'nn_learning_rate': 0.019169570932756372, 'batch_size': 64, 'num_epochs': 31}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:20,570] Trial 25 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 15, 'num_layers': 2, 'hidden_layer_0': 86, 'hidden_layer_1': 189, 'hidden_layer_2': 254, 'nn_learning_rate': 0.004663483984122882, 'batch_size': 64, 'num_epochs': 46}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:21,219] Trial 26 finished with value: 0.875 and parameters: {'num_heads': 8, 'embedding_dim': 56, 'num_layers': 2, 'hidden_layer_0': 159, 'hidden_layer_1': 217, 'hidden_layer_2': 56, 'nn_learning_rate': 0.014134360762296025, 'batch_size': 32, 'num_epochs': 68}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:21,841] Trial 27 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 49, 'num_layers': 3, 'hidden_layer_0': 56, 'hidden_layer_1': 148, 'hidden_layer_2': 225, 'nn_learning_rate': 0.0018497233634300182, 'batch_size': 256, 'num_epochs': 54}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:22,532] Trial 28 finished with value: 0.625 and parameters: {'num_heads': 6, 'embedding_dim': 36, 'num_layers': 2, 'hidden_layer_0': 32, 'hidden_layer_1': 241, 'hidden_layer_2': 188, 'nn_learning_rate': 0.055861330749043585, 'batch_size': 64, 'num_epochs': 85}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:22,912] Trial 29 finished with value: 0.625 and parameters: {'num_heads': 3, 'embedding_dim': 12, 'num_layers': 3, 'hidden_layer_0': 134, 'hidden_layer_1': 199, 'hidden_layer_2': 90, 'nn_learning_rate': 0.0006507152860162076, 'batch_size': 128, 'num_epochs': 10}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:23,365] Trial 30 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 24, 'num_layers': 3, 'hidden_layer_0': 100, 'hidden_layer_1': 124, 'hidden_layer_2': 157, 'nn_learning_rate': 0.00025892015244514683, 'batch_size': 32, 'num_epochs': 35}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:24,073] Trial 31 finished with value: 0.75 and parameters: {'num_heads': 4, 'embedding_dim': 20, 'num_layers': 1, 'hidden_layer_0': 88, 'hidden_layer_1': 200, 'hidden_layer_2': 128, 'nn_learning_rate': 0.007612779983894599, 'batch_size': 64, 'num_epochs': 100}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:25,475] Trial 32 finished with value: 0.75 and parameters: {'num_heads': 4, 'embedding_dim': 24, 'num_layers': 1, 'hidden_layer_0': 109, 'hidden_layer_1': 212, 'hidden_layer_2': 130, 'nn_learning_rate': 0.0056475194389182925, 'batch_size': 64, 'num_epochs': 99}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:26,148] Trial 33 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 20, 'num_layers': 1, 'hidden_layer_0': 54, 'hidden_layer_1': 162, 'hidden_layer_2': 108, 'nn_learning_rate': 0.01257367208841661, 'batch_size': 64, 'num_epochs': 91}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:26,893] Trial 34 finished with value: 0.875 and parameters: {'num_heads': 4, 'embedding_dim': 52, 'num_layers': 1, 'hidden_layer_0': 83, 'hidden_layer_1': 187, 'hidden_layer_2': 168, 'nn_learning_rate': 0.016335619110872838, 'batch_size': 64, 'num_epochs': 94}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 64] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:27,612] Trial 35 finished with value: 0.875 and parameters: {'num_heads': 3, 'embedding_dim': 27, 'num_layers': 2, 'hidden_layer_0': 161, 'hidden_layer_1': 211, 'hidden_layer_2': 119, 'nn_learning_rate': 0.0029243456596301912, 'batch_size': 32, 'num_epochs': 85}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:28,286] Trial 36 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 35, 'num_layers': 1, 'hidden_layer_0': 45, 'hidden_layer_1': 178, 'hidden_layer_2': 95, 'nn_learning_rate': 0.009866810226162674, 'batch_size': 64, 'num_epochs': 96}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:28,979] Trial 37 finished with value: 0.625 and parameters: {'num_heads': 4, 'embedding_dim': 44, 'num_layers': 2, 'hidden_layer_0': 131, 'hidden_layer_1': 158, 'hidden_layer_2': 142, 'nn_learning_rate': 0.027897440414488578, 'batch_size': 128, 'num_epochs': 73}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:29,505] Trial 38 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 63, 'num_layers': 3, 'hidden_layer_0': 65, 'hidden_layer_1': 199, 'hidden_layer_2': 74, 'nn_learning_rate': 0.010850083924040856, 'batch_size': 256, 'num_epochs': 24}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:29,937] Trial 39 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 12, 'num_layers': 1, 'hidden_layer_0': 100, 'hidden_layer_1': 139, 'hidden_layer_2': 56, 'nn_learning_rate': 0.053140828554615036, 'batch_size': 64, 'num_epochs': 59}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:30,418] Trial 40 finished with value: 0.875 and parameters: {'num_heads': 2, 'embedding_dim': 28, 'num_layers': 2, 'hidden_layer_0': 153, 'hidden_layer_1': 221, 'hidden_layer_2': 179, 'nn_learning_rate': 0.002445249772977645, 'batch_size': 32, 'num_epochs': 43}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:30,958] Trial 41 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 56, 'num_layers': 2, 'hidden_layer_0': 172, 'hidden_layer_1': 180, 'hidden_layer_2': 101, 'nn_learning_rate': 0.022649212865441082, 'batch_size': 128, 'num_epochs': 52}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:31,549] Trial 42 finished with value: 0.875 and parameters: {'num_heads': 8, 'embedding_dim': 56, 'num_layers': 2, 'hidden_layer_0': 181, 'hidden_layer_1': 191, 'hidden_layer_2': 118, 'nn_learning_rate': 0.0177191690086247, 'batch_size': 128, 'num_epochs': 48}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:32,200] Trial 43 finished with value: 0.75 and parameters: {'num_heads': 7, 'embedding_dim': 49, 'num_layers': 2, 'hidden_layer_0': 194, 'hidden_layer_1': 178, 'hidden_layer_2': 140, 'nn_learning_rate': 0.005864221477488749, 'batch_size': 128, 'num_epochs': 52}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 64] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:32,816] Trial 44 finished with value: 0.75 and parameters: {'num_heads': 5, 'embedding_dim': 55, 'num_layers': 2, 'hidden_layer_0': 219, 'hidden_layer_1': 238, 'hidden_layer_2': 47, 'nn_learning_rate': 0.03558360049109187, 'batch_size': 128, 'num_epochs': 58}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:33,410] Trial 45 finished with value: 0.875 and parameters: {'num_heads': 8, 'embedding_dim': 48, 'num_layers': 2, 'hidden_layer_0': 129, 'hidden_layer_1': 107, 'hidden_layer_2': 85, 'nn_learning_rate': 0.009197624757737485, 'batch_size': 256, 'num_epochs': 64}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:33,900] Trial 46 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 42, 'num_layers': 2, 'hidden_layer_0': 149, 'hidden_layer_1': 38, 'hidden_layer_2': 106, 'nn_learning_rate': 0.023560511550320566, 'batch_size': 128, 'num_epochs': 42}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 64] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 63].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:34,405] Trial 47 finished with value: 0.875 and parameters: {'num_heads': 7, 'embedding_dim': 63, 'num_layers': 1, 'hidden_layer_0': 176, 'hidden_layer_1': 167, 'hidden_layer_2': 67, 'nn_learning_rate': 0.07320844996938089, 'batch_size': 64, 'num_epochs': 50}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:34,989] Trial 48 finished with value: 0.75 and parameters: {'num_heads': 4, 'embedding_dim': 52, 'num_layers': 2, 'hidden_layer_0': 43, 'hidden_layer_1': 253, 'hidden_layer_2': 163, 'nn_learning_rate': 0.03054357505800684, 'batch_size': 64, 'num_epochs': 45}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 64] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 60].\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\299350481.py:106: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:35,637] Trial 49 finished with value: 0.75 and parameters: {'num_heads': 6, 'embedding_dim': 48, 'num_layers': 3, 'hidden_layer_0': 91, 'hidden_layer_1': 211, 'hidden_layer_2': 240, 'nn_learning_rate': 0.0014099841534211265, 'batch_size': 64, 'num_epochs': 38}. Best is trial 13 with value: 1.0.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression     0.75  0.927778                    0.002991   \n",
      "KNN                     0.75       1.0                    0.000996   \n",
      "Decision Tree          0.875  0.931746                       0.001   \n",
      "Random Forest           0.75  0.933333                    0.118717   \n",
      "Gradient Boosting      0.875  0.920635                    0.247005   \n",
      "XGBoost                 0.75  0.805556                    0.082778   \n",
      "LightGBM               0.125       0.5                    0.010972   \n",
      "CatBoost                0.75  0.977778                    0.119442   \n",
      "MLP                     0.75       1.0                    0.513627   \n",
      "DNN                     0.75  0.922222                     0.23946   \n",
      "DCN                    0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep          0.875  0.830556                    0.449732   \n",
      "XGBoost + NN           0.875  0.888889                    0.364206   \n",
      "LightGBM + NN          0.375       0.5                    0.253997   \n",
      "CatBoost + NN           0.75  0.755556                    0.277767   \n",
      "AutoInt                0.625  0.927778                    6.090192   \n",
      "FT-Transformer         0.625  0.683333                    0.246869   \n",
      "\n",
      "                    Inference Time (Best Params) Computation Time (Total)  \\\n",
      "Logistic Regression                     0.002992                 5.321193   \n",
      "KNN                                     0.002262                 0.203027   \n",
      "Decision Tree                                0.0                 0.095976   \n",
      "Random Forest                           0.008943                 5.180227   \n",
      "Gradient Boosting                       0.003386                 7.384311   \n",
      "XGBoost                                 0.000998                 1.069617   \n",
      "LightGBM                                0.000997                 4.737833   \n",
      "CatBoost                                0.001001                 6.248461   \n",
      "MLP                                          0.0                30.570727   \n",
      "DNN                                          0.0                22.263058   \n",
      "DCN                                     0.000997                 24.14075   \n",
      "Wide_and_Deep                                0.0                22.987482   \n",
      "XGBoost + NN                            0.000997                41.681399   \n",
      "LightGBM + NN                           0.000989                19.848618   \n",
      "CatBoost + NN                           0.000998                 48.66849   \n",
      "AutoInt                                 0.067931                33.747949   \n",
      "FT-Transformer                               0.0                44.479363   \n",
      "\n",
      "                                                       Best Parameters  \n",
      "Logistic Regression                    {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                          {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree              {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest        {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting    {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost              {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM             {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost             {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                  {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                  {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                  {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep        {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN         {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN        {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN        {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt              {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer       {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_heads: 7\n",
      "embedding_dim: 42\n",
      "num_layers: 2\n",
      "hidden_layer_0: 128\n",
      "hidden_layer_1: 168\n",
      "hidden_layer_2: 236\n",
      "nn_learning_rate: 0.026099443553451257\n",
      "batch_size: 64\n",
      "num_epochs: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "        super(FTTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for FT-Transformer\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    embedding_dim = trial.suggest_int('embedding_dim', num_heads, 64, step=num_heads)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    \n",
    "    # Train FT-Transformer model\n",
    "    ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "    optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "        ft_transformer_model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ft_transformer_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Extract features using FT-Transformer\n",
    "    ft_transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "        X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "    X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "    \n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(3)]\n",
    "    nn_learning_rate = trial.suggest_loguniform('nn_learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train_transformed.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nn_learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_transformed_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final FT-Transformer model with the best hyperparameters\n",
    "embedding_dim = best_params['embedding_dim']\n",
    "num_heads = best_params['num_heads']\n",
    "num_layers = best_params['num_layers']\n",
    "ft_transformer_model = FTTransformer(X_train.shape[1], embedding_dim, num_heads, num_layers).to(device)\n",
    "optimizer = optim.Adam(ft_transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor.float().unsqueeze(1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  # Fixed number of epochs for FT-Transformer\n",
    "    ft_transformer_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ft_transformer_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract features using FT-Transformer\n",
    "ft_transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_transformed = ft_transformer_model.embedding(X_train_tensor).cpu().numpy()\n",
    "    X_test_transformed = ft_transformer_model.embedding(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_transformed_tensor = torch.FloatTensor(X_train_transformed).to(device)\n",
    "X_test_transformed_tensor = torch.FloatTensor(X_test_transformed).to(device)\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "nn_best_params = {\n",
    "    'hidden_layers': [best_params[f'hidden_layer_{i}'] for i in range(3)],\n",
    "    'learning_rate': best_params['nn_learning_rate'],\n",
    "    'batch_size': best_params['batch_size'],\n",
    "    'num_epochs': best_params['num_epochs']\n",
    "}\n",
    "input_dim = X_train_transformed.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, nn_best_params['hidden_layers'], output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=nn_best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_transformed_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=nn_best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(nn_best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_transformed_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['FT-Transformer'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:50:36,225] A new study created in memory with name: no-name-ff75fcac-0335-43eb-b5f4-33102e0bea4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:37,197] Trial 0 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 82, 'learning_rate': 0.009076696335475115, 'batch_size': 64, 'num_epochs': 59}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:37,425] Trial 1 finished with value: 0.625 and parameters: {'num_layers': 2, 'hidden_layer_0': 72, 'hidden_layer_1': 64, 'learning_rate': 0.00045258260109776665, 'batch_size': 32, 'num_epochs': 72}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:37,651] Trial 2 finished with value: 0.75 and parameters: {'num_layers': 5, 'hidden_layer_0': 148, 'hidden_layer_1': 208, 'hidden_layer_2': 140, 'hidden_layer_3': 51, 'hidden_layer_4': 250, 'learning_rate': 0.03738912571197548, 'batch_size': 128, 'num_epochs': 33}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:37,740] Trial 3 finished with value: 0.375 and parameters: {'num_layers': 4, 'hidden_layer_0': 52, 'hidden_layer_1': 137, 'hidden_layer_2': 136, 'hidden_layer_3': 183, 'learning_rate': 0.0003696177519454313, 'batch_size': 256, 'num_epochs': 15}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:38,513] Trial 4 finished with value: 0.75 and parameters: {'num_layers': 5, 'hidden_layer_0': 72, 'hidden_layer_1': 106, 'hidden_layer_2': 121, 'hidden_layer_3': 101, 'hidden_layer_4': 212, 'learning_rate': 0.0001495469231704125, 'batch_size': 256, 'num_epochs': 78}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:38,754] Trial 5 finished with value: 0.75 and parameters: {'num_layers': 4, 'hidden_layer_0': 232, 'hidden_layer_1': 97, 'hidden_layer_2': 132, 'hidden_layer_3': 94, 'learning_rate': 0.003578979473028215, 'batch_size': 64, 'num_epochs': 39}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:39,050] Trial 6 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 168, 'hidden_layer_1': 93, 'hidden_layer_2': 114, 'learning_rate': 0.002932714697386917, 'batch_size': 256, 'num_epochs': 66}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:39,271] Trial 7 finished with value: 0.625 and parameters: {'num_layers': 1, 'hidden_layer_0': 39, 'learning_rate': 0.00032959577471139293, 'batch_size': 64, 'num_epochs': 87}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:39,579] Trial 8 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 109, 'hidden_layer_1': 63, 'hidden_layer_2': 185, 'learning_rate': 0.0026285536082907065, 'batch_size': 256, 'num_epochs': 70}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:39,778] Trial 9 finished with value: 0.5 and parameters: {'num_layers': 1, 'hidden_layer_0': 117, 'learning_rate': 0.0003641759871164033, 'batch_size': 64, 'num_epochs': 77}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:40,231] Trial 10 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 196, 'hidden_layer_1': 255, 'learning_rate': 0.08791287904210762, 'batch_size': 128, 'num_epochs': 100}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:40,584] Trial 11 finished with value: 0.75 and parameters: {'num_layers': 5, 'hidden_layer_0': 148, 'hidden_layer_1': 222, 'hidden_layer_2': 39, 'hidden_layer_3': 33, 'hidden_layer_4': 64, 'learning_rate': 0.042564937941566926, 'batch_size': 128, 'num_epochs': 39}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:40,983] Trial 12 finished with value: 0.75 and parameters: {'num_layers': 4, 'hidden_layer_0': 110, 'hidden_layer_1': 189, 'hidden_layer_2': 256, 'hidden_layer_3': 232, 'learning_rate': 0.011178519751745955, 'batch_size': 128, 'num_epochs': 46}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,117] Trial 13 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 184, 'hidden_layer_1': 182, 'learning_rate': 0.017630972942481112, 'batch_size': 32, 'num_epochs': 20}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,241] Trial 14 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 251, 'hidden_layer_1': 165, 'learning_rate': 0.011644026073272208, 'batch_size': 32, 'num_epochs': 17}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,483] Trial 15 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 193, 'learning_rate': 0.010094285718066557, 'batch_size': 32, 'num_epochs': 58}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,647] Trial 16 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 196, 'hidden_layer_1': 148, 'learning_rate': 0.02384536552161597, 'batch_size': 32, 'num_epochs': 29}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,810] Trial 17 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 97, 'learning_rate': 0.0013388181701239452, 'batch_size': 64, 'num_epochs': 53}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:41,939] Trial 18 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 180, 'hidden_layer_1': 245, 'learning_rate': 0.006628592040232973, 'batch_size': 64, 'num_epochs': 24}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:42,121] Trial 19 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 224, 'hidden_layer_1': 251, 'hidden_layer_2': 35, 'learning_rate': 0.0012757238277122454, 'batch_size': 32, 'num_epochs': 24}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:42,199] Trial 20 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 173, 'hidden_layer_1': 220, 'learning_rate': 0.005756888588443106, 'batch_size': 32, 'num_epochs': 10}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:42,301] Trial 21 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 172, 'learning_rate': 0.006297188916297294, 'batch_size': 64, 'num_epochs': 22}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:42,597] Trial 22 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 134, 'hidden_layer_1': 189, 'learning_rate': 0.02348482248395905, 'batch_size': 64, 'num_epochs': 55}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:42,837] Trial 23 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 131, 'hidden_layer_1': 183, 'learning_rate': 0.02040169572846645, 'batch_size': 64, 'num_epochs': 44}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:43,071] Trial 24 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 210, 'hidden_layer_1': 197, 'hidden_layer_2': 252, 'learning_rate': 0.021734064655759208, 'batch_size': 64, 'num_epochs': 30}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:43,154] Trial 25 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 159, 'hidden_layer_1': 234, 'learning_rate': 0.0980384076437665, 'batch_size': 64, 'num_epochs': 10}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:43,336] Trial 26 finished with value: 0.625 and parameters: {'num_layers': 3, 'hidden_layer_0': 130, 'hidden_layer_1': 166, 'hidden_layer_2': 193, 'learning_rate': 0.04248568273358001, 'batch_size': 64, 'num_epochs': 22}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:43,553] Trial 27 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 190, 'hidden_layer_1': 172, 'learning_rate': 0.01927322694814638, 'batch_size': 32, 'num_epochs': 36}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:43,857] Trial 28 finished with value: 0.625 and parameters: {'num_layers': 3, 'hidden_layer_0': 180, 'hidden_layer_1': 135, 'hidden_layer_2': 83, 'learning_rate': 0.06212222205306141, 'batch_size': 64, 'num_epochs': 50}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:44,317] Trial 29 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 214, 'hidden_layer_1': 239, 'learning_rate': 0.013870245904387842, 'batch_size': 64, 'num_epochs': 88}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:44,529] Trial 30 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 154, 'learning_rate': 0.005897963165401752, 'batch_size': 32, 'num_epochs': 26}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:44,784] Trial 31 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 133, 'hidden_layer_1': 189, 'learning_rate': 0.01885339618034461, 'batch_size': 64, 'num_epochs': 43}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:45,105] Trial 32 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 130, 'hidden_layer_1': 205, 'learning_rate': 0.03332922969966365, 'batch_size': 64, 'num_epochs': 63}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:46,177] Trial 33 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 93, 'hidden_layer_1': 183, 'learning_rate': 0.026064312329566918, 'batch_size': 64, 'num_epochs': 46}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:46,426] Trial 34 finished with value: 0.75 and parameters: {'num_layers': 2, 'hidden_layer_0': 138, 'hidden_layer_1': 158, 'learning_rate': 0.00640386957310635, 'batch_size': 64, 'num_epochs': 57}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:46,554] Trial 35 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 160, 'learning_rate': 0.05645911380124335, 'batch_size': 64, 'num_epochs': 33}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:46,673] Trial 36 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 117, 'hidden_layer_1': 117, 'learning_rate': 0.015323709225423355, 'batch_size': 256, 'num_epochs': 18}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:47,072] Trial 37 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 79, 'hidden_layer_1': 32, 'hidden_layer_2': 193, 'learning_rate': 0.004324521721862593, 'batch_size': 128, 'num_epochs': 51}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:47,707] Trial 38 finished with value: 1.0 and parameters: {'num_layers': 4, 'hidden_layer_0': 59, 'hidden_layer_1': 217, 'hidden_layer_2': 78, 'hidden_layer_3': 246, 'learning_rate': 0.008663078877075342, 'batch_size': 32, 'num_epochs': 62}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:48,208] Trial 39 finished with value: 0.75 and parameters: {'num_layers': 4, 'hidden_layer_0': 61, 'hidden_layer_1': 222, 'hidden_layer_2': 73, 'hidden_layer_3': 251, 'learning_rate': 0.00849805494306588, 'batch_size': 32, 'num_epochs': 66}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:48,714] Trial 40 finished with value: 0.625 and parameters: {'num_layers': 4, 'hidden_layer_0': 41, 'hidden_layer_1': 210, 'hidden_layer_2': 86, 'hidden_layer_3': 190, 'learning_rate': 0.0015666869851262758, 'batch_size': 32, 'num_epochs': 78}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:49,206] Trial 41 finished with value: 0.625 and parameters: {'num_layers': 5, 'hidden_layer_0': 62, 'hidden_layer_1': 236, 'hidden_layer_2': 169, 'hidden_layer_3': 163, 'hidden_layer_4': 33, 'learning_rate': 0.008446195146949017, 'batch_size': 32, 'num_epochs': 61}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:49,524] Trial 42 finished with value: 0.75 and parameters: {'num_layers': 4, 'hidden_layer_0': 182, 'hidden_layer_1': 182, 'hidden_layer_2': 56, 'hidden_layer_3': 215, 'learning_rate': 0.03064138924601584, 'batch_size': 256, 'num_epochs': 40}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:50,045] Trial 43 finished with value: 0.875 and parameters: {'num_layers': 3, 'hidden_layer_0': 91, 'hidden_layer_1': 202, 'hidden_layer_2': 223, 'learning_rate': 0.004140880340361855, 'batch_size': 32, 'num_epochs': 73}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:50,335] Trial 44 finished with value: 0.875 and parameters: {'num_layers': 2, 'hidden_layer_0': 209, 'hidden_layer_1': 214, 'learning_rate': 0.015850779099096386, 'batch_size': 64, 'num_epochs': 47}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:50,763] Trial 45 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 147, 'hidden_layer_1': 176, 'hidden_layer_2': 101, 'learning_rate': 0.0028943717796902934, 'batch_size': 64, 'num_epochs': 54}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:51,280] Trial 46 finished with value: 0.875 and parameters: {'num_layers': 5, 'hidden_layer_0': 114, 'hidden_layer_1': 152, 'hidden_layer_2': 161, 'hidden_layer_3': 137, 'hidden_layer_4': 139, 'learning_rate': 0.0021668195228629374, 'batch_size': 128, 'num_epochs': 64}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:51,413] Trial 47 finished with value: 0.75 and parameters: {'num_layers': 1, 'hidden_layer_0': 237, 'learning_rate': 0.0006740634074121759, 'batch_size': 256, 'num_epochs': 34}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:51,526] Trial 48 finished with value: 0.625 and parameters: {'num_layers': 2, 'hidden_layer_0': 164, 'hidden_layer_1': 243, 'learning_rate': 0.05469095019279447, 'batch_size': 32, 'num_epochs': 14}. Best is trial 38 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\3796604553.py:58: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:53,247] Trial 49 finished with value: 0.75 and parameters: {'num_layers': 3, 'hidden_layer_0': 124, 'hidden_layer_1': 229, 'hidden_layer_2': 61, 'learning_rate': 0.00010319003645640335, 'batch_size': 64, 'num_epochs': 69}. Best is trial 38 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_layers: 4\n",
      "hidden_layer_0: 59\n",
      "hidden_layer_1: 217\n",
      "hidden_layer_2: 78\n",
      "hidden_layer_3: 246\n",
      "learning_rate: 0.008663078877075342\n",
      "batch_size: 32\n",
      "num_epochs: 62\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_layers[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for Neural Network\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    hidden_layers = [trial.suggest_int(f'hidden_layer_{i}', 32, 256) for i in range(num_layers)]\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the Neural Network model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NeuralNetwork(input_dim, hidden_layers, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final Neural Network model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NeuralNetwork(input_dim, \n",
    "                           [best_params[f'hidden_layer_{i}'] for i in range(best_params['num_layers'])], \n",
    "                           output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        proba = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba)\n",
    "    else:  # Multi-class classification\n",
    "        proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['Neural Architecture Search'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:50:53,824] A new study created in memory with name: no-name-cb598485-e337-47ff-a458-342b40d7bac2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:54,275] Trial 0 finished with value: 0.75 and parameters: {'num_layers': 4, 'num_trees': 5, 'tree_dim': 59, 'learning_rate': 0.006039884739845829, 'batch_size': 32, 'num_epochs': 21}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:56,201] Trial 1 finished with value: 0.75 and parameters: {'num_layers': 5, 'num_trees': 7, 'tree_dim': 51, 'learning_rate': 0.005037138471410539, 'batch_size': 256, 'num_epochs': 83}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:56,733] Trial 2 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 2, 'tree_dim': 43, 'learning_rate': 0.014640401333604208, 'batch_size': 32, 'num_epochs': 94}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:56,909] Trial 3 finished with value: 0.875 and parameters: {'num_layers': 2, 'num_trees': 6, 'tree_dim': 49, 'learning_rate': 0.002079962530225977, 'batch_size': 32, 'num_epochs': 18}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:57,672] Trial 4 finished with value: 0.625 and parameters: {'num_layers': 3, 'num_trees': 10, 'tree_dim': 38, 'learning_rate': 0.0013787373058092285, 'batch_size': 256, 'num_epochs': 35}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:58,152] Trial 5 finished with value: 0.625 and parameters: {'num_layers': 2, 'num_trees': 3, 'tree_dim': 61, 'learning_rate': 0.07969664895577157, 'batch_size': 256, 'num_epochs': 57}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:50:59,752] Trial 6 finished with value: 0.75 and parameters: {'num_layers': 5, 'num_trees': 9, 'tree_dim': 57, 'learning_rate': 0.003968414071622543, 'batch_size': 64, 'num_epochs': 35}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:00,351] Trial 7 finished with value: 0.625 and parameters: {'num_layers': 5, 'num_trees': 2, 'tree_dim': 26, 'learning_rate': 0.00012553780676325624, 'batch_size': 128, 'num_epochs': 45}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:01,761] Trial 8 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 8, 'tree_dim': 18, 'learning_rate': 0.00677677408456471, 'batch_size': 128, 'num_epochs': 90}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:02,502] Trial 9 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 3, 'tree_dim': 50, 'learning_rate': 0.011037266768896948, 'batch_size': 256, 'num_epochs': 73}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:02,612] Trial 10 finished with value: 0.375 and parameters: {'num_layers': 1, 'num_trees': 6, 'tree_dim': 8, 'learning_rate': 0.0005460040162154909, 'batch_size': 32, 'num_epochs': 10}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:02,981] Trial 11 finished with value: 0.625 and parameters: {'num_layers': 4, 'num_trees': 5, 'tree_dim': 49, 'learning_rate': 0.0010014319706901863, 'batch_size': 32, 'num_epochs': 11}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:03,453] Trial 12 finished with value: 0.875 and parameters: {'num_layers': 4, 'num_trees': 5, 'tree_dim': 64, 'learning_rate': 0.03711187770137774, 'batch_size': 32, 'num_epochs': 24}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:03,619] Trial 13 finished with value: 0.875 and parameters: {'num_layers': 1, 'num_trees': 4, 'tree_dim': 64, 'learning_rate': 0.04379549275512412, 'batch_size': 32, 'num_epochs': 22}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:04,430] Trial 14 finished with value: 0.875 and parameters: {'num_layers': 2, 'num_trees': 6, 'tree_dim': 30, 'learning_rate': 0.03301098549909691, 'batch_size': 64, 'num_epochs': 58}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:05,246] Trial 15 finished with value: 0.625 and parameters: {'num_layers': 4, 'num_trees': 7, 'tree_dim': 42, 'learning_rate': 0.0016997534841389893, 'batch_size': 32, 'num_epochs': 30}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:05,725] Trial 16 finished with value: 0.5 and parameters: {'num_layers': 2, 'num_trees': 4, 'tree_dim': 54, 'learning_rate': 0.00030563998428974814, 'batch_size': 32, 'num_epochs': 48}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:05,886] Trial 17 finished with value: 0.75 and parameters: {'num_layers': 4, 'num_trees': 1, 'tree_dim': 45, 'learning_rate': 0.023165714189220605, 'batch_size': 32, 'num_epochs': 24}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:08,251] Trial 18 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 7, 'tree_dim': 35, 'learning_rate': 0.07818449865994906, 'batch_size': 64, 'num_epochs': 69}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:08,890] Trial 19 finished with value: 0.5 and parameters: {'num_layers': 1, 'num_trees': 8, 'tree_dim': 64, 'learning_rate': 0.0004837116066456872, 'batch_size': 128, 'num_epochs': 44}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:09,200] Trial 20 finished with value: 0.625 and parameters: {'num_layers': 3, 'num_trees': 4, 'tree_dim': 56, 'learning_rate': 0.0023889682038663136, 'batch_size': 32, 'num_epochs': 19}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:09,565] Trial 21 finished with value: 0.875 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 64, 'learning_rate': 0.04420695861289753, 'batch_size': 32, 'num_epochs': 28}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:09,802] Trial 22 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 4, 'tree_dim': 64, 'learning_rate': 0.012150684253838175, 'batch_size': 32, 'num_epochs': 16}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:10,254] Trial 23 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 6, 'tree_dim': 54, 'learning_rate': 0.045966318425528147, 'batch_size': 32, 'num_epochs': 37}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:10,385] Trial 24 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 3, 'tree_dim': 47, 'learning_rate': 0.021284080325913775, 'batch_size': 32, 'num_epochs': 26}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:10,545] Trial 25 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 4, 'tree_dim': 59, 'learning_rate': 0.09371938054177566, 'batch_size': 32, 'num_epochs': 16}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:10,747] Trial 26 finished with value: 0.625 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 53, 'learning_rate': 0.0030167627710408315, 'batch_size': 32, 'num_epochs': 41}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:11,300] Trial 27 finished with value: 0.875 and parameters: {'num_layers': 4, 'num_trees': 6, 'tree_dim': 60, 'learning_rate': 0.0472990763179265, 'batch_size': 128, 'num_epochs': 31}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:11,492] Trial 28 finished with value: 0.625 and parameters: {'num_layers': 2, 'num_trees': 8, 'tree_dim': 41, 'learning_rate': 0.00958853235089028, 'batch_size': 64, 'num_epochs': 17}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:11,748] Trial 29 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 5, 'tree_dim': 60, 'learning_rate': 0.006899130040335343, 'batch_size': 32, 'num_epochs': 23}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:11,895] Trial 30 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 2, 'tree_dim': 33, 'learning_rate': 0.02016242244364073, 'batch_size': 32, 'num_epochs': 51}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:12,394] Trial 31 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 6, 'tree_dim': 29, 'learning_rate': 0.03647387422191126, 'batch_size': 64, 'num_epochs': 60}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:13,077] Trial 32 finished with value: 0.875 and parameters: {'num_layers': 2, 'num_trees': 7, 'tree_dim': 23, 'learning_rate': 0.03497038812077506, 'batch_size': 64, 'num_epochs': 63}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:13,925] Trial 33 finished with value: 0.875 and parameters: {'num_layers': 3, 'num_trees': 6, 'tree_dim': 31, 'learning_rate': 0.02941712354540403, 'batch_size': 64, 'num_epochs': 75}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:15,700] Trial 34 finished with value: 0.875 and parameters: {'num_layers': 5, 'num_trees': 7, 'tree_dim': 19, 'learning_rate': 0.06555874280226258, 'batch_size': 64, 'num_epochs': 55}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:17,524] Trial 35 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 4, 'tree_dim': 38, 'learning_rate': 0.0154768728787783, 'batch_size': 256, 'num_epochs': 38}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:20,081] Trial 36 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 5, 'tree_dim': 39, 'learning_rate': 0.005277290998444672, 'batch_size': 64, 'num_epochs': 65}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:20,723] Trial 37 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 3, 'tree_dim': 57, 'learning_rate': 0.0011895859068256187, 'batch_size': 256, 'num_epochs': 32}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:22,405] Trial 38 finished with value: 0.875 and parameters: {'num_layers': 2, 'num_trees': 9, 'tree_dim': 45, 'learning_rate': 0.05813991603256471, 'batch_size': 32, 'num_epochs': 53}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:24,978] Trial 39 finished with value: 0.75 and parameters: {'num_layers': 5, 'num_trees': 5, 'tree_dim': 26, 'learning_rate': 0.0020190698493136646, 'batch_size': 128, 'num_epochs': 82}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:25,233] Trial 40 finished with value: 0.75 and parameters: {'num_layers': 4, 'num_trees': 6, 'tree_dim': 62, 'learning_rate': 0.003978314588219803, 'batch_size': 256, 'num_epochs': 12}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:25,431] Trial 41 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 64, 'learning_rate': 0.02852459279919651, 'batch_size': 32, 'num_epochs': 26}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:25,557] Trial 42 finished with value: 0.875 and parameters: {'num_layers': 1, 'num_trees': 4, 'tree_dim': 57, 'learning_rate': 0.04895148620264523, 'batch_size': 32, 'num_epochs': 20}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:25,730] Trial 43 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 50, 'learning_rate': 0.03924926033922741, 'batch_size': 32, 'num_epochs': 28}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:26,026] Trial 44 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 7, 'tree_dim': 60, 'learning_rate': 0.01582784313526258, 'batch_size': 32, 'num_epochs': 22}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:26,168] Trial 45 finished with value: 0.75 and parameters: {'num_layers': 1, 'num_trees': 6, 'tree_dim': 62, 'learning_rate': 0.07281274232214921, 'batch_size': 32, 'num_epochs': 15}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:26,390] Trial 46 finished with value: 0.75 and parameters: {'num_layers': 2, 'num_trees': 3, 'tree_dim': 52, 'learning_rate': 0.008797418568008471, 'batch_size': 32, 'num_epochs': 35}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:27,023] Trial 47 finished with value: 0.625 and parameters: {'num_layers': 4, 'num_trees': 4, 'tree_dim': 13, 'learning_rate': 0.0006497764667685338, 'batch_size': 64, 'num_epochs': 47}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:27,656] Trial 48 finished with value: 0.75 and parameters: {'num_layers': 3, 'num_trees': 8, 'tree_dim': 56, 'learning_rate': 0.09352624491466346, 'batch_size': 32, 'num_epochs': 32}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\4111450121.py:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:51:28,031] Trial 49 finished with value: 0.375 and parameters: {'num_layers': 1, 'num_trees': 5, 'tree_dim': 48, 'learning_rate': 0.00014523553534735812, 'batch_size': 128, 'num_epochs': 42}. Best is trial 3 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "NODE                          0.625  0.777778                    0.157083   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "NODE                                           0.000998   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "NODE                                      34.393028   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "NODE                        {'num_layers': 2, 'num_trees': 6, 'tree_dim': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "num_layers: 2\n",
      "num_trees: 6\n",
      "tree_dim: 49\n",
      "learning_rate: 0.002079962530225977\n",
      "batch_size: 32\n",
      "num_epochs: 18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, num_trees, tree_dim, output_dim):\n",
    "        super(NODE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = nn.ModuleList()\n",
    "            for _ in range(num_trees):\n",
    "                tree = nn.Sequential(\n",
    "                    nn.Linear(input_dim, tree_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(tree_dim, 1)\n",
    "                )\n",
    "                layer.append(tree)\n",
    "            self.layers.append(layer)\n",
    "        self.output = nn.Linear(num_layers * num_trees, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tree_outputs = []\n",
    "        for layer in self.layers:\n",
    "            layer_outputs = []\n",
    "            for tree in layer:\n",
    "                layer_outputs.append(tree(x))\n",
    "            layer_output = torch.cat(layer_outputs, dim=1)\n",
    "            tree_outputs.append(layer_output)\n",
    "        x = torch.cat(tree_outputs, dim=1)\n",
    "        return self.output(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for NODE\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    num_trees = trial.suggest_int('num_trees', 1, 10)\n",
    "    tree_dim = trial.suggest_int('tree_dim', 8, 64)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the NODE model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = NODE(input_dim, num_layers, num_trees, tree_dim, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final NODE model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = NODE(input_dim, \n",
    "                  best_params['num_layers'], \n",
    "                  best_params['num_trees'], \n",
    "                  best_params['tree_dim'], \n",
    "                  output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['NODE'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:51:28,267] A new study created in memory with name: no-name-65d91aa5-2e31-4c38-9900-2149d78c7c9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.03484 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.53832 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.39976 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 2.10387 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 1.88961 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 1.67937 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.53497 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 1.47155 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 1.25748 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 9  | loss: 1.15941 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 10 | loss: 1.01234 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 11 | loss: 0.85431 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 12 | loss: 0.7321  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 13 | loss: 0.63131 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 14 | loss: 0.53783 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 15 | loss: 0.45452 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 16 | loss: 0.39329 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 17 | loss: 0.33722 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:29,788] Trial 0 finished with value: 0.5 and parameters: {'n_d': 19, 'n_a': 30, 'n_steps': 5, 'gamma': 1.556564525825605, 'lambda_sparse': 3.1173888195380574e-05, 'learning_rate': 0.002232208624417936, 'batch_size': 256, 'num_epochs': 96}. Best is trial 0 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 0.2822  | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.61421 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 5.22528 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 2  | loss: 5.49748 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 3  | loss: 4.9106  | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 4  | loss: 4.98214 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 5  | loss: 4.70704 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 6  | loss: 4.17931 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 3.97757 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 8  | loss: 3.72314 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 9  | loss: 3.44004 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 10 | loss: 2.91679 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 11 | loss: 2.9444  | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 12 | loss: 2.45651 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 13 | loss: 2.21151 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 14 | loss: 2.38589 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 15 | loss: 2.23473 | val_accuracy: 0.25    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:32,031] Trial 1 finished with value: 0.25 and parameters: {'n_d': 31, 'n_a': 59, 'n_steps': 9, 'gamma': 1.9087487269211894, 'lambda_sparse': 7.258238302276175e-05, 'learning_rate': 0.0007502316990846142, 'batch_size': 128, 'num_epochs': 48}. Best is trial 0 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 2.12412 | val_accuracy: 0.25    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.31908 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.2873  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.03215 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.72689 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.31858 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.24203 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.22771 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.29577 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.05836 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.22151 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.28188 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.1801  | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:33,970] Trial 2 finished with value: 0.75 and parameters: {'n_d': 39, 'n_a': 21, 'n_steps': 8, 'gamma': 1.3028252142102472, 'lambda_sparse': 6.617813696848974e-06, 'learning_rate': 0.010491828327412765, 'batch_size': 32, 'num_epochs': 22}. Best is trial 2 with value: 0.75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.2458  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.31854 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.82017 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.59947 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.58559 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 3  | loss: 0.30334 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.35251 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.1936  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.20486 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.06346 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 8  | loss: 0.10369 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.19772 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.01403 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:35,642] Trial 3 finished with value: 0.875 and parameters: {'n_d': 53, 'n_a': 22, 'n_steps': 9, 'gamma': 1.8647734943223964, 'lambda_sparse': 8.733634015881521e-05, 'learning_rate': 0.018643588068473157, 'batch_size': 128, 'num_epochs': 70}. Best is trial 3 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.06089 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.00304 | val_accuracy: 0.75    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.7942  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 0.84023 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.69361 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.57125 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.68997 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.43867 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.41765 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.2317  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 8  | loss: 0.30155 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.32288 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 10 | loss: 0.12804 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 11 | loss: 0.11992 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 12 | loss: 0.05517 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 13 | loss: 0.02834 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 14 | loss: 0.11071 | val_accuracy: 0.625   |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:36,731] Trial 4 finished with value: 0.75 and parameters: {'n_d': 46, 'n_a': 23, 'n_steps': 4, 'gamma': 1.93500193295748, 'lambda_sparse': 0.0005362243130477063, 'learning_rate': 0.011396648187614777, 'batch_size': 32, 'num_epochs': 39}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.25718 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.87252 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.70745 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.71801 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.83841 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.77875 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 0.43579 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 0.22863 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 8  | loss: 0.12777 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.06096 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.05203 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.04027 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.02368 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.01491 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 14 | loss: 0.01019 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 15 | loss: 0.0071  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 16 | loss: 0.00469 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 17 | loss: 0.00334 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 18 | loss: 0.00233 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 19 | loss: 0.00184 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 20 | loss: 0.00144 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 21 | loss: 0.00116 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 22 | loss: 0.00099 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 23 | loss: 0.00081 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 24 | loss: 0.00069 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 25 | loss: 0.0006  | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 26 | loss: 0.00053 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 27 | loss: 0.00046 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 28 | loss: 0.00041 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 29 | loss: 0.00036 | val_accuracy: 0.5     |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:40,100] Trial 5 finished with value: 0.625 and parameters: {'n_d': 30, 'n_a': 54, 'n_steps': 8, 'gamma': 1.0932220007623157, 'lambda_sparse': 5.37579987029907e-06, 'learning_rate': 0.003877140174416138, 'batch_size': 256, 'num_epochs': 42}. Best is trial 3 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.00033 | val_accuracy: 0.5     |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.70753 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.51915 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 0.415   | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.85367 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.33942 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.10524 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 0.22911 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 7  | loss: 0.32055 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.05427 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.16377 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.01161 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 11 | loss: 0.00219 | val_accuracy: 0.5     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:42,621] Trial 6 finished with value: 0.75 and parameters: {'n_d': 59, 'n_a': 45, 'n_steps': 8, 'gamma': 1.2112221688064326, 'lambda_sparse': 7.955328501488355e-05, 'learning_rate': 0.038786769053035866, 'batch_size': 64, 'num_epochs': 38}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.67921 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 2.58568 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 1.93807 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.46323 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.95907 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 1.46162 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 1.0283  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 7  | loss: 0.95073 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 8  | loss: 0.90242 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 9  | loss: 0.89346 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 10 | loss: 1.30834 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.55264 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 12 | loss: 0.68679 | val_accuracy: 1.0     |  0:00:02s\n",
      "epoch 13 | loss: 0.79067 | val_accuracy: 1.0     |  0:00:02s\n",
      "epoch 14 | loss: 0.68402 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.37625 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.74194 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.63673 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 18 | loss: 0.40871 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 19 | loss: 0.27074 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 20 | loss: 0.3853  | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 21 | loss: 0.48601 | val_accuracy: 0.625   |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:46,600] Trial 7 finished with value: 1.0 and parameters: {'n_d': 57, 'n_a': 17, 'n_steps': 9, 'gamma': 1.3034245899800996, 'lambda_sparse': 7.612386327078212e-06, 'learning_rate': 0.0016123969024413201, 'batch_size': 32, 'num_epochs': 38}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 0.41653 | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_accuracy = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.08827 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 2.94747 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.97685 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.88588 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 2.80256 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 2.72652 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 2.67119 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 2.59109 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 2.54188 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 2.50543 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 10 | loss: 2.46434 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 11 | loss: 2.42132 | val_accuracy: 0.375   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:47,485] Trial 8 finished with value: 0.375 and parameters: {'n_d': 18, 'n_a': 17, 'n_steps': 6, 'gamma': 1.1197562112490531, 'lambda_sparse': 7.865725547242261e-06, 'learning_rate': 0.00021391281369107567, 'batch_size': 128, 'num_epochs': 43}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 2.3676  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 13 | loss: 2.3576  | val_accuracy: 0.25    |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.60874 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 3.4687  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.82399 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 2.69998 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 2.36756 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 1.77063 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.35233 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.66564 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.45678 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.53094 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.34905 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.37621 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.16669 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.16314 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.10188 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.06926 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 16 | loss: 0.05127 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 17 | loss: 0.069   | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 18 | loss: 0.05068 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 19 | loss: 0.03075 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 20 | loss: 0.05058 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 21 | loss: 0.01582 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 22 | loss: 0.01146 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 23 | loss: 0.00709 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 24 | loss: 0.00842 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 25 | loss: 0.00703 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 26 | loss: 0.00532 | val_accuracy: 0.875   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:50,624] Trial 9 finished with value: 0.875 and parameters: {'n_d': 29, 'n_a': 53, 'n_steps': 8, 'gamma': 1.6091211852136809, 'lambda_sparse': 0.0004124409399027308, 'learning_rate': 0.003478145022163928, 'batch_size': 256, 'num_epochs': 49}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.28729 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 6.00497 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 5.75246 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 4.8725  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 6.37624 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 5.94951 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 4.82025 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 5.34019 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 5.28923 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 5.02494 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 5.44393 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 5.40111 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:52,666] Trial 10 finished with value: 0.5 and parameters: {'n_d': 63, 'n_a': 37, 'n_steps': 10, 'gamma': 1.370490145223286, 'lambda_sparse': 2.7312909894349267e-06, 'learning_rate': 0.00011007196691242702, 'batch_size': 32, 'num_epochs': 12}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.62699 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.42588 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.66821 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 3  | loss: 0.63924 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 4  | loss: 0.91726 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 5  | loss: 1.67687 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 0.7964  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.44735 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 8  | loss: 0.52781 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.74244 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 10 | loss: 0.33466 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 11 | loss: 0.04602 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.00491 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 13 | loss: 0.30945 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.36166 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.10807 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.0043  | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 17 | loss: 0.00891 | val_accuracy: 0.75    |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:56,164] Trial 11 finished with value: 0.875 and parameters: {'n_d': 50, 'n_a': 8, 'n_steps': 10, 'gamma': 1.8176929918863265, 'lambda_sparse': 1.0772402330953988e-06, 'learning_rate': 0.07057026510783988, 'batch_size': 128, 'num_epochs': 76}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.71505 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.6601  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.71494 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.75709 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 1.48737 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 1.46083 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.23015 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 1.16298 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 1.09208 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 1.02651 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 1.02277 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 1.04077 | val_accuracy: 0.25    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:51:57,984] Trial 12 finished with value: 0.5 and parameters: {'n_d': 53, 'n_a': 9, 'n_steps': 7, 'gamma': 1.7506342516499096, 'lambda_sparse': 2.6794340864588764e-05, 'learning_rate': 0.0008013126652704683, 'batch_size': 64, 'num_epochs': 70}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.83019 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 13 | loss: 0.76777 | val_accuracy: 0.25    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.92818 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.17947 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.57006 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.54434 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.41692 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.33407 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.09777 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.01912 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.27756 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.22    | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.18488 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.011   | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.01699 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.01931 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.0041  | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.00429 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:00,477] Trial 13 finished with value: 0.625 and parameters: {'n_d': 42, 'n_a': 32, 'n_steps': 10, 'gamma': 1.4293380668114093, 'lambda_sparse': 0.00014271626679335613, 'learning_rate': 0.017185852121093637, 'batch_size': 128, 'num_epochs': 65}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.17872 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.88954 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 2.67798 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.71385 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 2.09457 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 1.91457 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 1.99209 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 1.72819 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 8  | loss: 1.7439  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 1.1715  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 1.59925 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 1.40371 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 1.65445 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 13 | loss: 1.29405 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 14 | loss: 1.47996 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 1.25678 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 1.04543 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 17 | loss: 1.16785 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 18 | loss: 1.19331 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 19 | loss: 1.04646 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 1.21612 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:02,756] Trial 14 finished with value: 0.875 and parameters: {'n_d': 56, 'n_a': 17, 'n_steps': 6, 'gamma': 1.664340522279427, 'lambda_sparse': 1.7659617696551092e-05, 'learning_rate': 0.0009069182415799667, 'batch_size': 32, 'num_epochs': 87}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.73375 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 1  | loss: 0.7706  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.38852 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.14676 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.05365 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.02394 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.22996 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.19586 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 8  | loss: 0.03438 | val_accuracy: 0.375   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:03,483] Trial 15 finished with value: 0.75 and parameters: {'n_d': 64, 'n_a': 26, 'n_steps': 3, 'gamma': 1.471874723678618, 'lambda_sparse': 0.00016518329868424274, 'learning_rate': 0.018406685124564573, 'batch_size': 128, 'num_epochs': 62}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.01615 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 10 | loss: 0.01313 | val_accuracy: 0.375   |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.01481 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 2.01307 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 2  | loss: 0.90863 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.2217  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.2983  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.18222 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 6  | loss: 0.11671 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.09933 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.07166 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.1104  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.07476 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 11 | loss: 0.13674 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 12 | loss: 0.05318 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 13 | loss: 0.09581 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 14 | loss: 0.02542 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 15 | loss: 0.03673 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.06489 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.05001 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.00806 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 19 | loss: 0.12218 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 20 | loss: 0.02502 | val_accuracy: 0.75    |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:06,969] Trial 16 finished with value: 0.875 and parameters: {'n_d': 53, 'n_a': 39, 'n_steps': 9, 'gamma': 1.0066330727174586, 'lambda_sparse': 1.4583266652296632e-05, 'learning_rate': 0.006704314033719113, 'batch_size': 32, 'num_epochs': 24}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 0.04478 | val_accuracy: 0.75    |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.59057 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 2.49338 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.84202 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 0.81008 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.51199 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.33289 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.44866 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 7  | loss: 0.14166 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.027   | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.0755  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.11924 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.00571 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.05179 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.00189 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.15819 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 15 | loss: 0.00259 | val_accuracy: 0.625   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:09,287] Trial 17 finished with value: 0.875 and parameters: {'n_d': 46, 'n_a': 14, 'n_steps': 9, 'gamma': 1.2774293943692865, 'lambda_sparse': 5.878636232830251e-05, 'learning_rate': 0.03823478571651991, 'batch_size': 64, 'num_epochs': 80}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 0.00293 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.18282 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.30465 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.19266 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.30917 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 4  | loss: 2.25482 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 5  | loss: 2.25161 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 6  | loss: 2.14871 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 7  | loss: 2.2743  | val_accuracy: 0.25    |  0:00:02s\n",
      "epoch 8  | loss: 2.17142 | val_accuracy: 0.25    |  0:00:02s\n",
      "epoch 9  | loss: 2.19935 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 10 | loss: 2.06765 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 11 | loss: 2.21652 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 12 | loss: 2.11195 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 13 | loss: 2.13165 | val_accuracy: 0.25    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:12,369] Trial 18 finished with value: 0.5 and parameters: {'n_d': 12, 'n_a': 27, 'n_steps': 7, 'gamma': 1.770242394528392, 'lambda_sparse': 2.1052703485076523e-06, 'learning_rate': 0.00039554843784019635, 'batch_size': 128, 'num_epochs': 57}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.61122 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 2.09753 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 2  | loss: 1.77585 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.91886 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.26659 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 1.31263 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.74004 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 7  | loss: 1.3632  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 1.30278 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 9  | loss: 1.58022 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 1.16553 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 0.73444 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 0.876   | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 13 | loss: 1.08539 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 14 | loss: 1.00669 | val_accuracy: 0.25    |  0:00:02s\n",
      "epoch 15 | loss: 0.48352 | val_accuracy: 0.25    |  0:00:02s\n",
      "epoch 16 | loss: 0.51201 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 17 | loss: 0.59208 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 18 | loss: 0.46957 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 19 | loss: 0.33994 | val_accuracy: 0.375   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:15,278] Trial 19 finished with value: 0.5 and parameters: {'n_d': 60, 'n_a': 43, 'n_steps': 9, 'gamma': 1.9985500692918534, 'lambda_sparse': 0.0002505328443623411, 'learning_rate': 0.0015361400779721618, 'batch_size': 32, 'num_epochs': 28}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.98261 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.31205 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.70144 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.80403 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.2485  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 1.02459 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 1.29502 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 0.55924 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 8  | loss: 0.78842 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 1.40273 | val_accuracy: 0.375   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:16,614] Trial 20 finished with value: 0.5 and parameters: {'n_d': 36, 'n_a': 14, 'n_steps': 7, 'gamma': 1.6615679765897018, 'lambda_sparse': 1.329836744785016e-05, 'learning_rate': 0.005642610785196317, 'batch_size': 32, 'num_epochs': 98}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 0.67516 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 0.75441 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.20448 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 4.81812 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 4.14604 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 3.86926 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 3.20738 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 3.61815 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 3.4245  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 7  | loss: 2.80176 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 2.98066 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 2.92229 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 2.56568 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 2.20845 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 1.95458 | val_accuracy: 0.5     |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:18,983] Trial 21 finished with value: 0.625 and parameters: {'n_d': 27, 'n_a': 52, 'n_steps': 8, 'gamma': 1.5660068535545482, 'lambda_sparse': 0.00019010641457810245, 'learning_rate': 0.0017352455857720394, 'batch_size': 256, 'num_epochs': 53}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 1.76727 | val_accuracy: 0.5     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.08751 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 1.70684 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.40799 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.24302 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 0.98645 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.91391 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.83267 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 0.8329  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 0.70898 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.45448 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.39284 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.23079 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.15603 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.10677 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.13571 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 15 | loss: 0.10789 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 16 | loss: 0.19593 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 17 | loss: 0.23181 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 18 | loss: 0.11508 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 19 | loss: 0.11437 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 0.08268 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 21 | loss: 0.06206 | val_accuracy: 0.75    |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:22,275] Trial 22 finished with value: 0.75 and parameters: {'n_d': 24, 'n_a': 50, 'n_steps': 9, 'gamma': 1.6197556849657164, 'lambda_sparse': 0.0005996580319049698, 'learning_rate': 0.0036467289090384423, 'batch_size': 256, 'num_epochs': 33}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 0.03691 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 23 | loss: 0.03776 | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.50954 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.52993 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.5343  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 0.19195 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.38341 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.68579 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 1.04034 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.29411 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 8  | loss: 0.20413 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 9  | loss: 0.43365 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.11341 | val_accuracy: 1.0     |  0:00:01s\n",
      "epoch 11 | loss: 0.00174 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.00292 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.02788 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 14 | loss: 0.15153 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.0078  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 0.00702 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 17 | loss: 0.00062 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.00106 | val_accuracy: 0.75    |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:24,821] Trial 23 finished with value: 1.0 and parameters: {'n_d': 34, 'n_a': 62, 'n_steps': 8, 'gamma': 1.4000309104710151, 'lambda_sparse': 0.0003778996182318105, 'learning_rate': 0.08209135334861486, 'batch_size': 256, 'num_epochs': 51}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.00217 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 0.00233 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.81028 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.14348 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.06397 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.32671 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 1.46931 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 2.95948 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 1.08031 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.45538 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.20974 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.27192 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.22217 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.03842 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 12 | loss: 0.00451 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.02419 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.00891 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.00884 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.09389 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.02803 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.00263 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 19 | loss: 0.08489 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 0.27116 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 21 | loss: 0.19407 | val_accuracy: 0.75    |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:28,292] Trial 24 finished with value: 0.875 and parameters: {'n_d': 35, 'n_a': 63, 'n_steps': 10, 'gamma': 1.3892455490696232, 'lambda_sparse': 0.00035834842756476206, 'learning_rate': 0.0929160914636805, 'batch_size': 256, 'num_epochs': 58}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.94252 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 1  | loss: 2.92783 | val_accuracy: 0.625   |  0:00:14s\n",
      "epoch 2  | loss: 1.8776  | val_accuracy: 0.5     |  0:00:14s\n",
      "epoch 3  | loss: 0.39668 | val_accuracy: 0.875   |  0:00:14s\n",
      "epoch 4  | loss: 0.8244  | val_accuracy: 0.875   |  0:00:15s\n",
      "epoch 5  | loss: 0.13624 | val_accuracy: 0.75    |  0:00:16s\n",
      "epoch 6  | loss: 0.00661 | val_accuracy: 0.625   |  0:00:16s\n",
      "epoch 7  | loss: 0.00992 | val_accuracy: 0.5     |  0:00:16s\n",
      "epoch 8  | loss: 0.02594 | val_accuracy: 0.5     |  0:00:16s\n",
      "epoch 9  | loss: 0.02778 | val_accuracy: 0.625   |  0:00:17s\n",
      "epoch 10 | loss: 0.00368 | val_accuracy: 0.75    |  0:00:18s\n",
      "epoch 11 | loss: 0.00249 | val_accuracy: 0.5     |  0:00:18s\n",
      "epoch 12 | loss: 0.00579 | val_accuracy: 0.625   |  0:00:19s\n",
      "epoch 13 | loss: 0.00225 | val_accuracy: 0.5     |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:52:50,195] Trial 25 finished with value: 0.875 and parameters: {'n_d': 46, 'n_a': 31, 'n_steps': 9, 'gamma': 1.2162827155261575, 'lambda_sparse': 0.0009224467293671625, 'learning_rate': 0.04218319001712552, 'batch_size': 256, 'num_epochs': 71}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.05701 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 0.98471 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.37495 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 3  | loss: 0.18087 | val_accuracy: 0.75    |  0:00:07s\n",
      "epoch 4  | loss: 0.04718 | val_accuracy: 0.75    |  0:00:10s\n",
      "epoch 5  | loss: 0.02249 | val_accuracy: 0.625   |  0:00:10s\n",
      "epoch 6  | loss: 0.01301 | val_accuracy: 0.625   |  0:00:10s\n",
      "epoch 7  | loss: 0.00582 | val_accuracy: 0.625   |  0:00:10s\n",
      "epoch 8  | loss: 0.0029  | val_accuracy: 0.625   |  0:00:11s\n",
      "epoch 9  | loss: 0.00123 | val_accuracy: 0.625   |  0:00:11s\n",
      "epoch 10 | loss: 0.00076 | val_accuracy: 0.625   |  0:00:11s\n",
      "epoch 11 | loss: 0.00063 | val_accuracy: 0.625   |  0:00:11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:02,302] Trial 26 finished with value: 0.75 and parameters: {'n_d': 42, 'n_a': 20, 'n_steps': 7, 'gamma': 1.4995597462009012, 'lambda_sparse': 0.00010417417307943548, 'learning_rate': 0.027300145768136823, 'batch_size': 128, 'num_epochs': 83}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.0005  | val_accuracy: 0.625   |  0:00:11s\n",
      "epoch 13 | loss: 0.00027 | val_accuracy: 0.625   |  0:00:11s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8.58302 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.60218 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.49392 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 3  | loss: 0.14777 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 4  | loss: 1.68633 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 5  | loss: 0.38713 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 6  | loss: 0.04107 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 7  | loss: 0.16581 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.0012  | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 9  | loss: 0.00023 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 10 | loss: 0.00123 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 11 | loss: 0.00619 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.01992 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 0.00416 | val_accuracy: 0.625   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:05,376] Trial 27 finished with value: 0.875 and parameters: {'n_d': 51, 'n_a': 64, 'n_steps': 8, 'gamma': 1.3314472931647428, 'lambda_sparse': 4.8103950060965734e-05, 'learning_rate': 0.06366478644846081, 'batch_size': 64, 'num_epochs': 66}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.15037 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 15 | loss: 0.00035 | val_accuracy: 0.875   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.56352 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 1.27093 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.65445 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 0.37594 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.29956 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.18693 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.11478 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.04857 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.02334 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.01064 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.00861 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.00339 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.00231 | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:07,096] Trial 28 finished with value: 0.75 and parameters: {'n_d': 56, 'n_a': 13, 'n_steps': 6, 'gamma': 1.2282981017988324, 'lambda_sparse': 4.2605455733059956e-05, 'learning_rate': 0.007476064504673291, 'batch_size': 256, 'num_epochs': 16}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 0.0017  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.00139 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.00107 | val_accuracy: 0.75    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.53035 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 0.73835 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.83482 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.78673 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.5351  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.22226 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.18687 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.39319 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 8  | loss: 0.03184 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 9  | loss: 0.17685 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 10 | loss: 0.20432 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.10829 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.14512 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.09924 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:08,801] Trial 29 finished with value: 0.75 and parameters: {'n_d': 22, 'n_a': 34, 'n_steps': 5, 'gamma': 1.5453551803432573, 'lambda_sparse': 2.8019504543914807e-05, 'learning_rate': 0.019899916798499444, 'batch_size': 32, 'num_epochs': 33}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.09597 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 15 | loss: 0.03303 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9.69684 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 9.49331 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 8.67491 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 7.7418  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 7.81776 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 7.24997 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 6.59123 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 7  | loss: 6.13259 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 8  | loss: 5.31325 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 5.30109 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 5.00581 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 4.80086 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 4.1359  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 13 | loss: 3.71915 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 14 | loss: 3.09687 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 15 | loss: 2.84551 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 16 | loss: 2.30726 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 17 | loss: 1.74393 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 1.45779 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 19 | loss: 1.06583 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 0.78611 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 21 | loss: 0.66338 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 22 | loss: 0.38538 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 23 | loss: 0.29727 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 24 | loss: 0.32712 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 25 | loss: 0.26524 | val_accuracy: 0.75    |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:12,481] Trial 30 finished with value: 0.75 and parameters: {'n_d': 34, 'n_a': 26, 'n_steps': 10, 'gamma': 1.436883085504353, 'lambda_sparse': 0.00025772099482927627, 'learning_rate': 0.0018471330038734779, 'batch_size': 128, 'num_epochs': 90}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | loss: 0.31392 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 27 | loss: 0.44991 | val_accuracy: 0.5     |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.25444 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.65625 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.52875 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 2.32448 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 1.51845 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 1.62165 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 1.26487 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.9661  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.8154  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.68673 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.62962 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.56533 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.34272 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.42452 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.30016 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 15 | loss: 0.31908 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 16 | loss: 0.24779 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 17 | loss: 0.35992 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 18 | loss: 0.405   | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 19 | loss: 0.18575 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 20 | loss: 0.12296 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 21 | loss: 0.091   | val_accuracy: 0.625   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:15,172] Trial 31 finished with value: 0.75 and parameters: {'n_d': 30, 'n_a': 58, 'n_steps': 8, 'gamma': 1.8104785951383606, 'lambda_sparse': 0.00041663029993119287, 'learning_rate': 0.0026842239549012864, 'batch_size': 256, 'num_epochs': 49}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 0.07127 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 7.34614 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 6.69593 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 6.31023 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 5.95487 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 5.54906 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 4.63043 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 4.62921 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 4.57672 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 4.36561 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 4.24154 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 3.93954 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 3.53632 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:16,913] Trial 32 finished with value: 0.5 and parameters: {'n_d': 40, 'n_a': 58, 'n_steps': 9, 'gamma': 1.716182192887831, 'lambda_sparse': 0.0009687929089092655, 'learning_rate': 0.0010524078524907596, 'batch_size': 256, 'num_epochs': 51}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 3.30935 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.6894  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.74136 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 3.77732 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 3.28272 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 3.34689 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 3.43669 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 2.87018 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 2.92976 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 3.02573 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 9  | loss: 3.06043 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 10 | loss: 2.91804 | val_accuracy: 0.25    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:18,941] Trial 33 finished with value: 0.5 and parameters: {'n_d': 18, 'n_a': 48, 'n_steps': 8, 'gamma': 1.8552574870081273, 'lambda_sparse': 0.00012986004573661716, 'learning_rate': 0.0005306945342287403, 'batch_size': 256, 'num_epochs': 45}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 2.95775 | val_accuracy: 0.25    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.27622 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 1.54457 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.99091 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.45791 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.4306  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.35022 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.30895 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.18397 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.21368 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.27595 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.26252 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 11 | loss: 0.19254 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 12 | loss: 0.08032 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 13 | loss: 0.20193 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 14 | loss: 0.07799 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 15 | loss: 0.08393 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 16 | loss: 0.06866 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 17 | loss: 0.05025 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.03179 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 19 | loss: 0.02635 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 20 | loss: 0.02792 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 21 | loss: 0.01356 | val_accuracy: 0.75    |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:21,888] Trial 34 finished with value: 0.875 and parameters: {'n_d': 33, 'n_a': 60, 'n_steps': 9, 'gamma': 1.5747964174154367, 'lambda_sparse': 0.0002795800144439728, 'learning_rate': 0.0048401622106278444, 'batch_size': 256, 'num_epochs': 59}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.53506 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 1.93407 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 0.9458  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 0.65964 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.33068 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.2742  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.21756 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.22453 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 8  | loss: 0.15182 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.13861 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.08897 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.07109 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.0511  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.02488 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.01209 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 15 | loss: 0.00754 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 16 | loss: 0.00573 | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:24,003] Trial 35 finished with value: 0.875 and parameters: {'n_d': 27, 'n_a': 55, 'n_steps': 8, 'gamma': 1.3560203777597766, 'lambda_sparse': 0.0006109977276838761, 'learning_rate': 0.011151200682186939, 'batch_size': 256, 'num_epochs': 35}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 0.01228 | val_accuracy: 0.875   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.74028 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.30749 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 2.35085 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.41886 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 1.17023 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.7196  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 1.0612  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.55987 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 0.57256 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.64435 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 10 | loss: 0.47835 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.5094  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.35975 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.19583 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:25,674] Trial 36 finished with value: 0.625 and parameters: {'n_d': 38, 'n_a': 21, 'n_steps': 7, 'gamma': 1.8910367133927672, 'lambda_sparse': 8.402652956468465e-05, 'learning_rate': 0.010194634368893502, 'batch_size': 32, 'num_epochs': 45}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.42521 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 15 | loss: 0.23606 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.56526 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 3.46152 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.7219  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.61589 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 2.51223 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 2.33208 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 2.19197 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 1.88667 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 8  | loss: 1.63369 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 9  | loss: 1.33703 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 10 | loss: 1.06723 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 11 | loss: 0.90419 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 12 | loss: 0.68899 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 13 | loss: 0.60022 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 14 | loss: 0.46608 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 15 | loss: 0.33382 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:27,623] Trial 37 finished with value: 0.375 and parameters: {'n_d': 13, 'n_a': 40, 'n_steps': 8, 'gamma': 1.2711887728714317, 'lambda_sparse': 5.398951550280452e-06, 'learning_rate': 0.0029726550955297255, 'batch_size': 256, 'num_epochs': 52}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.30705 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 5.30685 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 5.87369 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 5.80424 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 5.76852 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 5.16187 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 5.15159 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 5.1333  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 4.91867 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 5.03198 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 4.7411  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 4.5355  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 4.75449 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:29,540] Trial 38 finished with value: 0.5 and parameters: {'n_d': 49, 'n_a': 61, 'n_steps': 9, 'gamma': 1.9657739566087793, 'lambda_sparse': 8.76570981998317e-06, 'learning_rate': 0.0003305792552893228, 'batch_size': 128, 'num_epochs': 39}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.45463 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 3.6097  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 2.31082 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.05252 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.44635 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.2838  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.28314 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.4186  | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 8  | loss: 0.02638 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 9  | loss: 0.01321 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 10 | loss: 0.03048 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 11 | loss: 0.02925 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.04323 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.01548 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 14 | loss: 0.01124 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:31,155] Trial 39 finished with value: 0.875 and parameters: {'n_d': 59, 'n_a': 46, 'n_steps': 5, 'gamma': 1.1420448082266055, 'lambda_sparse': 0.0003877293264322241, 'learning_rate': 0.0995487290230079, 'batch_size': 64, 'num_epochs': 47}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.0335  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 0.01191 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 17 | loss: 0.01389 | val_accuracy: 0.75    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.86226 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 2.14555 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 2  | loss: 2.33664 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.1147  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.70731 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 1.56256 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 1.51119 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 7  | loss: 1.63218 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 1.27548 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 1.15785 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 1.54117 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 0.88421 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 12 | loss: 1.41877 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 13 | loss: 1.28524 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 14 | loss: 1.04793 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 15 | loss: 1.02044 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 16 | loss: 1.03901 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 17 | loss: 0.70481 | val_accuracy: 0.5     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:34,071] Trial 40 finished with value: 0.5 and parameters: {'n_d': 31, 'n_a': 55, 'n_steps': 10, 'gamma': 1.410770211394871, 'lambda_sparse': 3.0858934323569277e-06, 'learning_rate': 0.0012561539599503005, 'batch_size': 32, 'num_epochs': 71}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.50669 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.94786 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.10407 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 3  | loss: 3.02331 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.3522  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.31021 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 0.15172 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 0.74175 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 0.07827 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.04687 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.03097 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.02096 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 0.15248 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 13 | loss: 0.14754 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 14 | loss: 0.00775 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 15 | loss: 0.01134 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 16 | loss: 0.02112 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 17 | loss: 0.00288 | val_accuracy: 0.625   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:36,806] Trial 41 finished with value: 0.625 and parameters: {'n_d': 49, 'n_a': 8, 'n_steps': 10, 'gamma': 1.8434257186106227, 'lambda_sparse': 1.2511043318765517e-06, 'learning_rate': 0.056962684365044376, 'batch_size': 128, 'num_epochs': 78}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | loss: 0.00186 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.85427 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 6.08919 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 9.60929 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 6.73365 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 2.61528 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.75579 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 1.4514  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.51622 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.57949 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 2.45661 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.3574  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.06612 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 12 | loss: 0.10445 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.17816 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 14 | loss: 0.90304 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.26538 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 16 | loss: 0.06999 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 17 | loss: 0.16295 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 18 | loss: 0.12045 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 19 | loss: 0.1129  | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 20 | loss: 0.18739 | val_accuracy: 0.375   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:40,394] Trial 42 finished with value: 0.875 and parameters: {'n_d': 56, 'n_a': 10, 'n_steps': 10, 'gamma': 1.9344703406205523, 'lambda_sparse': 1.2709591478948665e-06, 'learning_rate': 0.07035996153229228, 'batch_size': 128, 'num_epochs': 74}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9.09274 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 2.61489 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 2  | loss: 0.81    | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 3  | loss: 0.53626 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 4  | loss: 0.35563 | val_accuracy: 0.625   |  0:00:03s\n",
      "epoch 5  | loss: 1.17934 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 6  | loss: 0.66638 | val_accuracy: 0.75    |  0:00:04s\n",
      "epoch 7  | loss: 0.37204 | val_accuracy: 0.375   |  0:00:04s\n",
      "epoch 8  | loss: 0.08292 | val_accuracy: 0.375   |  0:00:04s\n",
      "epoch 9  | loss: 0.04339 | val_accuracy: 0.375   |  0:00:05s\n",
      "epoch 10 | loss: 0.00999 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 11 | loss: 0.01835 | val_accuracy: 0.875   |  0:00:05s\n",
      "epoch 12 | loss: 0.00979 | val_accuracy: 0.5     |  0:00:05s\n",
      "epoch 13 | loss: 0.00502 | val_accuracy: 0.5     |  0:00:05s\n",
      "epoch 14 | loss: 0.00201 | val_accuracy: 0.625   |  0:00:05s\n",
      "epoch 15 | loss: 0.0009  | val_accuracy: 0.625   |  0:00:06s\n",
      "epoch 16 | loss: 0.00054 | val_accuracy: 0.5     |  0:00:06s\n",
      "epoch 17 | loss: 0.09501 | val_accuracy: 0.5     |  0:00:06s\n",
      "epoch 18 | loss: 0.00068 | val_accuracy: 0.625   |  0:00:06s\n",
      "epoch 19 | loss: 0.00079 | val_accuracy: 0.625   |  0:00:06s\n",
      "epoch 20 | loss: 0.00213 | val_accuracy: 0.625   |  0:00:06s\n",
      "epoch 21 | loss: 0.0007  | val_accuracy: 0.75    |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:47,574] Trial 43 finished with value: 0.875 and parameters: {'n_d': 44, 'n_a': 17, 'n_steps': 9, 'gamma': 1.7071121392012623, 'lambda_sparse': 4.0719079125513675e-06, 'learning_rate': 0.02604007937116507, 'batch_size': 128, 'num_epochs': 63}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.87416 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 4.44346 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.51336 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.53906 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 2.86004 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 2.7204  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 1.86495 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 1.46425 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 8  | loss: 0.16453 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.48328 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.29292 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.02188 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.00475 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 0.00555 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.13566 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.00131 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.56735 | val_accuracy: 0.75    |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:50,907] Trial 44 finished with value: 0.875 and parameters: {'n_d': 62, 'n_a': 11, 'n_steps': 10, 'gamma': 1.7970566548536064, 'lambda_sparse': 9.999725497663805e-06, 'learning_rate': 0.049170204709412905, 'batch_size': 128, 'num_epochs': 41}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 0.29242 | val_accuracy: 0.75    |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 7.07223 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.59616 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 3.50183 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 3.06094 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 1.60873 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.7972  | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 6  | loss: 0.28243 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.63912 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.15636 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.03843 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.63422 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.20867 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.43948 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.02503 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:53,424] Trial 45 finished with value: 0.875 and parameters: {'n_d': 52, 'n_a': 17, 'n_steps': 9, 'gamma': 1.8737960839093863, 'lambda_sparse': 0.000199044348626163, 'learning_rate': 0.029104411032351937, 'batch_size': 128, 'num_epochs': 67}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.10296 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.05144 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.59927 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 7.2009  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 0.92298 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 3.01105 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 2.99071 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.83669 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.10073 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.10033 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.34649 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.28345 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 10 | loss: 0.34854 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.31078 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 0.00656 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.00196 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 14 | loss: 0.05386 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 15 | loss: 0.00157 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 16 | loss: 0.00157 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.00444 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.38696 | val_accuracy: 0.75    |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:56,287] Trial 46 finished with value: 0.875 and parameters: {'n_d': 55, 'n_a': 23, 'n_steps': 8, 'gamma': 1.4786143695514284, 'lambda_sparse': 1.8233110408444885e-06, 'learning_rate': 0.08522082139195931, 'batch_size': 128, 'num_epochs': 92}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.11405 | val_accuracy: 0.875   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.4344  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.41192 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.23978 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.29479 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 1.04786 | val_accuracy: 1.0     |  0:00:00s\n",
      "epoch 5  | loss: 1.1175  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 6  | loss: 0.31609 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.45109 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.3147  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 9  | loss: 0.24384 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 10 | loss: 0.52912 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.27221 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.33142 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 13 | loss: 0.57567 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.53797 | val_accuracy: 1.0     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:53:58,978] Trial 47 finished with value: 1.0 and parameters: {'n_d': 27, 'n_a': 28, 'n_steps': 10, 'gamma': 1.3087236723184485, 'lambda_sparse': 3.6010095166494255e-05, 'learning_rate': 0.032136943711229576, 'batch_size': 32, 'num_epochs': 84}. Best is trial 7 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.64556 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.65761 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.01829 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.6276  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.3913  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.28392 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.25716 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.2219  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 8  | loss: 0.28046 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 9  | loss: 0.10343 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 10 | loss: 0.09104 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 11 | loss: 0.25747 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 12 | loss: 0.14195 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 13 | loss: 0.40013 | val_accuracy: 0.375   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:54:00,225] Trial 48 finished with value: 0.625 and parameters: {'n_d': 28, 'n_a': 34, 'n_steps': 3, 'gamma': 1.3369495069191328, 'lambda_sparse': 2.3445638971758493e-05, 'learning_rate': 0.01572585410784491, 'batch_size': 32, 'num_epochs': 82}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.05411 | val_accuracy: 0.25    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\716812650.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.58947 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 4.02601 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.98202 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 2.03288 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 1.39047 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 5  | loss: 0.84137 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 0.67585 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.34734 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.2705  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.24225 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 10 | loss: 0.38281 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 11 | loss: 0.16443 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.21916 | val_accuracy: 0.5     |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:54:03,463] Trial 49 finished with value: 0.75 and parameters: {'n_d': 23, 'n_a': 29, 'n_steps': 9, 'gamma': 1.1641557272828789, 'lambda_sparse': 6.600465112439704e-05, 'learning_rate': 0.007821917938746453, 'batch_size': 32, 'num_epochs': 55}. Best is trial 7 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 0.29652 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.67921 | val_accuracy: 0.0     |  0:00:00s\n",
      "epoch 1  | loss: 2.58568 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 1.93807 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.46323 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 4  | loss: 0.95907 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 5  | loss: 1.46162 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 1.0283  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 7  | loss: 0.95073 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 8  | loss: 0.90242 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 9  | loss: 0.89346 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 10 | loss: 1.30834 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 11 | loss: 0.55264 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 12 | loss: 0.68679 | val_accuracy: 1.0     |  0:00:02s\n",
      "epoch 13 | loss: 0.79067 | val_accuracy: 1.0     |  0:00:02s\n",
      "epoch 14 | loss: 0.68402 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 15 | loss: 0.37625 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 16 | loss: 0.74194 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 17 | loss: 0.63673 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 18 | loss: 0.40871 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 19 | loss: 0.27074 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 20 | loss: 0.3853  | val_accuracy: 0.75    |  0:00:04s\n",
      "epoch 21 | loss: 0.48601 | val_accuracy: 0.625   |  0:00:04s\n",
      "epoch 22 | loss: 0.41653 | val_accuracy: 0.625   |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_accuracy = 1.0\n",
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "NODE                          0.625  0.777778                    0.157083   \n",
      "TabNet                          1.0       1.0                     4.49895   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.034014   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "NODE                                      34.393028   \n",
      "TabNet                                   159.837336   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "NODE                        {'num_layers': 2, 'num_trees': 6, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 57, 'n_a': 17, 'n_steps': 9, 'gamma': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_d: 57\n",
      "n_a: 17\n",
      "n_steps: 9\n",
      "gamma: 1.3034245899800996\n",
      "lambda_sparse: 7.612386327078212e-06\n",
      "learning_rate: 0.0016123969024413201\n",
      "batch_size: 32\n",
      "num_epochs: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for TabNet\n",
    "    n_d = trial.suggest_int('n_d', 8, 64)\n",
    "    n_a = trial.suggest_int('n_a', 8, 64)\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the TabNet model\n",
    "    model = TabNetClassifier(\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        device_name=device\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        X_train=X_train_scaled, y_train=y_train.values,\n",
    "        eval_set=[(X_test_scaled, y_test.values)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=10,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=batch_size // 2,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final TabNet model with the best hyperparameters\n",
    "best_model = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "    device_name=device\n",
    ")\n",
    "\n",
    "training_start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train=X_train_scaled, y_train=y_train.values,\n",
    "    eval_set=[(X_test_scaled, y_test.values)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=best_params['num_epochs'],\n",
    "    patience=10,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    virtual_batch_size=best_params['batch_size'] // 2,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_start_time = time.time()\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['TabNet'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:54:08,233] A new study created in memory with name: no-name-31e1a594-edde-41c5-8805-881e363c58f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:08,703] Trial 0 finished with value: 0.75 and parameters: {'hidden_dim': 32, 'learning_rate': 0.008869898978533712, 'batch_size': 128, 'num_epochs': 83}. Best is trial 0 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:08,881] Trial 1 finished with value: 1.0 and parameters: {'hidden_dim': 86, 'learning_rate': 0.09549506729887208, 'batch_size': 128, 'num_epochs': 48}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:12,193] Trial 2 finished with value: 0.5 and parameters: {'hidden_dim': 37, 'learning_rate': 0.0003143798772683211, 'batch_size': 128, 'num_epochs': 74}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:12,563] Trial 3 finished with value: 0.75 and parameters: {'hidden_dim': 240, 'learning_rate': 0.009857933135696561, 'batch_size': 128, 'num_epochs': 59}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:13,664] Trial 4 finished with value: 0.75 and parameters: {'hidden_dim': 205, 'learning_rate': 0.0022622545055628043, 'batch_size': 32, 'num_epochs': 71}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,042] Trial 5 finished with value: 0.875 and parameters: {'hidden_dim': 114, 'learning_rate': 0.000236212309539722, 'batch_size': 128, 'num_epochs': 87}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,204] Trial 6 finished with value: 0.75 and parameters: {'hidden_dim': 125, 'learning_rate': 0.0009687572389833862, 'batch_size': 64, 'num_epochs': 52}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,327] Trial 7 finished with value: 0.375 and parameters: {'hidden_dim': 40, 'learning_rate': 0.00010976942256914482, 'batch_size': 64, 'num_epochs': 36}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,488] Trial 8 finished with value: 0.875 and parameters: {'hidden_dim': 70, 'learning_rate': 0.0015928594260852522, 'batch_size': 64, 'num_epochs': 47}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,750] Trial 9 finished with value: 0.75 and parameters: {'hidden_dim': 200, 'learning_rate': 0.004060190199292234, 'batch_size': 256, 'num_epochs': 36}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:14,867] Trial 10 finished with value: 0.875 and parameters: {'hidden_dim': 91, 'learning_rate': 0.07117610281987591, 'batch_size': 32, 'num_epochs': 14}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:15,504] Trial 11 finished with value: 0.875 and parameters: {'hidden_dim': 134, 'learning_rate': 0.07843395713130019, 'batch_size': 128, 'num_epochs': 97}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:15,905] Trial 12 finished with value: 0.875 and parameters: {'hidden_dim': 97, 'learning_rate': 0.031282874573228946, 'batch_size': 128, 'num_epochs': 98}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:15,990] Trial 13 finished with value: 0.75 and parameters: {'hidden_dim': 171, 'learning_rate': 0.0004592348484294914, 'batch_size': 256, 'num_epochs': 18}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:16,207] Trial 14 finished with value: 0.5 and parameters: {'hidden_dim': 95, 'learning_rate': 0.00013361524456238522, 'batch_size': 128, 'num_epochs': 69}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:16,594] Trial 15 finished with value: 0.75 and parameters: {'hidden_dim': 152, 'learning_rate': 0.018912950354406585, 'batch_size': 128, 'num_epochs': 86}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:16,728] Trial 16 finished with value: 0.375 and parameters: {'hidden_dim': 68, 'learning_rate': 0.0005542077566712213, 'batch_size': 128, 'num_epochs': 37}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:16,926] Trial 17 finished with value: 0.75 and parameters: {'hidden_dim': 118, 'learning_rate': 0.004882036047744012, 'batch_size': 256, 'num_epochs': 61}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:17,032] Trial 18 finished with value: 0.75 and parameters: {'hidden_dim': 163, 'learning_rate': 0.04070914554368885, 'batch_size': 32, 'num_epochs': 24}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:17,367] Trial 19 finished with value: 0.625 and parameters: {'hidden_dim': 63, 'learning_rate': 0.0011241049816930547, 'batch_size': 128, 'num_epochs': 82}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:17,593] Trial 20 finished with value: 0.5 and parameters: {'hidden_dim': 114, 'learning_rate': 0.0002465200751882556, 'batch_size': 128, 'num_epochs': 47}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:17,745] Trial 21 finished with value: 0.75 and parameters: {'hidden_dim': 68, 'learning_rate': 0.0019341979532170152, 'batch_size': 64, 'num_epochs': 46}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:17,895] Trial 22 finished with value: 0.75 and parameters: {'hidden_dim': 77, 'learning_rate': 0.0009125838565721521, 'batch_size': 64, 'num_epochs': 43}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:18,007] Trial 23 finished with value: 0.625 and parameters: {'hidden_dim': 107, 'learning_rate': 0.00845323415176841, 'batch_size': 64, 'num_epochs': 29}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:18,313] Trial 24 finished with value: 0.625 and parameters: {'hidden_dim': 58, 'learning_rate': 0.00018895915297573065, 'batch_size': 64, 'num_epochs': 55}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:19,073] Trial 25 finished with value: 0.75 and parameters: {'hidden_dim': 81, 'learning_rate': 0.001569850412557571, 'batch_size': 64, 'num_epochs': 62}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:19,289] Trial 26 finished with value: 0.75 and parameters: {'hidden_dim': 137, 'learning_rate': 0.0006458835466231139, 'batch_size': 256, 'num_epochs': 51}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:19,481] Trial 27 finished with value: 0.75 and parameters: {'hidden_dim': 105, 'learning_rate': 0.015880241459794945, 'batch_size': 32, 'num_epochs': 66}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:19,619] Trial 28 finished with value: 0.75 and parameters: {'hidden_dim': 48, 'learning_rate': 0.0029119553226406307, 'batch_size': 128, 'num_epochs': 41}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:19,847] Trial 29 finished with value: 0.75 and parameters: {'hidden_dim': 85, 'learning_rate': 0.006662963440731092, 'batch_size': 128, 'num_epochs': 80}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,242] Trial 30 finished with value: 0.75 and parameters: {'hidden_dim': 180, 'learning_rate': 0.00039483115526053544, 'batch_size': 64, 'num_epochs': 88}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,308] Trial 31 finished with value: 0.75 and parameters: {'hidden_dim': 89, 'learning_rate': 0.09904093766358006, 'batch_size': 32, 'num_epochs': 10}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,399] Trial 32 finished with value: 0.875 and parameters: {'hidden_dim': 101, 'learning_rate': 0.05722932680487482, 'batch_size': 32, 'num_epochs': 25}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,467] Trial 33 finished with value: 0.75 and parameters: {'hidden_dim': 51, 'learning_rate': 0.024040976396320857, 'batch_size': 32, 'num_epochs': 10}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,550] Trial 34 finished with value: 0.875 and parameters: {'hidden_dim': 246, 'learning_rate': 0.05651776471401954, 'batch_size': 32, 'num_epochs': 16}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:20,787] Trial 35 finished with value: 0.75 and parameters: {'hidden_dim': 126, 'learning_rate': 0.012260092207818636, 'batch_size': 128, 'num_epochs': 78}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:21,178] Trial 36 finished with value: 0.75 and parameters: {'hidden_dim': 76, 'learning_rate': 0.05594323645519729, 'batch_size': 32, 'num_epochs': 92}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:21,293] Trial 37 finished with value: 0.75 and parameters: {'hidden_dim': 37, 'learning_rate': 0.04045708735154874, 'batch_size': 128, 'num_epochs': 32}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:21,477] Trial 38 finished with value: 0.75 and parameters: {'hidden_dim': 149, 'learning_rate': 0.002996643871591808, 'batch_size': 64, 'num_epochs': 57}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:21,650] Trial 39 finished with value: 0.75 and parameters: {'hidden_dim': 117, 'learning_rate': 0.0003075787057662928, 'batch_size': 128, 'num_epochs': 52}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:21,965] Trial 40 finished with value: 0.75 and parameters: {'hidden_dim': 32, 'learning_rate': 0.08230625902169582, 'batch_size': 32, 'num_epochs': 65}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:22,316] Trial 41 finished with value: 0.875 and parameters: {'hidden_dim': 132, 'learning_rate': 0.07027145201136081, 'batch_size': 128, 'num_epochs': 99}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:22,594] Trial 42 finished with value: 0.75 and parameters: {'hidden_dim': 141, 'learning_rate': 0.03434818332843203, 'batch_size': 128, 'num_epochs': 94}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:22,816] Trial 43 finished with value: 0.75 and parameters: {'hidden_dim': 227, 'learning_rate': 0.09875096705498768, 'batch_size': 128, 'num_epochs': 77}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:23,241] Trial 44 finished with value: 0.75 and parameters: {'hidden_dim': 94, 'learning_rate': 0.02428408970524703, 'batch_size': 256, 'num_epochs': 100}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:23,470] Trial 45 finished with value: 0.75 and parameters: {'hidden_dim': 110, 'learning_rate': 0.04477674417322211, 'batch_size': 128, 'num_epochs': 74}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:23,737] Trial 46 finished with value: 0.625 and parameters: {'hidden_dim': 126, 'learning_rate': 0.0001613549919874945, 'batch_size': 128, 'num_epochs': 86}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:24,029] Trial 47 finished with value: 0.75 and parameters: {'hidden_dim': 93, 'learning_rate': 0.00484681504002359, 'batch_size': 128, 'num_epochs': 70}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:24,379] Trial 48 finished with value: 0.75 and parameters: {'hidden_dim': 159, 'learning_rate': 0.027143543093740133, 'batch_size': 256, 'num_epochs': 96}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\560273739.py:53: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "[I 2024-07-26 07:54:24,690] Trial 49 finished with value: 0.75 and parameters: {'hidden_dim': 73, 'learning_rate': 0.06734491924432891, 'batch_size': 64, 'num_epochs': 92}. Best is trial 1 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "NODE                          0.625  0.777778                    0.157083   \n",
      "TabNet                          1.0       1.0                     4.49895   \n",
      "KAN                            0.75  0.977778                    0.325044   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.034014   \n",
      "KAN                                            0.000997   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "NODE                                      34.393028   \n",
      "TabNet                                   159.837336   \n",
      "KAN                                       16.847538   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "NODE                        {'num_layers': 2, 'num_trees': 6, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 57, 'n_a': 17, 'n_steps': 9, 'gamma': ...  \n",
      "KAN                         {'hidden_dim': 86, 'learning_rate': 0.09549506...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_dim: 86\n",
      "learning_rate: 0.09549506729887208\n",
      "batch_size: 128\n",
      "num_epochs: 48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(KAN, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden_layer(x))\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for KAN\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 32, 256)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the KAN model\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = KAN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final KAN model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "best_model = KAN(input_dim, \n",
    "                 best_params['hidden_dim'], \n",
    "                 output_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if output_dim == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['KAN'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:54:25,227] A new study created in memory with name: no-name-d918826b-935d-4448-ba2c-793621b5c467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:28,455] Trial 0 finished with value: 0.5 and parameters: {'heads': 1, 'dim': 3, 'depth': 3, 'mlp_dim': 107, 'dropout': 0.27778102782570907, 'learning_rate': 0.0001716030028123978, 'batch_size': 32, 'num_epochs': 32}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:30,116] Trial 1 finished with value: 0.75 and parameters: {'heads': 4, 'dim': 36, 'depth': 2, 'mlp_dim': 71, 'dropout': 0.05274879792496301, 'learning_rate': 0.011689208013149276, 'batch_size': 256, 'num_epochs': 46}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:32,500] Trial 2 finished with value: 0.75 and parameters: {'heads': 1, 'dim': 145, 'depth': 2, 'mlp_dim': 114, 'dropout': 0.049208137321176626, 'learning_rate': 0.00016984020008410823, 'batch_size': 32, 'num_epochs': 58}. Best is trial 1 with value: 0.75.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:33,110] Trial 3 finished with value: 0.875 and parameters: {'heads': 1, 'dim': 114, 'depth': 1, 'mlp_dim': 66, 'dropout': 0.2226918671286099, 'learning_rate': 0.0011817005268262876, 'batch_size': 32, 'num_epochs': 60}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:33,470] Trial 4 finished with value: 0.875 and parameters: {'heads': 3, 'dim': 126, 'depth': 1, 'mlp_dim': 93, 'dropout': 0.4847020985448533, 'learning_rate': 0.07255032927972366, 'batch_size': 128, 'num_epochs': 14}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:54:59,563] Trial 5 finished with value: 0.25 and parameters: {'heads': 8, 'dim': 120, 'depth': 5, 'mlp_dim': 125, 'dropout': 0.013703368894758983, 'learning_rate': 0.03213787798155236, 'batch_size': 32, 'num_epochs': 87}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:43,722] Trial 6 finished with value: 0.875 and parameters: {'heads': 6, 'dim': 174, 'depth': 6, 'mlp_dim': 175, 'dropout': 0.02581934369660538, 'learning_rate': 0.0011248686766310617, 'batch_size': 64, 'num_epochs': 45}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:46,259] Trial 7 finished with value: 0.75 and parameters: {'heads': 7, 'dim': 70, 'depth': 6, 'mlp_dim': 210, 'dropout': 0.32571421328933936, 'learning_rate': 0.004915408674035815, 'batch_size': 32, 'num_epochs': 34}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:48,822] Trial 8 finished with value: 0.875 and parameters: {'heads': 3, 'dim': 27, 'depth': 2, 'mlp_dim': 166, 'dropout': 0.333194704133429, 'learning_rate': 0.002907591517268844, 'batch_size': 32, 'num_epochs': 97}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:49,697] Trial 9 finished with value: 0.75 and parameters: {'heads': 2, 'dim': 40, 'depth': 1, 'mlp_dim': 91, 'dropout': 0.29419748631929776, 'learning_rate': 0.0010206762348102564, 'batch_size': 128, 'num_epochs': 98}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:54,156] Trial 10 finished with value: 0.75 and parameters: {'heads': 5, 'dim': 235, 'depth': 4, 'mlp_dim': 34, 'dropout': 0.15180772530990497, 'learning_rate': 0.0005630908319360572, 'batch_size': 256, 'num_epochs': 72}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:54,394] Trial 11 finished with value: 0.5 and parameters: {'heads': 3, 'dim': 99, 'depth': 1, 'mlp_dim': 54, 'dropout': 0.48547046162523183, 'learning_rate': 0.060846461266649166, 'batch_size': 128, 'num_epochs': 15}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:54,591] Trial 12 finished with value: 0.375 and parameters: {'heads': 2, 'dim': 188, 'depth': 1, 'mlp_dim': 72, 'dropout': 0.4656846321972768, 'learning_rate': 0.015447001728555702, 'batch_size': 128, 'num_epochs': 10}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:57,662] Trial 13 finished with value: 0.375 and parameters: {'heads': 3, 'dim': 87, 'depth': 3, 'mlp_dim': 145, 'dropout': 0.18057336404347107, 'learning_rate': 0.08723191586017016, 'batch_size': 64, 'num_epochs': 69}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:55:58,161] Trial 14 finished with value: 0.75 and parameters: {'heads': 1, 'dim': 146, 'depth': 1, 'mlp_dim': 32, 'dropout': 0.38792665746734667, 'learning_rate': 0.0030189028539872064, 'batch_size': 128, 'num_epochs': 25}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:01,568] Trial 15 finished with value: 0.875 and parameters: {'heads': 4, 'dim': 200, 'depth': 2, 'mlp_dim': 256, 'dropout': 0.2101312918767987, 'learning_rate': 0.0004071101593536652, 'batch_size': 128, 'num_epochs': 62}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:04,429] Trial 16 finished with value: 0.375 and parameters: {'heads': 2, 'dim': 118, 'depth': 4, 'mlp_dim': 85, 'dropout': 0.39985237270323665, 'learning_rate': 0.008847123597768049, 'batch_size': 256, 'num_epochs': 47}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:06,527] Trial 17 finished with value: 0.75 and parameters: {'heads': 5, 'dim': 70, 'depth': 3, 'mlp_dim': 140, 'dropout': 0.11611861788017991, 'learning_rate': 0.0015899346694725862, 'batch_size': 64, 'num_epochs': 74}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:07,740] Trial 18 finished with value: 0.5 and parameters: {'heads': 2, 'dim': 160, 'depth': 1, 'mlp_dim': 59, 'dropout': 0.24760145743427125, 'learning_rate': 0.02679671892738901, 'batch_size': 32, 'num_epochs': 82}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:08,504] Trial 19 finished with value: 0.75 and parameters: {'heads': 3, 'dim': 231, 'depth': 2, 'mlp_dim': 101, 'dropout': 0.4140353895977515, 'learning_rate': 0.00039247848967866267, 'batch_size': 128, 'num_epochs': 24}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:10,680] Trial 20 finished with value: 0.375 and parameters: {'heads': 1, 'dim': 208, 'depth': 5, 'mlp_dim': 53, 'dropout': 0.22480893832120333, 'learning_rate': 0.0069977847436664305, 'batch_size': 32, 'num_epochs': 38}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:16,174] Trial 21 finished with value: 0.875 and parameters: {'heads': 6, 'dim': 156, 'depth': 6, 'mlp_dim': 178, 'dropout': 0.09807185606992763, 'learning_rate': 0.0011306525253026998, 'batch_size': 64, 'num_epochs': 49}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:20,770] Trial 22 finished with value: 0.875 and parameters: {'heads': 6, 'dim': 180, 'depth': 5, 'mlp_dim': 181, 'dropout': 0.0012688838582241221, 'learning_rate': 0.0018480064809840244, 'batch_size': 64, 'num_epochs': 64}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:23,977] Trial 23 finished with value: 0.75 and parameters: {'heads': 6, 'dim': 108, 'depth': 4, 'mlp_dim': 207, 'dropout': 0.1344217197393531, 'learning_rate': 0.0006469804839614771, 'batch_size': 64, 'num_epochs': 54}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:28,119] Trial 24 finished with value: 0.5 and parameters: {'heads': 7, 'dim': 133, 'depth': 6, 'mlp_dim': 134, 'dropout': 0.36890449991738944, 'learning_rate': 0.004486795306868608, 'batch_size': 64, 'num_epochs': 22}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:32,369] Trial 25 finished with value: 0.875 and parameters: {'heads': 4, 'dim': 172, 'depth': 3, 'mlp_dim': 156, 'dropout': 0.07354216412955938, 'learning_rate': 0.002123954802727674, 'batch_size': 64, 'num_epochs': 38}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:36,172] Trial 26 finished with value: 0.625 and parameters: {'heads': 5, 'dim': 255, 'depth': 1, 'mlp_dim': 228, 'dropout': 0.4549063901334831, 'learning_rate': 0.0003091541855251314, 'batch_size': 128, 'num_epochs': 53}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:40,264] Trial 27 finished with value: 0.75 and parameters: {'heads': 7, 'dim': 84, 'depth': 5, 'mlp_dim': 117, 'dropout': 0.1754968517524835, 'learning_rate': 0.0010363093789286092, 'batch_size': 256, 'num_epochs': 42}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:43,687] Trial 28 finished with value: 0.875 and parameters: {'heads': 4, 'dim': 132, 'depth': 2, 'mlp_dim': 194, 'dropout': 0.34749892359760737, 'learning_rate': 0.000116453402656908, 'batch_size': 32, 'num_epochs': 78}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:48,370] Trial 29 finished with value: 0.75 and parameters: {'heads': 8, 'dim': 168, 'depth': 4, 'mlp_dim': 101, 'dropout': 0.30289907351083467, 'learning_rate': 0.0008841784129365582, 'batch_size': 128, 'num_epochs': 28}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:49,012] Trial 30 finished with value: 0.25 and parameters: {'heads': 1, 'dim': 210, 'depth': 3, 'mlp_dim': 73, 'dropout': 0.2784116047056467, 'learning_rate': 0.031162654947432904, 'batch_size': 64, 'num_epochs': 16}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:52,286] Trial 31 finished with value: 0.75 and parameters: {'heads': 3, 'dim': 6, 'depth': 2, 'mlp_dim': 167, 'dropout': 0.4355428977393461, 'learning_rate': 0.0026963637634622624, 'batch_size': 32, 'num_epochs': 100}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:55,976] Trial 32 finished with value: 0.75 and parameters: {'heads': 3, 'dim': 33, 'depth': 1, 'mlp_dim': 156, 'dropout': 0.34751375084700437, 'learning_rate': 0.0036923368651750315, 'batch_size': 32, 'num_epochs': 91}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:57,603] Trial 33 finished with value: 0.75 and parameters: {'heads': 2, 'dim': 10, 'depth': 2, 'mlp_dim': 175, 'dropout': 0.24967251460982484, 'learning_rate': 0.0015336819440580668, 'batch_size': 32, 'num_epochs': 91}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:59,220] Trial 34 finished with value: 0.75 and parameters: {'heads': 4, 'dim': 64, 'depth': 2, 'mlp_dim': 126, 'dropout': 0.48612708287718426, 'learning_rate': 0.013353201086185843, 'batch_size': 32, 'num_epochs': 63}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:56:59,722] Trial 35 finished with value: 0.75 and parameters: {'heads': 5, 'dim': 45, 'depth': 1, 'mlp_dim': 193, 'dropout': 0.037478312777515344, 'learning_rate': 0.00020889212521005285, 'batch_size': 32, 'num_epochs': 32}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:03,332] Trial 36 finished with value: 0.875 and parameters: {'heads': 2, 'dim': 108, 'depth': 1, 'mlp_dim': 157, 'dropout': 0.320960524489308, 'learning_rate': 0.005991221267360071, 'batch_size': 32, 'num_epochs': 86}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [6, 256] and step=6, but the range is not divisible by `step`. It will be replaced by [6, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:04,785] Trial 37 finished with value: 0.875 and parameters: {'heads': 6, 'dim': 150, 'depth': 2, 'mlp_dim': 86, 'dropout': 0.08148905710135451, 'learning_rate': 0.0006339963877506281, 'batch_size': 256, 'num_epochs': 56}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:05,056] Trial 38 finished with value: 0.75 and parameters: {'heads': 3, 'dim': 27, 'depth': 1, 'mlp_dim': 114, 'dropout': 0.2131068191616094, 'learning_rate': 0.0024730278363985824, 'batch_size': 32, 'num_epochs': 17}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:05,830] Trial 39 finished with value: 0.5 and parameters: {'heads': 1, 'dim': 137, 'depth': 1, 'mlp_dim': 225, 'dropout': 0.42962459866408453, 'learning_rate': 0.02157463013481785, 'batch_size': 128, 'num_epochs': 41}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:06,373] Trial 40 finished with value: 0.875 and parameters: {'heads': 4, 'dim': 92, 'depth': 3, 'mlp_dim': 62, 'dropout': 0.03193779725863277, 'learning_rate': 0.009015946779807101, 'batch_size': 32, 'num_epochs': 10}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:08,760] Trial 41 finished with value: 0.75 and parameters: {'heads': 4, 'dim': 196, 'depth': 2, 'mlp_dim': 250, 'dropout': 0.20958069204924115, 'learning_rate': 0.00038137246278968477, 'batch_size': 128, 'num_epochs': 60}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:11,520] Trial 42 finished with value: 0.875 and parameters: {'heads': 4, 'dim': 220, 'depth': 2, 'mlp_dim': 219, 'dropout': 0.17370226611910097, 'learning_rate': 0.0002515429642340128, 'batch_size': 128, 'num_epochs': 66}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:13,287] Trial 43 finished with value: 0.875 and parameters: {'heads': 3, 'dim': 192, 'depth': 2, 'mlp_dim': 253, 'dropout': 0.28455307833612276, 'learning_rate': 0.000488342524177516, 'batch_size': 128, 'num_epochs': 73}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:14,701] Trial 44 finished with value: 0.625 and parameters: {'heads': 5, 'dim': 120, 'depth': 1, 'mlp_dim': 42, 'dropout': 0.1910426838524287, 'learning_rate': 0.0012490699573830001, 'batch_size': 128, 'num_epochs': 52}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [7, 256] and step=7, but the range is not divisible by `step`. It will be replaced by [7, 252].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:17,192] Trial 45 finished with value: 0.375 and parameters: {'heads': 7, 'dim': 49, 'depth': 3, 'mlp_dim': 241, 'dropout': 0.13631934806500548, 'learning_rate': 0.04470345125982266, 'batch_size': 128, 'num_epochs': 68}. Best is trial 3 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:22,089] Trial 46 finished with value: 0.625 and parameters: {'heads': 2, 'dim': 250, 'depth': 6, 'mlp_dim': 75, 'dropout': 0.2611585916179635, 'learning_rate': 0.0008942416801719002, 'batch_size': 256, 'num_epochs': 59}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:23,249] Trial 47 finished with value: 0.75 and parameters: {'heads': 3, 'dim': 183, 'depth': 1, 'mlp_dim': 194, 'dropout': 0.22694201877103382, 'learning_rate': 0.003865427450749163, 'batch_size': 128, 'num_epochs': 79}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [5, 256] and step=5, but the range is not divisible by `step`. It will be replaced by [5, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:27,867] Trial 48 finished with value: 0.875 and parameters: {'heads': 5, 'dim': 200, 'depth': 2, 'mlp_dim': 94, 'dropout': 0.37580524777449326, 'learning_rate': 0.0007474647402075095, 'batch_size': 64, 'num_epochs': 95}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [3, 256] and step=3, but the range is not divisible by `step`. It will be replaced by [3, 255].\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\1518152593.py:65: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[I 2024-07-26 07:57:31,128] Trial 49 finished with value: 0.875 and parameters: {'heads': 3, 'dim': 165, 'depth': 4, 'mlp_dim': 145, 'dropout': 0.15303268568621395, 'learning_rate': 0.0014420638116208649, 'batch_size': 32, 'num_epochs': 45}. Best is trial 3 with value: 0.875.\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "NODE                          0.625  0.777778                    0.157083   \n",
      "TabNet                          1.0       1.0                     4.49895   \n",
      "KAN                            0.75  0.977778                    0.325044   \n",
      "SAINT                         0.625  0.905556                    0.975393   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.034014   \n",
      "KAN                                            0.000997   \n",
      "SAINT                                          0.001995   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "NODE                                      34.393028   \n",
      "TabNet                                   159.837336   \n",
      "KAN                                       16.847538   \n",
      "SAINT                                    186.924322   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "NODE                        {'num_layers': 2, 'num_trees': 6, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 57, 'n_a': 17, 'n_steps': 9, 'gamma': ...  \n",
      "KAN                         {'hidden_dim': 86, 'learning_rate': 0.09549506...  \n",
      "SAINT                       {'heads': 1, 'dim': 114, 'depth': 1, 'mlp_dim'...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "heads: 1\n",
      "dim: 114\n",
      "depth: 1\n",
      "mlp_dim: 66\n",
      "dropout: 0.2226918671286099\n",
      "learning_rate: 0.0011817005268262876\n",
      "batch_size: 32\n",
      "num_epochs: 60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.embeds = nn.Linear(input_dim, dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeds(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for SAINT\n",
    "    heads = trial.suggest_int('heads', 1, 8)\n",
    "    dim = trial.suggest_int('dim', heads, 256, step=heads)  # Ensure dim is divisible by heads\n",
    "    depth = trial.suggest_int('depth', 1, 6)\n",
    "    mlp_dim = trial.suggest_int('mlp_dim', 32, 256)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the SAINT model\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = SAINT(input_dim, num_classes, dim, depth, heads, mlp_dim, dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final SAINT model with the best hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "best_model = SAINT(input_dim, num_classes, \n",
    "                   best_params['dim'], \n",
    "                   best_params['depth'], \n",
    "                   best_params['heads'], \n",
    "                   best_params['mlp_dim'], \n",
    "                   best_params['dropout']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "training_start_time = time.time()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    best_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    inference_start_time = time.time()\n",
    "    outputs = best_model(X_test_tensor)\n",
    "    inference_time = time.time() - inference_start_time\n",
    "\n",
    "    # Convert to numpy for metric calculation\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    proba = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if num_classes == 2:  # Binary classification\n",
    "        auc = roc_auc_score(y_true, proba[:, 1])\n",
    "    else:  # Multi-class classification\n",
    "        auc = roc_auc_score(y_true, proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['SAINT'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 07:57:32,222] A new study created in memory with name: no-name-9d43ab35-eb79-4e26-a516-277b27d33e12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.50374 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.89285 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 1.9433  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 1.58311 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 1.61411 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 1.3291  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 1.40681 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 1.21003 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 1.38053 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 1.34901 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 10 | loss: 1.09436 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 1.05671 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 1.0244  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.77091 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.85441 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.63634 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 16 | loss: 0.65374 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.50675 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 18 | loss: 0.49253 | val_accuracy: 0.5     |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:35,108] Trial 0 finished with value: 0.875 and parameters: {'n_d': 11, 'n_a': 16, 'n_steps': 10, 'gamma': 1.6273843140950668, 'lambda_sparse': 6.757651400226338e-05, 'learning_rate': 0.005768493906323893, 'batch_size': 64, 'num_epochs': 35}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.49697 | val_accuracy: 0.625   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.875\n",
      "epoch 0  | loss: 2.71194 | val_accuracy: 0.625   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.65524 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.34635 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.1353  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.06497 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.04482 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.01546 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.02013 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.00936 | val_accuracy: 0.625   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:35,834] Trial 1 finished with value: 0.625 and parameters: {'n_d': 28, 'n_a': 34, 'n_steps': 3, 'gamma': 1.1064586646341397, 'lambda_sparse': 6.421522258600302e-05, 'learning_rate': 0.04499005936868059, 'batch_size': 64, 'num_epochs': 67}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.00526 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 10 | loss: 0.00312 | val_accuracy: 0.5     |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 11.98331| val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 10.70578| val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 9.05365 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 8.02818 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 6.51115 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 5.31637 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 4.47703 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 3.43643 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 2.40229 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 1.51601 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.92001 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.4099  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.30818 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 13 | loss: 0.18114 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 14 | loss: 0.11717 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.12478 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.10967 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 17 | loss: 0.10603 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.07022 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 19 | loss: 0.05909 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 20 | loss: 0.02562 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 21 | loss: 0.01321 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 22 | loss: 0.00899 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 23 | loss: 0.006   | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 24 | loss: 0.00465 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 25 | loss: 0.00411 | val_accuracy: 0.75    |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:40,094] Trial 2 finished with value: 0.875 and parameters: {'n_d': 45, 'n_a': 37, 'n_steps': 9, 'gamma': 1.1011109035567221, 'lambda_sparse': 5.246541295022321e-05, 'learning_rate': 0.0029550976487433002, 'batch_size': 128, 'num_epochs': 70}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | loss: 0.00351 | val_accuracy: 0.75    |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.24197 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 0.87388 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.71165 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.47937 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.40654 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.34433 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.28329 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.28135 | val_accuracy: 0.5     |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:41,130] Trial 3 finished with value: 0.625 and parameters: {'n_d': 18, 'n_a': 48, 'n_steps': 5, 'gamma': 1.3998557459768195, 'lambda_sparse': 3.095444578877207e-06, 'learning_rate': 0.0019700817028198126, 'batch_size': 256, 'num_epochs': 93}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 0.21534 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.19121 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 10 | loss: 0.18697 | val_accuracy: 0.625   |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.96939 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.6088  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 2.34375 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 1.62997 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.18817 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 1.22511 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.741   | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.68664 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.80786 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.81594 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 1.178   | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.73027 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.56515 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.50578 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.57784 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.65469 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 16 | loss: 0.80753 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 17 | loss: 0.62478 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 18 | loss: 0.55643 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 19 | loss: 0.48663 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 20 | loss: 0.26688 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 21 | loss: 0.41048 | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:44,395] Trial 4 finished with value: 0.75 and parameters: {'n_d': 18, 'n_a': 39, 'n_steps': 9, 'gamma': 1.7446834881392843, 'lambda_sparse': 5.857528179341757e-05, 'learning_rate': 0.014112369438392006, 'batch_size': 32, 'num_epochs': 58}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.61736 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 0.83891 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.43252 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.07615 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 4  | loss: 0.0999  | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 5  | loss: 0.00974 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.00312 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.0022  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.00177 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.00182 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.0013  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.00129 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.00088 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:46,105] Trial 5 finished with value: 0.875 and parameters: {'n_d': 54, 'n_a': 55, 'n_steps': 6, 'gamma': 1.4307181138232081, 'lambda_sparse': 1.6896693073525064e-06, 'learning_rate': 0.01148585948319755, 'batch_size': 128, 'num_epochs': 69}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 0.00144 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.39439 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 3.44014 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 3.55524 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 3  | loss: 2.47877 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 4  | loss: 2.26724 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 5  | loss: 1.78668 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 6  | loss: 1.75436 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 1.60245 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 1.66498 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 9  | loss: 1.25218 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 10 | loss: 1.11111 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 11 | loss: 0.83505 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 12 | loss: 0.58902 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 0.44256 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 14 | loss: 0.65791 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 15 | loss: 0.26783 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 16 | loss: 0.21789 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 17 | loss: 0.14581 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 18 | loss: 0.12206 | val_accuracy: 0.5     |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:50,375] Trial 6 finished with value: 0.625 and parameters: {'n_d': 54, 'n_a': 22, 'n_steps': 9, 'gamma': 1.680954230356611, 'lambda_sparse': 4.5682820550622e-05, 'learning_rate': 0.001700705845658843, 'batch_size': 256, 'num_epochs': 48}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.11799 | val_accuracy: 0.375   |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.78158 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 4.73881 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 2  | loss: 4.05516 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 3  | loss: 3.31367 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 2.98248 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 5  | loss: 2.49209 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 6  | loss: 2.2387  | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 7  | loss: 1.50477 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 8  | loss: 1.15974 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 9  | loss: 0.66804 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 10 | loss: 0.51651 | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 11 | loss: 0.54971 | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 12 | loss: 0.4114  | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 13 | loss: 0.24282 | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 14 | loss: 0.2474  | val_accuracy: 0.375   |  0:00:04s\n",
      "epoch 15 | loss: 0.20594 | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 16 | loss: 0.11004 | val_accuracy: 0.5     |  0:00:04s\n",
      "epoch 17 | loss: 0.08369 | val_accuracy: 0.625   |  0:00:05s\n",
      "epoch 18 | loss: 0.12397 | val_accuracy: 0.5     |  0:00:05s\n",
      "epoch 19 | loss: 0.02421 | val_accuracy: 0.5     |  0:00:05s\n",
      "epoch 20 | loss: 0.12901 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 21 | loss: 0.01775 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 22 | loss: 0.01431 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 23 | loss: 0.00901 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 24 | loss: 0.00793 | val_accuracy: 0.625   |  0:00:06s\n",
      "epoch 25 | loss: 0.00621 | val_accuracy: 0.75    |  0:00:06s\n",
      "epoch 26 | loss: 0.00931 | val_accuracy: 0.75    |  0:00:06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:56,943] Trial 7 finished with value: 0.75 and parameters: {'n_d': 21, 'n_a': 22, 'n_steps': 10, 'gamma': 1.3022263925106232, 'lambda_sparse': 3.29321236608444e-06, 'learning_rate': 0.007255937347827829, 'batch_size': 64, 'num_epochs': 28}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | loss: 0.00444 | val_accuracy: 0.75    |  0:00:06s\n",
      "Stop training because you reached max_epochs = 28 with best_epoch = 20 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.21366 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.23226 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 3.25109 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 3.21364 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 3.22078 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 3.13421 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 3.01928 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 3.0033  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 2.86331 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 2.88308 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 3.11733 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 2.96281 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 2.94956 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 13 | loss: 3.09216 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 14 | loss: 2.93179 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 15 | loss: 2.88976 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 16 | loss: 2.87069 | val_accuracy: 0.375   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:59,104] Trial 8 finished with value: 0.5 and parameters: {'n_d': 12, 'n_a': 26, 'n_steps': 9, 'gamma': 1.9617007271486753, 'lambda_sparse': 9.464612100945171e-05, 'learning_rate': 0.0001014320440647219, 'batch_size': 128, 'num_epochs': 93}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.1309  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 1  | loss: 0.56968 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.33646 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.27344 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.18375 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.13446 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.09861 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.07324 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.05333 | val_accuracy: 0.625   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:57:59,713] Trial 9 finished with value: 0.75 and parameters: {'n_d': 28, 'n_a': 55, 'n_steps': 3, 'gamma': 1.7953902705873932, 'lambda_sparse': 2.501663240204828e-06, 'learning_rate': 0.014415056744622717, 'batch_size': 128, 'num_epochs': 50}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.04085 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 10 | loss: 0.03079 | val_accuracy: 0.5     |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.04645 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 3.23027 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 3.33237 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 3.16034 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 3.19334 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 3.27941 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 3.02643 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 2.96474 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 8  | loss: 2.8174  | val_accuracy: 0.25    |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:00,966] Trial 10 finished with value: 0.25 and parameters: {'n_d': 8, 'n_a': 8, 'n_steps': 7, 'gamma': 1.57375679303858, 'lambda_sparse': 0.0008206558879857576, 'learning_rate': 0.000450110733517511, 'batch_size': 64, 'num_epochs': 13}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 2.79677 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 10 | loss: 2.88721 | val_accuracy: 0.125   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.90071 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.30481 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.43334 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 2.17222 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 2.69349 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 2.23141 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.53146 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 7  | loss: 1.44682 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 1.73977 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 1.34117 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 1.10957 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 1.02034 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 1.22196 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 1.37118 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.91145 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.53126 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.57154 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.85001 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.69915 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 19 | loss: 0.60447 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 20 | loss: 0.64482 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 21 | loss: 0.44034 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 22 | loss: 0.70408 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 23 | loss: 0.23542 | val_accuracy: 0.75    |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:05,100] Trial 11 finished with value: 0.75 and parameters: {'n_d': 42, 'n_a': 10, 'n_steps': 10, 'gamma': 1.0665676414027303, 'lambda_sparse': 0.0002582425611202243, 'learning_rate': 0.0011793252098365966, 'batch_size': 32, 'num_epochs': 30}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | loss: 0.36729 | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.59475 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 2.58269 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 0.63155 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.18075 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.51749 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.26549 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.17406 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 0.07502 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.00786 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.01846 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:06,992] Trial 12 finished with value: 0.75 and parameters: {'n_d': 42, 'n_a': 42, 'n_steps': 8, 'gamma': 1.216253083848298, 'lambda_sparse': 1.4162014127672613e-05, 'learning_rate': 0.08310278884529404, 'batch_size': 128, 'num_epochs': 79}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 0.00288 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.03398 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.80414 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.05715 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 2  | loss: 0.32982 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.11699 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.24796 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.11024 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.05937 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 0.03808 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 8  | loss: 0.03635 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 0.08068 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.06389 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.05295 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:08,579] Trial 13 finished with value: 0.875 and parameters: {'n_d': 60, 'n_a': 32, 'n_steps': 8, 'gamma': 1.5569827723780318, 'lambda_sparse': 1.2220071154050046e-05, 'learning_rate': 0.004776736204548879, 'batch_size': 64, 'num_epochs': 40}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.02295 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 5.82747 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 2  | loss: 5.66633 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 4.7686  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 4.77387 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 4.51862 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 4.46372 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 4.35199 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 4.22326 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 3.92623 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:10,280] Trial 14 finished with value: 0.375 and parameters: {'n_d': 37, 'n_a': 64, 'n_steps': 10, 'gamma': 1.9020136004297377, 'lambda_sparse': 0.00018688965736154793, 'learning_rate': 0.0005193371560617653, 'batch_size': 64, 'num_epochs': 77}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 4.20146 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.26682 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 0.72235 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.29186 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 3  | loss: 0.26244 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 4  | loss: 0.0653  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.05192 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.02126 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.15039 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.17439 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.00322 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.0156  | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:11,788] Trial 15 finished with value: 0.875 and parameters: {'n_d': 49, 'n_a': 16, 'n_steps': 8, 'gamma': 1.2404636428781979, 'lambda_sparse': 1.6127615244733703e-05, 'learning_rate': 0.02601553013896955, 'batch_size': 128, 'num_epochs': 16}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.01099 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.0023  | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.52978 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.36005 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.80907 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.51253 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 1.03521 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.87241 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.51632 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 0.38246 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 0.33696 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.24449 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 10 | loss: 0.28412 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 11 | loss: 0.3223  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.16941 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:13,226] Trial 16 finished with value: 0.625 and parameters: {'n_d': 33, 'n_a': 29, 'n_steps': 6, 'gamma': 1.0004006258868772, 'lambda_sparse': 0.00020343521134985344, 'learning_rate': 0.004132356380412554, 'batch_size': 32, 'num_epochs': 32}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 0.14143 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.5044  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 1  | loss: 3.02679 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 2  | loss: 3.07226 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 3  | loss: 2.85474 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 4  | loss: 2.76687 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 5  | loss: 2.43931 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 6  | loss: 2.34271 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 7  | loss: 2.19651 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 8  | loss: 2.45491 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 9  | loss: 2.41725 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 10 | loss: 2.10112 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 11 | loss: 1.86105 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 12 | loss: 1.38531 | val_accuracy: 0.375   |  0:00:03s\n",
      "epoch 13 | loss: 1.15466 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 14 | loss: 1.05488 | val_accuracy: 0.5     |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:17,424] Trial 17 finished with value: 0.625 and parameters: {'n_d': 45, 'n_a': 45, 'n_steps': 7, 'gamma': 1.6521146307348793, 'lambda_sparse': 2.5414877960165033e-05, 'learning_rate': 0.0005136848124139033, 'batch_size': 256, 'num_epochs': 59}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.91886 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 16 | loss: 0.9311  | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.58929 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 3.42847 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 2.68823 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 1.8308  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 1.21094 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 1.22486 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.78502 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 7  | loss: 0.95486 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 0.8377  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.88186 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.66955 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.46952 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:19,633] Trial 18 finished with value: 0.75 and parameters: {'n_d': 62, 'n_a': 16, 'n_steps': 9, 'gamma': 1.4525578248326165, 'lambda_sparse': 0.0006119522040081667, 'learning_rate': 0.002938791017366825, 'batch_size': 64, 'num_epochs': 40}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.40645 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.11002 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.66126 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.55696 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.11381 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 1.06363 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.95711 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.85549 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 0.72729 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 0.6274  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.56609 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 10 | loss: 0.48174 | val_accuracy: 0.5     |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:20,568] Trial 19 finished with value: 0.5 and parameters: {'n_d': 34, 'n_a': 36, 'n_steps': 5, 'gamma': 1.3737982272837062, 'lambda_sparse': 0.00011735391105975612, 'learning_rate': 0.0010011893349017066, 'batch_size': 128, 'num_epochs': 100}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.4102  | val_accuracy: 0.5     |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.89163 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.51379 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.71545 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 2.48709 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 2.74735 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 2.84035 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 2.61419 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 2.67863 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 2.67694 | val_accuracy: 0.5     |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:22,194] Trial 20 finished with value: 0.5 and parameters: {'n_d': 25, 'n_a': 50, 'n_steps': 10, 'gamma': 1.838487161989383, 'lambda_sparse': 6.452638873963963e-06, 'learning_rate': 0.00018268199084555472, 'batch_size': 64, 'num_epochs': 79}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 2.51386 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 2.58098 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.69357 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 0.58895 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.5165  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.46693 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 4  | loss: 0.0903  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.13577 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.15559 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.0245  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 8  | loss: 0.0113  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 9  | loss: 0.00815 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 10 | loss: 0.00917 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 11 | loss: 0.00709 | val_accuracy: 0.75    |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:23,460] Trial 21 finished with value: 0.875 and parameters: {'n_d': 54, 'n_a': 62, 'n_steps': 5, 'gamma': 1.5158473878786, 'lambda_sparse': 1.2179913354843392e-06, 'learning_rate': 0.01044708536673412, 'batch_size': 128, 'num_epochs': 68}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.06606 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.00963 | val_accuracy: 0.75    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.76743 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 1.44899 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 0.51906 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 0.096   | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.047   | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.31605 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.01277 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.00581 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.00719 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 9  | loss: 0.00484 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 10 | loss: 0.00301 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.00198 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.00128 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.00065 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.0004  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.00026 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 0.00017 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 17 | loss: 0.00017 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 18 | loss: 0.00012 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:25,494] Trial 22 finished with value: 0.875 and parameters: {'n_d': 54, 'n_a': 54, 'n_steps': 6, 'gamma': 1.167066881458854, 'lambda_sparse': 3.0630741145632804e-05, 'learning_rate': 0.02834262951436619, 'batch_size': 128, 'num_epochs': 68}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.00012 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.14135 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 1.03106 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.47019 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.23905 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.3211  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.08674 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.0248  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.01486 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 8  | loss: 0.00982 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.00511 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 10 | loss: 0.00358 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 11 | loss: 0.00271 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 12 | loss: 0.00217 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 13 | loss: 0.00179 | val_accuracy: 0.5     |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:26,688] Trial 23 finished with value: 0.75 and parameters: {'n_d': 48, 'n_a': 58, 'n_steps': 4, 'gamma': 1.3374463167418176, 'lambda_sparse': 7.089277078572495e-06, 'learning_rate': 0.00705166894454283, 'batch_size': 128, 'num_epochs': 73}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.00148 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.92359 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.72063 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.97925 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 2.07942 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.55547 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 0.96591 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.78694 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.47133 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 8  | loss: 0.29051 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.17436 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.10301 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.07285 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.05722 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.05631 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 14 | loss: 0.03078 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 15 | loss: 0.02217 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:28,786] Trial 24 finished with value: 0.75 and parameters: {'n_d': 40, 'n_a': 40, 'n_steps': 7, 'gamma': 1.6218963125384522, 'lambda_sparse': 0.00034218703367135003, 'learning_rate': 0.0028478432510061482, 'batch_size': 128, 'num_epochs': 61}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 0.01714 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 17 | loss: 0.01324 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.9571  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.07416 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 0.82134 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.41433 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.35413 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.11317 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.10193 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.02068 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.01559 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 9  | loss: 0.00363 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 10 | loss: 0.0061  | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 11 | loss: 0.00483 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.00369 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 0.00227 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 14 | loss: 0.04998 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 15 | loss: 0.00157 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.00097 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.00086 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 18 | loss: 0.00123 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 19 | loss: 0.00096 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 20 | loss: 0.0008  | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 21 | loss: 0.00095 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 22 | loss: 0.00087 | val_accuracy: 0.625   |  0:00:03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:32,834] Trial 25 finished with value: 0.75 and parameters: {'n_d': 58, 'n_a': 14, 'n_steps': 6, 'gamma': 1.4670775432186125, 'lambda_sparse': 0.0001074132631290509, 'learning_rate': 0.02176148920751818, 'batch_size': 128, 'num_epochs': 86}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 0.00056 | val_accuracy: 0.625   |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.22073 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 3.14343 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 3.27465 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.53394 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.93024 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 1.14947 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 0.64436 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 0.32417 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 0.27221 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 9  | loss: 0.36158 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.12649 | val_accuracy: 0.375   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:34,244] Trial 26 finished with value: 0.5 and parameters: {'n_d': 50, 'n_a': 23, 'n_steps': 8, 'gamma': 1.7136186148268095, 'lambda_sparse': 3.7214219082361295e-05, 'learning_rate': 0.005738034349407903, 'batch_size': 256, 'num_epochs': 22}. Best is trial 0 with value: 0.875.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.94432 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.95283 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.551   | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.87481 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 4  | loss: 0.24836 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.44117 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 6  | loss: 0.34095 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.62406 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.26209 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.11276 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.4099  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.10169 | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:36,734] Trial 27 finished with value: 0.875 and parameters: {'n_d': 64, 'n_a': 49, 'n_steps': 9, 'gamma': 1.2745179984088855, 'lambda_sparse': 1.3049567108295784e-06, 'learning_rate': 0.00994792749154684, 'batch_size': 32, 'num_epochs': 50}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.15103 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 13 | loss: 0.24506 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.9432  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.16392 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 2  | loss: 0.68187 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.67949 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.17484 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 5  | loss: 0.39107 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 6  | loss: 0.02555 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 0.00856 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 0.00837 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.00037 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.00031 | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:38,815] Trial 28 finished with value: 0.75 and parameters: {'n_d': 46, 'n_a': 30, 'n_steps': 10, 'gamma': 1.1419741837098498, 'lambda_sparse': 7.560708622125292e-06, 'learning_rate': 0.05207179071566849, 'batch_size': 128, 'num_epochs': 41}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.00047 | val_accuracy: 0.75    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.26745 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 2.02769 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 1.30501 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 1.00651 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.73641 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.50735 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.36803 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.27433 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.2276  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.17042 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 10 | loss: 0.13349 | val_accuracy: 0.625   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:39,818] Trial 29 finished with value: 0.625 and parameters: {'n_d': 30, 'n_a': 37, 'n_steps': 4, 'gamma': 1.0015138947725764, 'lambda_sparse': 5.652537897285923e-05, 'learning_rate': 0.0023308297049763455, 'batch_size': 64, 'num_epochs': 63}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.10288 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 12 | loss: 0.08751 | val_accuracy: 0.625   |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.81877 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 1  | loss: 1.03134 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.73949 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.16225 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.03415 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.0678  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.17773 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.00233 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.00443 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.00273 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 10 | loss: 0.00143 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:41,067] Trial 30 finished with value: 0.75 and parameters: {'n_d': 38, 'n_a': 34, 'n_steps': 7, 'gamma': 1.0882058816706188, 'lambda_sparse': 2.6965988632747936e-05, 'learning_rate': 0.04306125030709091, 'batch_size': 64, 'num_epochs': 85}. Best is trial 0 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.94984 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 2.64501 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 1.8386  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.26554 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.85521 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.54311 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.5092  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.33521 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.40992 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 0.23863 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.17465 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.31905 | val_accuracy: 0.375   |  0:00:02s\n",
      "epoch 12 | loss: 0.43888 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 13 | loss: 0.21215 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 14 | loss: 0.23946 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 15 | loss: 0.10086 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 16 | loss: 0.05508 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 17 | loss: 0.03856 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 18 | loss: 0.03406 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 19 | loss: 0.01735 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 20 | loss: 0.01047 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 21 | loss: 0.00837 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 22 | loss: 0.00325 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 23 | loss: 0.00434 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 24 | loss: 0.00302 | val_accuracy: 0.875   |  0:00:04s\n",
      "epoch 25 | loss: 0.00187 | val_accuracy: 0.875   |  0:00:04s\n",
      "epoch 26 | loss: 0.00192 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 27 | loss: 0.00175 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 28 | loss: 0.00179 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 29 | loss: 0.0044  | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 30 | loss: 0.00243 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 31 | loss: 0.00199 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 32 | loss: 0.00238 | val_accuracy: 1.0     |  0:00:04s\n",
      "epoch 33 | loss: 0.0019  | val_accuracy: 1.0     |  0:00:05s\n",
      "epoch 34 | loss: 0.0184  | val_accuracy: 0.875   |  0:00:05s\n",
      "epoch 35 | loss: 0.00105 | val_accuracy: 0.875   |  0:00:05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:46,781] Trial 31 finished with value: 1.0 and parameters: {'n_d': 59, 'n_a': 32, 'n_steps': 8, 'gamma': 1.5408740103567724, 'lambda_sparse': 1.5950842365142268e-05, 'learning_rate': 0.0043312781000985475, 'batch_size': 64, 'num_epochs': 43}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 | loss: 0.00085 | val_accuracy: 0.875   |  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_accuracy = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.85321 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 6.16179 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 5.08308 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 4.797   | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 4.23418 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 5  | loss: 4.0357  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 3.24042 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 7  | loss: 2.90419 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 8  | loss: 2.66295 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 2.28824 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 1.97374 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:48,174] Trial 32 finished with value: 0.5 and parameters: {'n_d': 56, 'n_a': 44, 'n_steps': 8, 'gamma': 1.5810275409147458, 'lambda_sparse': 9.074146598670773e-05, 'learning_rate': 0.001166238518847287, 'batch_size': 64, 'num_epochs': 53}. Best is trial 31 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.93326 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 3.3898  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.49898 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 3  | loss: 1.27167 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.45984 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.65128 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.63931 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 7  | loss: 0.59069 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.40915 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.3     | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.29268 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.23673 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.22557 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.18066 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.08991 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.1105  | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:50,371] Trial 33 finished with value: 0.75 and parameters: {'n_d': 51, 'n_a': 18, 'n_steps': 9, 'gamma': 1.4163570749646526, 'lambda_sparse': 4.755465148737065e-06, 'learning_rate': 0.0040269887810437935, 'batch_size': 64, 'num_epochs': 38}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 0.06779 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.51837 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.50334 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.56697 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.87172 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.48802 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.05809 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 6  | loss: 0.21242 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.19586 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.37268 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.45209 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.60953 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.11319 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.05339 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 13 | loss: 0.0954  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 14 | loss: 0.01193 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:52,669] Trial 34 finished with value: 0.875 and parameters: {'n_d': 60, 'n_a': 27, 'n_steps': 9, 'gamma': 1.7558604019651474, 'lambda_sparse': 2.1712323054301282e-06, 'learning_rate': 0.014333622897234577, 'batch_size': 256, 'num_epochs': 71}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.1367  | val_accuracy: 0.375   |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.94892 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.37918 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 0.54213 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.5144  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.38529 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.38231 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.09755 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 0.04047 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 0.02356 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.02434 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.01243 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.01171 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 12 | loss: 0.00467 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 13 | loss: 0.00227 | val_accuracy: 0.625   |  0:00:03s\n",
      "epoch 14 | loss: 0.21753 | val_accuracy: 0.5     |  0:00:03s\n",
      "epoch 15 | loss: 0.00125 | val_accuracy: 0.5     |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:56,654] Trial 35 finished with value: 0.625 and parameters: {'n_d': 52, 'n_a': 53, 'n_steps': 8, 'gamma': 1.6332732255004623, 'lambda_sparse': 6.50944703000834e-05, 'learning_rate': 0.009630078846706077, 'batch_size': 64, 'num_epochs': 46}. Best is trial 31 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.22885 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 2.05712 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.40016 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 1.8992  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 2.1894  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 1.91919 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 2.11978 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 7  | loss: 2.36193 | val_accuracy: 0.125   |  0:00:01s\n",
      "epoch 8  | loss: 1.58666 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 9  | loss: 1.58973 | val_accuracy: 0.25    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:58:58,468] Trial 36 finished with value: 0.375 and parameters: {'n_d': 14, 'n_a': 25, 'n_steps': 10, 'gamma': 1.5067153047541284, 'lambda_sparse': 1.711263735568933e-05, 'learning_rate': 0.0019602606665652476, 'batch_size': 32, 'num_epochs': 56}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 1.7698  | val_accuracy: 0.25    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 7.03398 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 5.85626 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 5.10245 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 3.99526 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 2.97964 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 2.03491 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 1.6766  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.99748 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.77816 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.36757 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.22624 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.19019 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.17109 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.1836  | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:00,521] Trial 37 finished with value: 0.625 and parameters: {'n_d': 23, 'n_a': 21, 'n_steps': 9, 'gamma': 1.3408489234184484, 'lambda_sparse': 5.236883120029269e-05, 'learning_rate': 0.005989892001361654, 'batch_size': 128, 'num_epochs': 21}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.11569 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 15 | loss: 0.10027 | val_accuracy: 0.5     |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.66953 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.85675 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 2.02896 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.67299 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 1.35288 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 5  | loss: 1.66042 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 6  | loss: 1.3654  | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 7  | loss: 0.98912 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 8  | loss: 0.76169 | val_accuracy: 0.375   |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:01,553] Trial 38 finished with value: 0.5 and parameters: {'n_d': 45, 'n_a': 46, 'n_steps': 6, 'gamma': 1.687392057422002, 'lambda_sparse': 1.7359457091744607e-06, 'learning_rate': 0.0015385413801399222, 'batch_size': 64, 'num_epochs': 34}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.54938 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 10 | loss: 0.37701 | val_accuracy: 0.375   |  0:00:00s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.25086 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 1.12444 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.50771 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 0.58778 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 0.19285 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.14788 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.02641 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 7  | loss: 0.01495 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.02017 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 9  | loss: 0.01039 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 10 | loss: 0.00721 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 11 | loss: 0.05021 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.01802 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.00295 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.02228 | val_accuracy: 0.75    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:03,713] Trial 39 finished with value: 0.875 and parameters: {'n_d': 58, 'n_a': 59, 'n_steps': 7, 'gamma': 1.41364151793716, 'lambda_sparse': 4.156369402507204e-06, 'learning_rate': 0.003501274296425856, 'batch_size': 256, 'num_epochs': 44}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.00139 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 0.00194 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.83064 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.00008 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 0.87434 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.72055 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.82431 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 5  | loss: 0.69291 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.40507 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 7  | loss: 0.46788 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 8  | loss: 0.29796 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 9  | loss: 0.3326  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 10 | loss: 0.32226 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 11 | loss: 0.20975 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.12876 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.08629 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.13104 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.0555  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 16 | loss: 0.04627 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 17 | loss: 0.04781 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 18 | loss: 0.04157 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 19 | loss: 0.04325 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 20 | loss: 0.0251  | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 21 | loss: 0.05253 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 22 | loss: 0.05677 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 23 | loss: 0.08772 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 24 | loss: 0.06085 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 25 | loss: 0.06659 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 26 | loss: 0.01734 | val_accuracy: 0.625   |  0:00:02s\n",
      "Stop training because you reached max_epochs = 27 with best_epoch = 17 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:06,175] Trial 40 finished with value: 0.875 and parameters: {'n_d': 17, 'n_a': 11, 'n_steps': 10, 'gamma': 1.834405269981342, 'lambda_sparse': 0.0001492086724261764, 'learning_rate': 0.01706907092963576, 'batch_size': 128, 'num_epochs': 27}. Best is trial 31 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.24379 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 1.12827 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.47355 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 3  | loss: 0.22193 | val_accuracy: 0.875   |  0:00:00s\n",
      "epoch 4  | loss: 0.48924 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.1743  | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 6  | loss: 0.12286 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 7  | loss: 0.14829 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.0678  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.11441 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.03505 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.0254  | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 12 | loss: 0.01596 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.01427 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:08,187] Trial 41 finished with value: 0.875 and parameters: {'n_d': 61, 'n_a': 32, 'n_steps': 8, 'gamma': 1.5539457592606334, 'lambda_sparse': 1.2952289550757822e-05, 'learning_rate': 0.0046779735880009615, 'batch_size': 64, 'num_epochs': 36}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.75746 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 1  | loss: 1.59292 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.74516 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 3  | loss: 0.83627 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 4  | loss: 0.56993 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 5  | loss: 0.30549 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 6  | loss: 0.23912 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 7  | loss: 0.31375 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.34751 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.26847 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 10 | loss: 0.27595 | val_accuracy: 0.625   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:11,251] Trial 42 finished with value: 0.75 and parameters: {'n_d': 58, 'n_a': 39, 'n_steps': 9, 'gamma': 1.5557848004955233, 'lambda_sparse': 1.0541816921060027e-05, 'learning_rate': 0.007457308462786435, 'batch_size': 64, 'num_epochs': 44}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | loss: 0.07615 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 12 | loss: 0.02558 | val_accuracy: 0.5     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.204   | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 1  | loss: 4.66643 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 2  | loss: 5.06794 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 4.86533 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 4.49774 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 3.74022 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 3.24596 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 3.44922 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 8  | loss: 2.86126 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 2.61963 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 2.62612 | val_accuracy: 0.375   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:12,817] Trial 43 finished with value: 0.375 and parameters: {'n_d': 64, 'n_a': 33, 'n_steps': 8, 'gamma': 1.6093865379859045, 'lambda_sparse': 1.943543568770303e-05, 'learning_rate': 0.0007601815543087718, 'batch_size': 64, 'num_epochs': 64}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.7827  | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 1.82302 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 1.08859 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 0.66852 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.49719 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 5  | loss: 0.21772 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 0.09279 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.05716 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.05219 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.05967 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.07166 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 11 | loss: 0.01773 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.01914 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 13 | loss: 0.01878 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.0275  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 15 | loss: 0.01222 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 16 | loss: 0.00684 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 17 | loss: 0.00111 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 18 | loss: 0.00079 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 19 | loss: 0.00082 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 20 | loss: 0.00056 | val_accuracy: 0.875   |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:15,641] Trial 44 finished with value: 0.875 and parameters: {'n_d': 55, 'n_a': 30, 'n_steps': 7, 'gamma': 1.4704565203530249, 'lambda_sparse': 7.145267580087573e-05, 'learning_rate': 0.004771581777653516, 'batch_size': 64, 'num_epochs': 24}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 0.00046 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 22 | loss: 0.00044 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.13245 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 3.10792 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 2.29688 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 2.47072 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 4  | loss: 1.97803 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 1.32653 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 6  | loss: 1.12633 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.78939 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 8  | loss: 0.73261 | val_accuracy: 0.5     |  0:00:02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:18,150] Trial 45 finished with value: 0.5 and parameters: {'n_d': 42, 'n_a': 20, 'n_steps': 9, 'gamma': 1.5285527035662576, 'lambda_sparse': 3.9265925217367914e-05, 'learning_rate': 0.002424537988238844, 'batch_size': 64, 'num_epochs': 53}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.45318 | val_accuracy: 0.5     |  0:00:02s\n",
      "epoch 10 | loss: 0.40487 | val_accuracy: 0.5     |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.82123 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.92292 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 1.31763 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 3  | loss: 0.7775  | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 4  | loss: 0.67585 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.44879 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.25245 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 7  | loss: 0.1232  | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 8  | loss: 0.17511 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.06495 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.22535 | val_accuracy: 0.875   |  0:00:01s\n",
      "epoch 11 | loss: 0.04789 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 12 | loss: 0.05619 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 13 | loss: 0.01724 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 14 | loss: 0.04076 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 15 | loss: 0.02022 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 16 | loss: 0.01058 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 17 | loss: 0.0112  | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 18 | loss: 0.00443 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 19 | loss: 0.0038  | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 20 | loss: 0.11252 | val_accuracy: 0.75    |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_accuracy = 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:20,917] Trial 46 finished with value: 0.875 and parameters: {'n_d': 52, 'n_a': 25, 'n_steps': 8, 'gamma': 1.6762600629746514, 'lambda_sparse': 9.797660948053517e-06, 'learning_rate': 0.00929154883522301, 'batch_size': 64, 'num_epochs': 48}. Best is trial 31 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.29169 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 4.03317 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 4.12065 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 3  | loss: 3.95388 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 3.66074 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 5  | loss: 3.28639 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 6  | loss: 3.46495 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 7  | loss: 2.85954 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 8  | loss: 3.20364 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 9  | loss: 3.21005 | val_accuracy: 0.25    |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:22,829] Trial 47 finished with value: 0.25 and parameters: {'n_d': 10, 'n_a': 41, 'n_steps': 10, 'gamma': 1.7407524029358228, 'lambda_sparse': 2.4215311554306948e-05, 'learning_rate': 0.0014882976094222834, 'batch_size': 32, 'num_epochs': 74}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 3.56346 | val_accuracy: 0.25    |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_accuracy = 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.05306 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 1  | loss: 2.80653 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 2  | loss: 2.21792 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 3  | loss: 1.49314 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 4  | loss: 0.76207 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 5  | loss: 0.4857  | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 6  | loss: 0.27492 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 7  | loss: 0.19728 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 8  | loss: 0.11624 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 9  | loss: 0.10082 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 10 | loss: 0.24119 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 11 | loss: 0.06915 | val_accuracy: 0.25    |  0:00:01s\n",
      "epoch 12 | loss: 0.0498  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 13 | loss: 0.0356  | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 14 | loss: 0.02395 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 15 | loss: 0.01613 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 16 | loss: 0.01127 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 17 | loss: 0.00862 | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_accuracy = 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:25,003] Trial 48 finished with value: 0.625 and parameters: {'n_d': 47, 'n_a': 37, 'n_steps': 6, 'gamma': 1.2304268068961273, 'lambda_sparse': 3.360291921322935e-06, 'learning_rate': 0.0031980524525252046, 'batch_size': 128, 'num_epochs': 85}. Best is trial 31 with value: 1.0.\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_8880\\2451662473.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.31063 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.41809 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 2  | loss: 0.58963 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 0.33952 | val_accuracy: 0.625   |  0:00:00s\n",
      "epoch 4  | loss: 0.26917 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 5  | loss: 0.53059 | val_accuracy: 0.75    |  0:00:00s\n",
      "epoch 6  | loss: 0.16661 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 7  | loss: 0.15403 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.24342 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 9  | loss: 0.27824 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.24446 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.02642 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 12 | loss: 0.022   | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.00969 | val_accuracy: 0.625   |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-07-26 07:59:27,141] Trial 49 finished with value: 0.75 and parameters: {'n_d': 60, 'n_a': 29, 'n_steps': 9, 'gamma': 1.915282786924466, 'lambda_sparse': 0.0003325181149945809, 'learning_rate': 0.012373116095869423, 'batch_size': 64, 'num_epochs': 31}. Best is trial 31 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.1169  | val_accuracy: 0.625   |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.94984 | val_accuracy: 0.125   |  0:00:00s\n",
      "epoch 1  | loss: 2.64501 | val_accuracy: 0.25    |  0:00:00s\n",
      "epoch 2  | loss: 1.8386  | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 3  | loss: 1.26554 | val_accuracy: 0.375   |  0:00:00s\n",
      "epoch 4  | loss: 0.85521 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 5  | loss: 0.54311 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 6  | loss: 0.5092  | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 7  | loss: 0.33521 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 8  | loss: 0.40992 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 9  | loss: 0.23863 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 10 | loss: 0.17465 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 11 | loss: 0.31905 | val_accuracy: 0.375   |  0:00:01s\n",
      "epoch 12 | loss: 0.43888 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 13 | loss: 0.21215 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 14 | loss: 0.23946 | val_accuracy: 0.625   |  0:00:01s\n",
      "epoch 15 | loss: 0.10086 | val_accuracy: 0.75    |  0:00:01s\n",
      "epoch 16 | loss: 0.05508 | val_accuracy: 0.625   |  0:00:02s\n",
      "epoch 17 | loss: 0.03856 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 18 | loss: 0.03406 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 19 | loss: 0.01735 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 20 | loss: 0.01047 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 21 | loss: 0.00837 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 22 | loss: 0.00325 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 23 | loss: 0.00434 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 24 | loss: 0.00302 | val_accuracy: 0.875   |  0:00:02s\n",
      "epoch 25 | loss: 0.00187 | val_accuracy: 0.875   |  0:00:03s\n",
      "epoch 26 | loss: 0.00192 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 27 | loss: 0.00175 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 28 | loss: 0.00179 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 29 | loss: 0.0044  | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 30 | loss: 0.00243 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 31 | loss: 0.00199 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 32 | loss: 0.00238 | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 33 | loss: 0.0019  | val_accuracy: 1.0     |  0:00:03s\n",
      "epoch 34 | loss: 0.0184  | val_accuracy: 0.875   |  0:00:04s\n",
      "epoch 35 | loss: 0.00105 | val_accuracy: 0.875   |  0:00:04s\n",
      "epoch 36 | loss: 0.00085 | val_accuracy: 0.875   |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_accuracy = 1.0\n",
      "                           Accuracy AUC Score Training Time (Best Params)  \\\n",
      "Logistic Regression            0.75  0.927778                    0.002991   \n",
      "KNN                            0.75       1.0                    0.000996   \n",
      "Decision Tree                 0.875  0.931746                       0.001   \n",
      "Random Forest                  0.75  0.933333                    0.118717   \n",
      "Gradient Boosting             0.875  0.920635                    0.247005   \n",
      "XGBoost                        0.75  0.805556                    0.082778   \n",
      "LightGBM                      0.125       0.5                    0.010972   \n",
      "CatBoost                       0.75  0.977778                    0.119442   \n",
      "MLP                            0.75       1.0                    0.513627   \n",
      "DNN                            0.75  0.922222                     0.23946   \n",
      "DCN                           0.875  0.927778                    0.297203   \n",
      "Wide_and_Deep                 0.875  0.830556                    0.449732   \n",
      "XGBoost + NN                  0.875  0.888889                    0.364206   \n",
      "LightGBM + NN                 0.375       0.5                    0.253997   \n",
      "CatBoost + NN                  0.75  0.755556                    0.277767   \n",
      "AutoInt                       0.625  0.927778                    6.090192   \n",
      "FT-Transformer                0.625  0.683333                    0.246869   \n",
      "Neural Architecture Search    0.625  0.822222                    0.495034   \n",
      "NODE                          0.625  0.777778                    0.157083   \n",
      "TabNet                          1.0       1.0                     4.49895   \n",
      "KAN                            0.75  0.977778                    0.325044   \n",
      "SAINT                         0.625  0.905556                    0.975393   \n",
      "VIME                            1.0       1.0                    4.441126   \n",
      "\n",
      "                           Inference Time (Best Params)  \\\n",
      "Logistic Regression                            0.002992   \n",
      "KNN                                            0.002262   \n",
      "Decision Tree                                       0.0   \n",
      "Random Forest                                  0.008943   \n",
      "Gradient Boosting                              0.003386   \n",
      "XGBoost                                        0.000998   \n",
      "LightGBM                                       0.000997   \n",
      "CatBoost                                       0.001001   \n",
      "MLP                                                 0.0   \n",
      "DNN                                                 0.0   \n",
      "DCN                                            0.000997   \n",
      "Wide_and_Deep                                       0.0   \n",
      "XGBoost + NN                                   0.000997   \n",
      "LightGBM + NN                                  0.000989   \n",
      "CatBoost + NN                                  0.000998   \n",
      "AutoInt                                        0.067931   \n",
      "FT-Transformer                                      0.0   \n",
      "Neural Architecture Search                          0.0   \n",
      "NODE                                           0.000998   \n",
      "TabNet                                         0.034014   \n",
      "KAN                                            0.000997   \n",
      "SAINT                                          0.001995   \n",
      "VIME                                           0.027925   \n",
      "\n",
      "                           Computation Time (Total)  \\\n",
      "Logistic Regression                        5.321193   \n",
      "KNN                                        0.203027   \n",
      "Decision Tree                              0.095976   \n",
      "Random Forest                              5.180227   \n",
      "Gradient Boosting                          7.384311   \n",
      "XGBoost                                    1.069617   \n",
      "LightGBM                                   4.737833   \n",
      "CatBoost                                   6.248461   \n",
      "MLP                                       30.570727   \n",
      "DNN                                       22.263058   \n",
      "DCN                                        24.14075   \n",
      "Wide_and_Deep                             22.987482   \n",
      "XGBoost + NN                              41.681399   \n",
      "LightGBM + NN                             19.848618   \n",
      "CatBoost + NN                              48.66849   \n",
      "AutoInt                                   33.747949   \n",
      "FT-Transformer                            44.479363   \n",
      "Neural Architecture Search                17.547768   \n",
      "NODE                                      34.393028   \n",
      "TabNet                                   159.837336   \n",
      "KAN                                       16.847538   \n",
      "SAINT                                    186.924322   \n",
      "VIME                                     119.444413   \n",
      "\n",
      "                                                              Best Parameters  \n",
      "Logistic Regression                           {'C': 1, 'solver': 'liblinear'}  \n",
      "KNN                                 {'n_neighbors': 7, 'weights': 'distance'}  \n",
      "Decision Tree                     {'max_depth': None, 'min_samples_split': 5}  \n",
      "Random Forest               {'max_depth': 10, 'min_samples_split': 2, 'n_e...  \n",
      "Gradient Boosting           {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "XGBoost                     {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  \n",
      "LightGBM                    {'learning_rate': 0.01, 'n_estimators': 100, '...  \n",
      "CatBoost                    {'depth': 4, 'iterations': 200, 'learning_rate...  \n",
      "MLP                         {'activation': 'relu', 'alpha': 0.0001, 'hidde...  \n",
      "DNN                         {'hidden_dim_0': 41, 'hidden_dim_1': 93, 'hidd...  \n",
      "DCN                         {'cross_layers': 4, 'hidden_layer_0': 184, 'hi...  \n",
      "Wide_and_Deep               {'hidden_layer_0': 205, 'hidden_layer_1': 188,...  \n",
      "XGBoost + NN                {'n_estimators': 204, 'max_depth': 7, 'xgb_lea...  \n",
      "LightGBM + NN               {'n_estimators': 135, 'max_depth': 6, 'lgb_lea...  \n",
      "CatBoost + NN               {'iterations': 85, 'depth': 8, 'catboost_learn...  \n",
      "AutoInt                     {'num_heads': 2, 'embedding_dim': 12, 'num_lay...  \n",
      "FT-Transformer              {'num_heads': 7, 'embedding_dim': 42, 'num_lay...  \n",
      "Neural Architecture Search  {'num_layers': 4, 'hidden_layer_0': 59, 'hidde...  \n",
      "NODE                        {'num_layers': 2, 'num_trees': 6, 'tree_dim': ...  \n",
      "TabNet                      {'n_d': 57, 'n_a': 17, 'n_steps': 9, 'gamma': ...  \n",
      "KAN                         {'hidden_dim': 86, 'learning_rate': 0.09549506...  \n",
      "SAINT                       {'heads': 1, 'dim': 114, 'depth': 1, 'mlp_dim'...  \n",
      "VIME                        {'n_d': 59, 'n_a': 32, 'n_steps': 8, 'gamma': ...  \n",
      "\n",
      "Best Hyperparameters:\n",
      "n_d: 59\n",
      "n_a: 32\n",
      "n_steps: 8\n",
      "gamma: 1.5408740103567724\n",
      "lambda_sparse: 1.5950842365142268e-05\n",
      "learning_rate: 0.0043312781000985475\n",
      "batch_size: 64\n",
      "num_epochs: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start timing the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune for VIME-like model (using TabNet as proxy)\n",
    "    n_d = trial.suggest_int('n_d', 8, 64)\n",
    "    n_a = trial.suggest_int('n_a', 8, 64)\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "    # Create the TabNet model\n",
    "    model = TabNetClassifier(\n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=learning_rate),\n",
    "        device_name=device\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        X_train=X_train_scaled, y_train=y_train.values,\n",
    "        eval_set=[(X_test_scaled, y_test.values)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=10,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=batch_size // 2,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Perform hyperparameter tuning with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final TabNet model with the best hyperparameters\n",
    "best_model = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "    device_name=device\n",
    ")\n",
    "\n",
    "training_start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train=X_train_scaled, y_train=y_train.values,\n",
    "    eval_set=[(X_test_scaled, y_test.values)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=best_params['num_epochs'],\n",
    "    patience=10,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    virtual_batch_size=best_params['batch_size'] // 2,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "# Evaluation\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "inference_start_time = time.time()\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
    "inference_time = time.time() - inference_start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "if len(np.unique(y)) == 2:  # Binary classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "else:  # Multiclass classification\n",
    "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# Calculate total computation time\n",
    "computation_time = time.time() - start_time\n",
    "\n",
    "# Store results in the existing result DataFrame\n",
    "result.loc['VIME'] = [accuracy, auc, training_time, inference_time, computation_time, best_params]\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
