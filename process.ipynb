{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "df =  pd.read_csv('California_Houses.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df - df.mean()) / df.std()\n",
    "df = df[(np.abs(df) < 4).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Y with lambda: 0.8607471157190429\n",
      "Transformed Median_Income with lambda: 0.960220475167138\n",
      "Transformed Median_Age with lambda: 0.9799683619027184\n",
      "Transformed Tot_Rooms with lambda: 0.9981594230014806\n",
      "Transformed Tot_Bedrooms with lambda: 0.9954765433151639\n",
      "Transformed Population with lambda: 0.98965430677062\n",
      "Transformed Households with lambda: 1.0003037457599773\n",
      "Transformed Latitude with lambda: 0.7268953160836403\n",
      "Transformed Longitude with lambda: 1.0857071618165173\n",
      "Transformed Distance_to_coast with lambda: 0.36338880601400225\n",
      "Transformed Distance_to_LA with lambda: 0.29445961769220624\n",
      "Transformed Distance_to_SanDiego with lambda: 0.6823721769637111\n",
      "Transformed Distance_to_SanJose with lambda: 0.9824361962141578\n",
      "Transformed Distance_to_SanFrancisco with lambda: 1.0010415989248287\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Adjust and transform\n",
    "for column in df.columns:\n",
    "    # Ensure all data is positive by shifting the column if necessary\n",
    "    min_value = df[column].min()\n",
    "    shift = -min_value + 1 if min_value <= 0 else 0\n",
    "    try:\n",
    "        if shift > 0:\n",
    "            df[column], fitted_lambda = boxcox(df[column] + shift)\n",
    "        else:\n",
    "            df[column], fitted_lambda = boxcox(df[column])\n",
    "        print(f\"Transformed {column} with lambda: {fitted_lambda}\")\n",
    "    except ValueError:\n",
    "        print(f\"Column {column} cannot be transformed. It may contain zero or negative values after adjustment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3113\n",
      "[LightGBM] [Info] Number of data points in the train set: 16124, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.761867\n",
      "Linear Regression: RMSE = 0.136, Training Time = 0.008s, Inference Time = 0.000s\n",
      "KNN: RMSE = 0.121, Training Time = 0.073s, Inference Time = 0.243s\n",
      "Decision Tree: RMSE = 0.137, Training Time = 0.256s, Inference Time = 0.003s\n",
      "Random Forest: RMSE = 0.098, Training Time = 28.587s, Inference Time = 0.131s\n",
      "XGBoost: RMSE = 0.097, Training Time = 0.362s, Inference Time = 0.006s\n",
      "LightGBM: RMSE = 0.098, Training Time = 0.211s, Inference Time = 0.015s\n",
      "CatBoost: RMSE = 0.093, Training Time = 5.457s, Inference Time = 0.004s\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame and 'Y' is the target column\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'LightGBM': LGBMRegressor(),\n",
    "    'CatBoost': CatBoostRegressor(verbose=0)  # verbose=0 to keep output clean\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    results[name] = {'RMSE': rmse, 'Training Time': training_time, 'Inference Time': inference_time}\n",
    "\n",
    "# Display results\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: RMSE = {metrics['RMSE']:.3f}, Training Time = {metrics['Training Time']:.3f}s, Inference Time = {metrics['Inference Time']:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'Y' is the target column\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.2557 - mean_squared_error: 1.2557 - val_loss: 0.2185 - val_mean_squared_error: 0.2185\n",
      "Epoch 2/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1980 - mean_squared_error: 0.1980 - val_loss: 0.1762 - val_mean_squared_error: 0.1762\n",
      "Epoch 3/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1681 - mean_squared_error: 0.1681 - val_loss: 0.1664 - val_mean_squared_error: 0.1664\n",
      "Epoch 4/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1701 - mean_squared_error: 0.1701 - val_loss: 0.1579 - val_mean_squared_error: 0.1579\n",
      "Epoch 5/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.1608 - val_mean_squared_error: 0.1608\n",
      "Epoch 6/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1512 - mean_squared_error: 0.1512 - val_loss: 0.1509 - val_mean_squared_error: 0.1509\n",
      "Epoch 7/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1514 - mean_squared_error: 0.1514 - val_loss: 0.1888 - val_mean_squared_error: 0.1888\n",
      "Epoch 8/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1504 - mean_squared_error: 0.1504 - val_loss: 0.1526 - val_mean_squared_error: 0.1526\n",
      "Epoch 9/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1497 - mean_squared_error: 0.1497 - val_loss: 0.1422 - val_mean_squared_error: 0.1422\n",
      "Epoch 10/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1435 - mean_squared_error: 0.1435 - val_loss: 0.1412 - val_mean_squared_error: 0.1412\n",
      "Epoch 11/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1328 - mean_squared_error: 0.1328 - val_loss: 0.1431 - val_mean_squared_error: 0.1431\n",
      "Epoch 12/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1358 - mean_squared_error: 0.1358 - val_loss: 0.1547 - val_mean_squared_error: 0.1547\n",
      "Epoch 13/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1337 - mean_squared_error: 0.1337 - val_loss: 0.1385 - val_mean_squared_error: 0.1385\n",
      "Epoch 14/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1281 - mean_squared_error: 0.1281 - val_loss: 0.1324 - val_mean_squared_error: 0.1324\n",
      "Epoch 15/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1264 - mean_squared_error: 0.1264 - val_loss: 0.1356 - val_mean_squared_error: 0.1356\n",
      "Epoch 16/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1292 - mean_squared_error: 0.1292 - val_loss: 0.1342 - val_mean_squared_error: 0.1342\n",
      "Epoch 17/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1284 - mean_squared_error: 0.1284 - val_loss: 0.1285 - val_mean_squared_error: 0.1285\n",
      "Epoch 18/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1219 - mean_squared_error: 0.1219 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 19/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1305 - mean_squared_error: 0.1305 - val_loss: 0.1286 - val_mean_squared_error: 0.1286\n",
      "Epoch 20/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1265 - mean_squared_error: 0.1265 - val_loss: 0.1463 - val_mean_squared_error: 0.1463\n",
      "Epoch 21/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1197 - mean_squared_error: 0.1197 - val_loss: 0.1447 - val_mean_squared_error: 0.1447\n",
      "Epoch 22/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1231 - mean_squared_error: 0.1231 - val_loss: 0.1277 - val_mean_squared_error: 0.1277\n",
      "Epoch 23/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1197 - mean_squared_error: 0.1197 - val_loss: 0.1302 - val_mean_squared_error: 0.1302\n",
      "Epoch 24/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1164 - mean_squared_error: 0.1164 - val_loss: 0.1256 - val_mean_squared_error: 0.1256\n",
      "Epoch 25/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1134 - mean_squared_error: 0.1134 - val_loss: 0.1250 - val_mean_squared_error: 0.1250\n",
      "Epoch 26/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1100 - mean_squared_error: 0.1100 - val_loss: 0.1278 - val_mean_squared_error: 0.1278\n",
      "Epoch 27/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1146 - mean_squared_error: 0.1146 - val_loss: 0.1246 - val_mean_squared_error: 0.1246\n",
      "Epoch 28/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1120 - mean_squared_error: 0.1120 - val_loss: 0.1373 - val_mean_squared_error: 0.1373\n",
      "Epoch 29/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1177 - mean_squared_error: 0.1177 - val_loss: 0.1269 - val_mean_squared_error: 0.1269\n",
      "Epoch 30/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1096 - mean_squared_error: 0.1096 - val_loss: 0.1243 - val_mean_squared_error: 0.1243\n",
      "Epoch 31/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1093 - mean_squared_error: 0.1093 - val_loss: 0.1237 - val_mean_squared_error: 0.1237\n",
      "Epoch 32/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1084 - mean_squared_error: 0.1084 - val_loss: 0.1230 - val_mean_squared_error: 0.1230\n",
      "Epoch 33/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1094 - mean_squared_error: 0.1094 - val_loss: 0.1245 - val_mean_squared_error: 0.1245\n",
      "Epoch 34/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1081 - mean_squared_error: 0.1081 - val_loss: 0.1272 - val_mean_squared_error: 0.1272\n",
      "Epoch 35/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1061 - mean_squared_error: 0.1061 - val_loss: 0.1329 - val_mean_squared_error: 0.1329\n",
      "Epoch 36/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1150 - mean_squared_error: 0.1150 - val_loss: 0.1223 - val_mean_squared_error: 0.1223\n",
      "Epoch 37/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1086 - mean_squared_error: 0.1086 - val_loss: 0.1270 - val_mean_squared_error: 0.1270\n",
      "Epoch 38/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1062 - mean_squared_error: 0.1062 - val_loss: 0.1313 - val_mean_squared_error: 0.1313\n",
      "Epoch 39/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1092 - mean_squared_error: 0.1092 - val_loss: 0.1232 - val_mean_squared_error: 0.1232\n",
      "Epoch 40/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1037 - mean_squared_error: 0.1037 - val_loss: 0.1237 - val_mean_squared_error: 0.1237\n",
      "Epoch 41/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1065 - mean_squared_error: 0.1065 - val_loss: 0.1262 - val_mean_squared_error: 0.1262\n",
      "Epoch 42/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1038 - mean_squared_error: 0.1038 - val_loss: 0.1250 - val_mean_squared_error: 0.1250\n",
      "Epoch 43/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1039 - mean_squared_error: 0.1039 - val_loss: 0.1222 - val_mean_squared_error: 0.1222\n",
      "Epoch 44/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1033 - mean_squared_error: 0.1033 - val_loss: 0.1239 - val_mean_squared_error: 0.1239\n",
      "Epoch 45/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1019 - mean_squared_error: 0.1019 - val_loss: 0.1279 - val_mean_squared_error: 0.1279\n",
      "Epoch 46/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1011 - mean_squared_error: 0.1011 - val_loss: 0.1240 - val_mean_squared_error: 0.1240\n",
      "Epoch 47/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1032 - mean_squared_error: 0.1032 - val_loss: 0.1277 - val_mean_squared_error: 0.1277\n",
      "Epoch 48/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1017 - mean_squared_error: 0.1017 - val_loss: 0.1243 - val_mean_squared_error: 0.1243\n",
      "Epoch 49/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0983 - mean_squared_error: 0.0983 - val_loss: 0.1288 - val_mean_squared_error: 0.1288\n",
      "Epoch 50/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1006 - mean_squared_error: 0.1006 - val_loss: 0.1240 - val_mean_squared_error: 0.1240\n",
      "Training completed in: 31.946 seconds\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1220 - mean_squared_error: 0.1220\n",
      "Inference time for evaluation: 0.230 seconds\n",
      "Test MSE: 0.11932644248008728\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Inference time per sample: 0.000084 seconds\n",
      "Calculated Test MSE: 0.11932640533538409\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression; no activation function\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in: {training_time:.3f} seconds\")\n",
    "\n",
    "# Evaluate the model\n",
    "start_time = time.time()\n",
    "loss, mse = model.evaluate(X_test, y_test)\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference time for evaluation: {inference_time:.3f} seconds\")\n",
    "print(f\"Test MSE: {mse}\")\n",
    "\n",
    "# Make predictions and calculate MSE manually\n",
    "start_time = time.time()\n",
    "predictions = model.predict(X_test)\n",
    "inference_time_per_sample = (time.time() - start_time) / len(X_test)\n",
    "mse_manual = mean_squared_error(y_test, predictions)\n",
    "print(f\"Inference time per sample: {inference_time_per_sample:.6f} seconds\")\n",
    "print(f\"Calculated Test MSE: {mse_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3604 - val_loss: 0.2142\n",
      "Epoch 2/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1864 - val_loss: 0.1708\n",
      "Epoch 3/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1640 - val_loss: 0.1627\n",
      "Epoch 4/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1544 - val_loss: 0.1663\n",
      "Epoch 5/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1535 - val_loss: 0.1771\n",
      "Epoch 6/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1436 - val_loss: 0.1432\n",
      "Epoch 7/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1382 - val_loss: 0.1432\n",
      "Epoch 8/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1337 - val_loss: 0.1420\n",
      "Epoch 9/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1319 - val_loss: 0.1409\n",
      "Epoch 10/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1298 - val_loss: 0.1473\n",
      "Epoch 11/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1246 - val_loss: 0.1437\n",
      "Epoch 12/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1202 - val_loss: 0.1373\n",
      "Epoch 13/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1196 - val_loss: 0.1349\n",
      "Epoch 14/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1170 - val_loss: 0.1423\n",
      "Epoch 15/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1179 - val_loss: 0.1319\n",
      "Epoch 16/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1168 - val_loss: 0.1392\n",
      "Epoch 17/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1124 - val_loss: 0.1314\n",
      "Epoch 18/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1055 - val_loss: 0.1340\n",
      "Epoch 19/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1120 - val_loss: 0.1331\n",
      "Epoch 20/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1051 - val_loss: 0.1342\n",
      "Epoch 21/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1045 - val_loss: 0.1293\n",
      "Epoch 22/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1015 - val_loss: 0.1355\n",
      "Epoch 23/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1093 - val_loss: 0.1297\n",
      "Epoch 24/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1021 - val_loss: 0.1268\n",
      "Epoch 25/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1014 - val_loss: 0.1346\n",
      "Epoch 26/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0991 - val_loss: 0.1338\n",
      "Epoch 27/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0990 - val_loss: 0.1286\n",
      "Epoch 28/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0955 - val_loss: 0.1279\n",
      "Epoch 29/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0980 - val_loss: 0.1276\n",
      "Epoch 30/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0984 - val_loss: 0.1486\n",
      "Epoch 31/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0960 - val_loss: 0.1282\n",
      "Epoch 32/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0969 - val_loss: 0.1310\n",
      "Epoch 33/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0903 - val_loss: 0.1405\n",
      "Epoch 34/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0926 - val_loss: 0.1333\n",
      "Epoch 35/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0927 - val_loss: 0.1259\n",
      "Epoch 36/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0890 - val_loss: 0.1305\n",
      "Epoch 37/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0911 - val_loss: 0.1312\n",
      "Epoch 38/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0902 - val_loss: 0.1303\n",
      "Epoch 39/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0886 - val_loss: 0.1322\n",
      "Epoch 40/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0878 - val_loss: 0.1382\n",
      "Epoch 41/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0851 - val_loss: 0.1351\n",
      "Epoch 42/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0859 - val_loss: 0.1296\n",
      "Epoch 43/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0838 - val_loss: 0.1324\n",
      "Epoch 44/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0825 - val_loss: 0.1295\n",
      "Epoch 45/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0855 - val_loss: 0.1341\n",
      "Epoch 46/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0815 - val_loss: 0.1252\n",
      "Epoch 47/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0809 - val_loss: 0.1310\n",
      "Epoch 48/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0758 - val_loss: 0.1307\n",
      "Epoch 49/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0801 - val_loss: 0.1301\n",
      "Epoch 50/50\n",
      "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0775 - val_loss: 0.1318\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 0.1152\n",
      "Mean Squared Error: 0.118390753865242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_rln_model(input_dim):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # First layer\n",
    "    x = Dense(64, activation='relu')(inputs)\n",
    "    \n",
    "    # Residual block 1\n",
    "    x1 = Dense(64, activation='relu')(x)\n",
    "    x1 = Dense(64, activation='relu')(x1)\n",
    "    block_1_out = add([x, x1])  # Adding input of the block to its output\n",
    "\n",
    "    # Residual block 2\n",
    "    x2 = Dense(64, activation='relu')(block_1_out)\n",
    "    x2 = Dense(64, activation='relu')(x2)\n",
    "    block_2_out = add([block_1_out, x2])  # Adding input of the block to its output\n",
    "\n",
    "    # Output layer for regression\n",
    "    outputs = Dense(1)(block_2_out)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming input_dim is the number of features in X_train\n",
    "model = build_rln_model(input_dim=X_train.shape[1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Y', axis=1).values\n",
    "y = df['Y'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape y_train and y_test to be 2D\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.06955 | train_rmse: 1.68216 | valid_rmse: 1.66649 |  0:00:02s\n",
      "epoch 1  | loss: 0.38353 | train_rmse: 1.2345  | valid_rmse: 1.21493 |  0:00:05s\n",
      "epoch 2  | loss: 0.29676 | train_rmse: 1.01229 | valid_rmse: 1.02259 |  0:00:07s\n",
      "epoch 3  | loss: 0.26111 | train_rmse: 0.84398 | valid_rmse: 0.86743 |  0:00:09s\n",
      "epoch 4  | loss: 0.24028 | train_rmse: 0.82805 | valid_rmse: 0.84625 |  0:00:12s\n",
      "epoch 5  | loss: 0.22179 | train_rmse: 0.7278  | valid_rmse: 0.7438  |  0:00:14s\n",
      "epoch 6  | loss: 0.20889 | train_rmse: 0.69366 | valid_rmse: 0.69468 |  0:00:17s\n",
      "epoch 7  | loss: 0.20865 | train_rmse: 0.62879 | valid_rmse: 0.63297 |  0:00:19s\n",
      "epoch 8  | loss: 0.19562 | train_rmse: 0.58878 | valid_rmse: 0.58628 |  0:00:21s\n",
      "epoch 9  | loss: 0.18968 | train_rmse: 0.54019 | valid_rmse: 0.53812 |  0:00:24s\n",
      "epoch 10 | loss: 0.18495 | train_rmse: 0.53108 | valid_rmse: 0.5292  |  0:00:26s\n",
      "epoch 11 | loss: 0.18026 | train_rmse: 0.58089 | valid_rmse: 0.57294 |  0:00:28s\n",
      "epoch 12 | loss: 0.17908 | train_rmse: 0.54776 | valid_rmse: 0.53941 |  0:00:31s\n",
      "epoch 13 | loss: 0.17669 | train_rmse: 0.4952  | valid_rmse: 0.48778 |  0:00:33s\n",
      "epoch 14 | loss: 0.17149 | train_rmse: 0.48589 | valid_rmse: 0.48119 |  0:00:35s\n",
      "epoch 15 | loss: 0.17775 | train_rmse: 0.52087 | valid_rmse: 0.51314 |  0:00:38s\n",
      "epoch 16 | loss: 0.17455 | train_rmse: 0.45722 | valid_rmse: 0.4505  |  0:00:40s\n",
      "epoch 17 | loss: 0.16759 | train_rmse: 0.44263 | valid_rmse: 0.43796 |  0:00:43s\n",
      "epoch 18 | loss: 0.16744 | train_rmse: 0.47657 | valid_rmse: 0.46948 |  0:00:45s\n",
      "epoch 19 | loss: 0.16939 | train_rmse: 0.44571 | valid_rmse: 0.44124 |  0:00:48s\n",
      "epoch 20 | loss: 0.16839 | train_rmse: 0.43011 | valid_rmse: 0.42261 |  0:00:50s\n",
      "epoch 21 | loss: 0.16338 | train_rmse: 0.38835 | valid_rmse: 0.38593 |  0:00:52s\n",
      "epoch 22 | loss: 0.17517 | train_rmse: 0.48022 | valid_rmse: 0.47033 |  0:00:55s\n",
      "epoch 23 | loss: 0.18131 | train_rmse: 0.413   | valid_rmse: 0.40794 |  0:00:57s\n",
      "epoch 24 | loss: 0.16723 | train_rmse: 0.40057 | valid_rmse: 0.39873 |  0:01:00s\n",
      "epoch 25 | loss: 0.16104 | train_rmse: 0.39671 | valid_rmse: 0.39417 |  0:01:02s\n",
      "epoch 26 | loss: 0.16626 | train_rmse: 0.47935 | valid_rmse: 0.47102 |  0:01:05s\n",
      "epoch 27 | loss: 0.16385 | train_rmse: 0.44414 | valid_rmse: 0.43642 |  0:01:07s\n",
      "epoch 28 | loss: 0.1592  | train_rmse: 0.37919 | valid_rmse: 0.37795 |  0:01:10s\n",
      "epoch 29 | loss: 0.15776 | train_rmse: 0.37374 | valid_rmse: 0.37174 |  0:01:12s\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_valid_rmse = 0.37174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.13819212327366764\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "# Set up TabNet Regressor\n",
    "# Note: Make sure to adjust the hyperparameters according to your specific needs\n",
    "model = TabNetRegressor(optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=2e-2),\n",
    "                        scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        mask_type='sparsemax'  # This will be used for sparse features\n",
    "                        )\n",
    "\n",
    "# Fit the model\n",
    "max_epochs = 30 if not torch.cuda.is_available() else 30  # Adjust epochs based on your GPU availability\n",
    "model.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,  # Early stopping patience\n",
    "    batch_size=1024,  # Adjust based on your GPU capacity\n",
    "    virtual_batch_size=128,  # Size of the mini batches for Ghost Batch Normalization\n",
    "    num_workers=0,  # Based on your system: can increase if you have more CPU cores\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Predicting and Evaluating\n",
    "predictions = model.predict(X_test)\n",
    "mse = np.mean((y_test - predictions)**2)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.73203 | test_rmse: 0.61274 |  0:00:02s\n",
      "epoch 1  | loss: 0.3013  | test_rmse: 0.50425 |  0:00:05s\n",
      "epoch 2  | loss: 0.24849 | test_rmse: 0.47658 |  0:00:08s\n",
      "epoch 3  | loss: 0.22508 | test_rmse: 0.44943 |  0:00:11s\n",
      "epoch 4  | loss: 0.21512 | test_rmse: 0.48218 |  0:00:14s\n",
      "epoch 5  | loss: 0.20837 | test_rmse: 0.44457 |  0:00:17s\n",
      "epoch 6  | loss: 0.20282 | test_rmse: 0.43617 |  0:00:20s\n",
      "epoch 7  | loss: 0.2091  | test_rmse: 0.43699 |  0:00:22s\n",
      "epoch 8  | loss: 0.19905 | test_rmse: 0.46584 |  0:00:25s\n",
      "epoch 9  | loss: 0.21054 | test_rmse: 0.44556 |  0:00:28s\n",
      "epoch 10 | loss: 0.19675 | test_rmse: 0.42091 |  0:00:30s\n",
      "epoch 11 | loss: 0.18898 | test_rmse: 0.41133 |  0:00:33s\n",
      "epoch 12 | loss: 0.18459 | test_rmse: 0.42966 |  0:00:36s\n",
      "epoch 13 | loss: 0.18579 | test_rmse: 0.41667 |  0:00:39s\n",
      "epoch 14 | loss: 0.18434 | test_rmse: 0.3992  |  0:00:41s\n",
      "epoch 15 | loss: 0.17325 | test_rmse: 0.39603 |  0:00:44s\n",
      "epoch 16 | loss: 0.17265 | test_rmse: 0.39341 |  0:00:48s\n",
      "epoch 17 | loss: 0.16679 | test_rmse: 0.39525 |  0:00:50s\n",
      "epoch 18 | loss: 0.17204 | test_rmse: 0.39476 |  0:00:53s\n",
      "epoch 19 | loss: 0.1704  | test_rmse: 0.42755 |  0:00:56s\n",
      "epoch 20 | loss: 0.17351 | test_rmse: 0.41977 |  0:00:59s\n",
      "epoch 21 | loss: 0.16606 | test_rmse: 0.391   |  0:01:02s\n",
      "epoch 22 | loss: 0.15876 | test_rmse: 0.37552 |  0:01:05s\n",
      "epoch 23 | loss: 0.16351 | test_rmse: 0.41219 |  0:01:08s\n",
      "epoch 24 | loss: 0.16673 | test_rmse: 0.40463 |  0:01:10s\n",
      "epoch 25 | loss: 0.16292 | test_rmse: 0.41568 |  0:01:13s\n",
      "epoch 26 | loss: 0.16102 | test_rmse: 0.37821 |  0:01:16s\n",
      "epoch 27 | loss: 0.15552 | test_rmse: 0.37907 |  0:01:19s\n",
      "epoch 28 | loss: 0.15351 | test_rmse: 0.39612 |  0:01:22s\n",
      "epoch 29 | loss: 0.15506 | test_rmse: 0.37857 |  0:01:24s\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 22 and best_test_rmse = 0.37552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in: 86.503 seconds\n",
      "Inference completed in: 0.263 seconds\n",
      "Test MSE: 0.14101276557535194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import time  # Import time to measure training and inference time\n",
    "\n",
    "X = df.drop('Y', axis=1).values\n",
    "y = df['Y'].values.reshape(-1, 1)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize TabNetRegressor\n",
    "model = TabNetRegressor(verbose=1, optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=2e-2))\n",
    "\n",
    "# Start timing for training\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    X_train=X_train_scaled, y_train=y_train,\n",
    "    eval_set=[(X_test_scaled, y_test)],\n",
    "    eval_name=['test'],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=30,\n",
    "    patience=50,  # Early stopping\n",
    "    batch_size=256,  # Mini-batch size for training\n",
    "    virtual_batch_size=128  # Size of the mini-batches for the \"Ghost Batch Normalization\"\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in: {training_time:.3f} seconds\")\n",
    "\n",
    "# Start timing for inference\n",
    "start_time = time.time()\n",
    "preds = model.predict(X_test_scaled)\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference completed in: {inference_time:.3f} seconds\")\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "print(f\"Test MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in: 41.608 seconds\n",
      "Inference completed in: 0.039 seconds\n",
      "Test MSE: 0.1275930404663086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Example DataFrame setup (assuming df is predefined)\n",
    "Y = df['Y'].values  # Target variable\n",
    "X = df.drop('Y', axis=1).values  # Features\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# DataLoader setup\n",
    "train_data = DataLoader(TensorDataset(X_train_torch, y_train_torch), batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(TensorDataset(X_test_torch, y_test_torch), batch_size=64, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model with time measurement\n",
    "start_time = time.time()\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in: {training_time:.3f} seconds\")\n",
    "\n",
    "# Inference with time measurement\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for inputs, targets in test_data:\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "predictions = torch.cat(predictions)\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference completed in: {inference_time:.3f} seconds\")\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = criterion(predictions, y_test_torch)\n",
    "print(f\"Test MSE: {mse.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in: 94.881 seconds\n",
      "Inference completed in: 0.081 seconds\n",
      "Test MSE: 0.14129550755023956\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assuming df is your DataFrame and 'Y' is your target column\n",
    "Y = df['Y'].values\n",
    "X = df.drop('Y', axis=1).values\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# DataLoader setup\n",
    "train_data = DataLoader(TensorDataset(X_train_torch, y_train_torch), batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(TensorDataset(X_test_torch, y_test_torch), batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a neural network with stochastic gates (dropout)\n",
    "class StochasticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StochasticNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = StochasticNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with timing\n",
    "start_time = time.time()\n",
    "epochs = 100\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in: {training_time:.3f} seconds\")\n",
    "\n",
    "# Inference with timing\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for inputs, _ in test_data:\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "predictions = torch.cat(predictions)\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference completed in: {inference_time:.3f} seconds\")\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = criterion(predictions, y_test_torch)\n",
    "print(f\"Test MSE: {mse.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Sample data preparation\n",
    "Y = df['Y'].values\n",
    "X = df.drop('Y', axis=1).values\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_torch, y_train_torch), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_torch, y_test_torch), batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleTransformer(input_dim\u001b[38;5;241m=\u001b[39mX_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     19\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[1;32mIn[57], line 9\u001b[0m, in \u001b[0;36mSimpleTransformer.__init__\u001b[1;34m(self, input_dim, num_heads, num_layers, dropout_rate)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, num_heads, num_layers, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SimpleTransformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      7\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, input_dim),\n\u001b[0;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mDropout(dropout_rate),\n\u001b[1;32m----> 9\u001b[0m         nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(d_model\u001b[38;5;241m=\u001b[39minput_dim, nhead\u001b[38;5;241m=\u001b[39mnum_heads, dropout\u001b[38;5;241m=\u001b[39mdropout_rate),\n\u001b[0;32m     10\u001b[0m         nn\u001b[38;5;241m.\u001b[39mTransformerEncoder(nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(d_model\u001b[38;5;241m=\u001b[39minput_dim, nhead\u001b[38;5;241m=\u001b[39mnum_heads), num_layers\u001b[38;5;241m=\u001b[39mnum_layers),\n\u001b[0;32m     11\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:589\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[0;32m    587\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    590\u001b[0m                                     bias\u001b[38;5;241m=\u001b[39mbias, batch_first\u001b[38;5;241m=\u001b[39mbatch_first,\n\u001b[0;32m    591\u001b[0m                                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tommy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1011\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m embed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim must be divisible by num_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, dropout_rate=0.1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dropout=dropout_rate),\n",
    "            nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads), num_layers=num_layers),\n",
    "            nn.Linear(input_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SimpleTransformer(input_dim=X_train_scaled.shape[1], num_heads=2, num_layers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
